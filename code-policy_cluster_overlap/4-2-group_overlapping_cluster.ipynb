{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68e2efb",
   "metadata": {},
   "source": [
    "### é‡å æ”¿ç­–ä¸åŒé˜ˆå€¼ç°‡èšç±»å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cb146aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================\n",
      " ğŸš€ å¼€å§‹å¤„ç†: Breadth\n",
      "============================================\n",
      "   ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: f:\\Desktop\\ç§‘ç ”é¡¹ç›®\\1.è´Ÿè´£ç§‘ç ”é¡¹ç›®\\Climate Policy\\CAMPF_Supplementary\\data\\4-2-group_overlapping_cluster\\Breadth\n",
      "   æ­£åœ¨è®¡ç®—èšç±»æ ‘å¹¶æ£€æŸ¥é‡å¿ƒ...\n",
      "   [Breadth é‡å¿ƒ] å·¦: 6977 | å³: 8088\n",
      "   >>> âš ï¸ æ£€æµ‹åˆ°é‡å¿ƒåœ¨å³ï¼Œæ‰§è¡Œé•œåƒç¿»è½¬...\n",
      "   ğŸ¯ å¾…å¤„ç† K å€¼ (20ä¸ª): [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   6 out of  20 | elapsed:  1.9min remaining:  4.4min\n",
      "[Parallel(n_jobs=-3)]: Done  11 out of  20 | elapsed:  2.0min remaining:  1.6min\n",
      "[Parallel(n_jobs=-3)]: Done  16 out of  20 | elapsed:  2.0min remaining:   30.0s\n",
      "[Parallel(n_jobs=-3)]: Done  20 out of  20 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Breadth] k=2 å®Œæˆ\n",
      "   [Breadth] k=3 å®Œæˆ\n",
      "   [Breadth] k=4 å®Œæˆ\n",
      "   [Breadth] k=5 å®Œæˆ\n",
      "   [Breadth] k=6 å®Œæˆ\n",
      "   [Breadth] k=7 å®Œæˆ\n",
      "   [Breadth] k=8 å®Œæˆ\n",
      "   [Breadth] k=9 å®Œæˆ\n",
      "   [Breadth] k=10 å®Œæˆ\n",
      "   [Breadth] k=11 å®Œæˆ\n",
      "   [Breadth] k=12 å®Œæˆ\n",
      "   [Breadth] k=13 å®Œæˆ\n",
      "   [Breadth] k=14 å®Œæˆ\n",
      "   [Breadth] k=15 å®Œæˆ\n",
      "   [Breadth] k=16 å®Œæˆ\n",
      "   [Breadth] k=17 å®Œæˆ\n",
      "   [Breadth] k=18 å®Œæˆ\n",
      "   [Breadth] k=19 å®Œæˆ\n",
      "   [Breadth] k=20 å®Œæˆ\n",
      "   [Breadth] k=21 å®Œæˆ\n",
      "\n",
      "============================================\n",
      " ğŸš€ å¼€å§‹å¤„ç†: Intensity\n",
      "============================================\n",
      "   ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: f:\\Desktop\\ç§‘ç ”é¡¹ç›®\\1.è´Ÿè´£ç§‘ç ”é¡¹ç›®\\Climate Policy\\CAMPF_Supplementary\\data\\4-2-group_overlapping_cluster\\Intensity\n",
      "   æ­£åœ¨è®¡ç®—èšç±»æ ‘å¹¶æ£€æŸ¥é‡å¿ƒ...\n",
      "   [Intensity é‡å¿ƒ] å·¦: 8066 | å³: 7731\n",
      "   >>> âœ… é‡å¿ƒæ­£å¸¸ã€‚\n",
      "   ğŸ¯ å¾…å¤„ç† K å€¼ (20ä¸ª): [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   6 out of  20 | elapsed:  1.3min remaining:  3.0min\n",
      "[Parallel(n_jobs=-3)]: Done  11 out of  20 | elapsed:  1.3min remaining:  1.1min\n",
      "[Parallel(n_jobs=-3)]: Done  16 out of  20 | elapsed:  1.3min remaining:   20.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Intensity] k=2 å®Œæˆ\n",
      "   [Intensity] k=3 å®Œæˆ\n",
      "   [Intensity] k=4 å®Œæˆ\n",
      "   [Intensity] k=5 å®Œæˆ\n",
      "   [Intensity] k=6 å®Œæˆ\n",
      "   [Intensity] k=7 å®Œæˆ\n",
      "   [Intensity] k=8 å®Œæˆ\n",
      "   [Intensity] k=9 å®Œæˆ\n",
      "   [Intensity] k=10 å®Œæˆ\n",
      "   [Intensity] k=11 å®Œæˆ\n",
      "   [Intensity] k=12 å®Œæˆ\n",
      "   [Intensity] k=13 å®Œæˆ\n",
      "   [Intensity] k=14 å®Œæˆ\n",
      "   [Intensity] k=15 å®Œæˆ\n",
      "   [Intensity] k=16 å®Œæˆ\n",
      "   [Intensity] k=17 å®Œæˆ\n",
      "   [Intensity] k=18 å®Œæˆ\n",
      "   [Intensity] k=19 å®Œæˆ\n",
      "   [Intensity] k=20 å®Œæˆ\n",
      "   [Intensity] k=21 å®Œæˆ\n",
      "\n",
      "âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Done  20 out of  20 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, leaves_list\n",
    "from pathlib import Path\n",
    "import colorsys\n",
    "import warnings\n",
    "from joblib import Parallel, delayed  # âœ… é‡æ–°å¼•å…¥å¹¶è¡Œ\n",
    "\n",
    "# ç¦ç”¨è­¦å‘Š\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. åŸºç¡€é…ç½®\n",
    "# ==========================================\n",
    "current_dir = Path.cwd()\n",
    "data_dir = current_dir.parent / \"data\"\n",
    "base_output_dir = data_dir / \"4-2-group_overlapping_cluster\"\n",
    "base_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_times_black_font():\n",
    "    return FontProperties(family='Times New Roman', size=26, weight='black')\n",
    "\n",
    "T_BLACK = get_times_black_font()\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'font.weight': 'black',\n",
    "    'axes.labelweight': 'black',\n",
    "    'axes.titleweight': 'black',\n",
    "    'axes.unicode_minus': False,\n",
    "    'figure.dpi': 300\n",
    "})\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.rcParams['axes.axisbelow'] = False\n",
    "\n",
    "def lighten_color_slightly(hex_color: str) -> str:\n",
    "    r, g, b = plt.cm.colors.to_rgb(hex_color)\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "    return plt.cm.colors.to_hex(colorsys.hls_to_rgb(h, min(1.0, l + 0.12), s * 0.88))\n",
    "\n",
    "def get_palette(k: int) -> list:\n",
    "    colors = ['#1f77b4', '#2ca02c', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "    return (colors * (k // len(colors) + 1))[:k]\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ•°æ®å¤„ç†\n",
    "# ==========================================\n",
    "def build_overlap_matrix(input_file: Path) -> tuple[pd.DataFrame, int]:\n",
    "    if not input_file.exists():\n",
    "        return None, 0\n",
    "\n",
    "    df = pd.read_csv(input_file, encoding='utf-8-sig')\n",
    "\n",
    "    if 'L2æ”¿ç­–' not in df.columns and 'L2æ”¿ç­–ä¸­æ–‡å' in df.columns:\n",
    "        df.rename(columns={'L2æ”¿ç­–ä¸­æ–‡å': 'L2æ”¿ç­–'}, inplace=True)\n",
    "\n",
    "    def safe_col(d, name):\n",
    "        return d[name].iloc[:, 0] if isinstance(d[name], pd.DataFrame) else d[name]\n",
    "\n",
    "    df_clean = pd.DataFrame({\n",
    "        'P': safe_col(df, 'L2æ”¿ç­–').astype(str),\n",
    "        'C': safe_col(df, 'å›½å®¶').astype(str),\n",
    "        'G': safe_col(df, 'èšç±»ID')\n",
    "    }).drop_duplicates()\n",
    "\n",
    "    policies = sorted(df_clean['P'].unique())\n",
    "    countries = sorted(df_clean['C'].unique())\n",
    "\n",
    "    res_map = {\n",
    "        p: dict(zip(\n",
    "            df_clean[df_clean['P'] == p]['C'],\n",
    "            df_clean[df_clean['P'] == p]['G']\n",
    "        )) for p in policies\n",
    "    }\n",
    "\n",
    "    n = len(countries)\n",
    "    matrix = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    for i, c1 in enumerate(countries):\n",
    "        for j in range(i, n):\n",
    "            c2 = countries[j]\n",
    "            common = sum(\n",
    "                1 for p in policies\n",
    "                if c1 in res_map[p]\n",
    "                and c2 in res_map[p]\n",
    "                and (res_map[p][c1] == res_map[p][c2] if i != j else True)\n",
    "            )\n",
    "            matrix[i, j] = matrix[j, i] = common\n",
    "\n",
    "    return pd.DataFrame(matrix, index=countries, columns=countries), len(policies)\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ ¸å¿ƒç»˜å›¾ (ç‹¬ç«‹å‡½æ•°ï¼Œä¾›å¹¶è¡Œè°ƒç”¨)\n",
    "# ==========================================\n",
    "def draw_and_save_k(\n",
    "    k: int,\n",
    "    Z: np.ndarray,\n",
    "    df_o: pd.DataFrame,\n",
    "    max_o: int,\n",
    "    output_folder: Path,\n",
    "    type_name: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    å•ä¸ªKå€¼çš„å¤„ç†é€»è¾‘ï¼šç”Ÿæˆé¢œè‰² -> å¯¼å‡ºCSV -> ç»˜å›¾\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. è®¡ç®—æ ‡ç­¾\n",
    "        labels = fcluster(Z, t=k, criterion='maxclust') - 1\n",
    "        palette = get_palette(k)\n",
    "        row_colors = pd.Series(\n",
    "            [lighten_color_slightly(palette[l % len(palette)]) for l in labels],\n",
    "            index=df_o.index\n",
    "        )\n",
    "\n",
    "        # 2. å¯¼å‡º CSV\n",
    "        csv_filename = f'{type_name}_cluster_k={k}.csv'\n",
    "        groups = {}\n",
    "        for country, cid in zip(df_o.index.tolist(), labels.astype(int).tolist()):\n",
    "            groups.setdefault(cid, []).append(country)\n",
    "\n",
    "        rows = []\n",
    "        for cid in sorted(groups):\n",
    "            members = groups[cid]\n",
    "            rows.append({\n",
    "                'å…±è¯†ID': cid + 1,\n",
    "                'é…è‰²HEX': lighten_color_slightly(palette[cid % len(palette)]),\n",
    "                'å›½å®¶æ•°': len(members),\n",
    "                'å›½å®¶åˆ—è¡¨': ', '.join(sorted(members)),\n",
    "                'ç°‡å†…å¹³å‡é‡å ': f'{df_o.loc[members, members].values.mean():.2f}'\n",
    "            })\n",
    "        \n",
    "        pd.DataFrame(rows).to_csv(output_folder / csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        # 3. ç»˜å›¾ (ä¸¥æ ¼ Style Reference)\n",
    "        png_filename = f'{type_name}_cluster_k={k}.png'\n",
    "        size = max(18, len(df_o) * 0.72)\n",
    "\n",
    "        g = sns.clustermap(\n",
    "            df_o,\n",
    "            row_linkage=Z,\n",
    "            col_linkage=Z,\n",
    "            cmap='RdYlBu_r',\n",
    "            row_colors=row_colors,\n",
    "            col_colors=row_colors,\n",
    "            dendrogram_ratio=0.1,\n",
    "            linewidths=0.5,\n",
    "            figsize=(size, size),\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            vmin=0,\n",
    "            vmax=max_o,\n",
    "            annot_kws={'fontsize': 28, 'weight': 'black', 'fontproperties': T_BLACK},\n",
    "            cbar_kws={'ticks': [0, int(max_o*0.25), int(max_o*0.5), int(max_o*0.75), max_o]},\n",
    "            tree_kws={'linewidths': 6}\n",
    "        )\n",
    "\n",
    "        # è°ƒæ•´ Colorbar ä½ç½®\n",
    "        hm_pos = g.ax_heatmap.get_position()\n",
    "        pad = 0.04; cbar_w = 0.018; cbar_h = hm_pos.height * 0.78\n",
    "        cbar_y = hm_pos.y0 + hm_pos.height * 0.11\n",
    "        g.cax.set_position([hm_pos.x1 + pad, cbar_y, cbar_w, cbar_h])\n",
    "        g.cax.yaxis.set_label_position('right')\n",
    "\n",
    "        g.cax.set_axisbelow(False)\n",
    "        for coll in g.cax.collections: coll.set_zorder(0)\n",
    "        g.cax.yaxis.set_ticks_position('both')\n",
    "        g.cax.tick_params(axis='y', direction='in', length=12, width=3, colors='black', left=True, right=True, pad=6)\n",
    "\n",
    "        for t in g.cax.yaxis.get_major_ticks():\n",
    "            for ln in (t.tick1line, t.tick2line):\n",
    "                ln.set_zorder(10); ln.set_clip_on(False)\n",
    "        for lab in g.cax.get_yticklabels(): lab.set_zorder(11)\n",
    "\n",
    "        g.cax.set_ylabel(f'Overlap Count ({type_name})', fontproperties=T_BLACK, fontsize=32, weight='black', labelpad=20)\n",
    "        plt.setp(g.cax.get_yticklabels(), fontproperties=T_BLACK, fontsize=28, weight='black')\n",
    "\n",
    "        # è½´æ ‡ç­¾\n",
    "        for lab in g.ax_heatmap.get_xticklabels():\n",
    "            lab.set_fontproperties(T_BLACK); lab.set_rotation(45); lab.set_ha('right')\n",
    "        for lab in g.ax_heatmap.get_yticklabels():\n",
    "            lab.set_fontproperties(T_BLACK); lab.set_rotation(0)\n",
    "\n",
    "        # ç²—è¾¹æ¡†\n",
    "        ax = g.ax_heatmap\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True); spine.set_linewidth(4); spine.set_edgecolor('black')\n",
    "\n",
    "        # ç»˜åˆ¶èšç±»é»‘æ¡†\n",
    "        order = g.dendrogram_row.reordered_ind\n",
    "        ordered_labels = labels[order].astype(int).tolist()\n",
    "        ranges = {}\n",
    "        for idx, cid in enumerate(ordered_labels):\n",
    "            if cid not in ranges: ranges[cid] = (idx, idx)\n",
    "            else: ranges[cid] = (ranges[cid][0], idx)\n",
    "\n",
    "        boundary_color = '#4D4D4D'\n",
    "        for _, (s, e) in ranges.items():\n",
    "            ax.plot([s, s], [s, e + 1], color=boundary_color, linewidth=6, clip_on=False, zorder=20)\n",
    "            ax.plot([e + 1, e + 1], [s, e + 1], color=boundary_color, linewidth=6, clip_on=False, zorder=20)\n",
    "            ax.plot([s, e + 1], [s, s], color=boundary_color, linewidth=6, clip_on=False, zorder=20)\n",
    "            ax.plot([s, e + 1], [e + 1, e + 1], color=boundary_color, linewidth=6, clip_on=False, zorder=20)\n",
    "\n",
    "        # ä¿å­˜\n",
    "        plt.savefig(output_folder / png_filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close('all') # å…³é”®ï¼šå…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜\n",
    "        \n",
    "        return f\"[{type_name}] k={k} å®Œæˆ\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[{type_name}] k={k} å¤±è´¥: {str(e)}\"\n",
    "\n",
    "# ==========================================\n",
    "# 4. å¼ºåˆ¶å·¦åæ’åº (Force Left Linkage)\n",
    "# ==========================================\n",
    "def compute_force_left_linkage(df: pd.DataFrame, type_name: str) -> np.ndarray:\n",
    "    print(f\"   æ­£åœ¨è®¡ç®—èšç±»æ ‘å¹¶æ£€æŸ¥é‡å¿ƒ...\")\n",
    "    Z = linkage(df.values, method='ward')\n",
    "    weights = df.sum(axis=1).values\n",
    "\n",
    "    # å±€éƒ¨æ’åº\n",
    "    w_map = {i: v for i, v in enumerate(weights)}\n",
    "    for i, (c1, c2, _, _) in enumerate(Z):\n",
    "        c1, c2 = int(c1), int(c2)\n",
    "        if w_map[c1] < w_map[c2]: \n",
    "            Z[i, 0], Z[i, 1] = c2, c1\n",
    "        w_map[len(weights) + i] = w_map[c1] + w_map[c2]\n",
    "\n",
    "    # å…¨å±€é‡å¿ƒç¿»è½¬\n",
    "    leaves = leaves_list(Z)\n",
    "    mid = len(leaves) // 2\n",
    "    left_weight = weights[leaves[:mid]].sum()\n",
    "    right_weight = weights[leaves[mid:]].sum()\n",
    "\n",
    "    print(f\"   [{type_name} é‡å¿ƒ] å·¦: {left_weight:.0f} | å³: {right_weight:.0f}\")\n",
    "\n",
    "    if right_weight > left_weight:\n",
    "        print(f\"   >>> âš ï¸ æ£€æµ‹åˆ°é‡å¿ƒåœ¨å³ï¼Œæ‰§è¡Œé•œåƒç¿»è½¬...\")\n",
    "        Z[:, [0, 1]] = Z[:, [1, 0]]\n",
    "    else:\n",
    "        print(f\"   >>> âœ… é‡å¿ƒæ­£å¸¸ã€‚\")\n",
    "    \n",
    "    return Z\n",
    "\n",
    "def get_valid_k_values(Z: np.ndarray, n_items: int) -> list[int]:\n",
    "    valid = []\n",
    "    for k in range(2, min(30, n_items) + 1):\n",
    "        labels = fcluster(Z, t=k, criterion='maxclust')\n",
    "        if len(set(labels)) == k:\n",
    "            valid.append(k)\n",
    "            if len(valid) == 20:\n",
    "                break\n",
    "    return valid\n",
    "\n",
    "# ==========================================\n",
    "# 5. ä¸»æµç¨‹\n",
    "# ==========================================\n",
    "def process_type_pipeline(file_path: Path, type_name: str):\n",
    "    \"\"\"\n",
    "    å•ä¸ªç±»å‹çš„ä¸»å¤„ç†é€»è¾‘\n",
    "    \"\"\"\n",
    "    print(f\"\\n============================================\")\n",
    "    print(f\" ğŸš€ å¼€å§‹å¤„ç†: {type_name}\")\n",
    "    print(f\"============================================\")\n",
    "\n",
    "    # 1. å‡†å¤‡æ–‡ä»¶å¤¹\n",
    "    # âœ… å…³é”®ä¿®æ”¹ï¼šä¸ºæ¯ä¸ªç±»å‹åˆ›å»ºä¸€ä¸ªå­æ–‡ä»¶å¤¹\n",
    "    type_output_dir = base_output_dir / type_name\n",
    "    type_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"   ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: {type_output_dir}\")\n",
    "\n",
    "    # 2. è¯»å–æ•°æ®\n",
    "    df_o, max_o = build_overlap_matrix(file_path)\n",
    "    if df_o is None: \n",
    "        print(f\"   âŒ è·³è¿‡ï¼Œæ–‡ä»¶æœªæ‰¾åˆ°\")\n",
    "        return\n",
    "\n",
    "    # 3. è®¡ç®— Linkage\n",
    "    Z = compute_force_left_linkage(df_o, type_name)\n",
    "\n",
    "    # 4. è·å–æœ‰æ•ˆ K å€¼\n",
    "    valid_k = get_valid_k_values(Z, len(df_o))\n",
    "    print(f\"   ğŸ¯ å¾…å¤„ç† K å€¼ ({len(valid_k)}ä¸ª): {valid_k}\")\n",
    "\n",
    "    # 5. å¹¶è¡Œæ‰§è¡Œç»˜å›¾\n",
    "    # âœ… å…³é”®ä¿®æ”¹ï¼šå¹¶è¡Œå¤„ç†\n",
    "    results = Parallel(n_jobs=-3, verbose=5)(\n",
    "        delayed(draw_and_save_k)(\n",
    "            k, Z, df_o, max_o, type_output_dir, type_name\n",
    "        ) for k in valid_k\n",
    "    )\n",
    "\n",
    "    for res in results:\n",
    "        print(f\"   {res}\")\n",
    "\n",
    "def main():\n",
    "    # å®šä¹‰ä»»åŠ¡åˆ—è¡¨\n",
    "    files = {\n",
    "        \"Breadth\": data_dir / \"3-1-L2_Policy_Clustering_Breadth.csv\",\n",
    "        \"Intensity\": data_dir / \"3-1-L2_Policy_Clustering_Intensity.csv\"\n",
    "    }\n",
    "\n",
    "    for type_name, path in files.items():\n",
    "        process_type_pipeline(path, type_name)\n",
    "\n",
    "    print(\"\\nâœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea38adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================\n",
      " ğŸš€ å¼€å§‹å¤„ç†: Breadth\n",
      "============================================\n",
      "   ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: f:\\Desktop\\ç§‘ç ”é¡¹ç›®\\1.è´Ÿè´£ç§‘ç ”é¡¹ç›®\\Climate Policy\\CAMPF_Supplementary\\data\\4-2-group_overlapping_cluster\\Breadth\n",
      "   æ­£åœ¨è®¡ç®—èšç±»æ ‘ï¼ˆComplete Linkageï¼‰å¹¶æ£€æŸ¥é‡å¿ƒ...\n",
      "   [Breadth é‡å¿ƒ] å·¦: 7972 | å³: 7093\n",
      "   >>> âœ… é‡å¿ƒæ­£å¸¸ã€‚\n",
      "   ğŸ¯ å¾…å¤„ç†é˜ˆå€¼RC (11ä¸ª): [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "      è§£é‡Šï¼šæ¯ä¸ª t è¡¨ç¤º Overlap âˆˆ (t-1, +âˆ]ï¼ˆç­‰ä»· Overlap â‰¥ tï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   3 out of  11 | elapsed:   25.2s remaining:  1.1min\n",
      "[Parallel(n_jobs=-3)]: Done   6 out of  11 | elapsed:   27.0s remaining:   22.5s\n",
      "[Parallel(n_jobs=-3)]: Done   9 out of  11 | elapsed:   27.2s remaining:    6.0s\n",
      "[Parallel(n_jobs=-3)]: Done  11 out of  11 | elapsed:   27.5s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 30 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Breadth] RCé˜ˆå€¼ t=5 å®Œæˆ (ç°‡æ•°=6, d_cut=10)\n",
      "   [Breadth] RCé˜ˆå€¼ t=6 å®Œæˆ (ç°‡æ•°=8, d_cut=9)\n",
      "   [Breadth] RCé˜ˆå€¼ t=7 å®Œæˆ (ç°‡æ•°=9, d_cut=8)\n",
      "   [Breadth] RCé˜ˆå€¼ t=8 å®Œæˆ (ç°‡æ•°=14, d_cut=7)\n",
      "   [Breadth] RCé˜ˆå€¼ t=9 å®Œæˆ (ç°‡æ•°=18, d_cut=6)\n",
      "   [Breadth] RCé˜ˆå€¼ t=10 å®Œæˆ (ç°‡æ•°=23, d_cut=5)\n",
      "   [Breadth] RCé˜ˆå€¼ t=11 å®Œæˆ (ç°‡æ•°=29, d_cut=4)\n",
      "   [Breadth] RCé˜ˆå€¼ t=12 å®Œæˆ (ç°‡æ•°=37, d_cut=3)\n",
      "   [Breadth] RCé˜ˆå€¼ t=13 å®Œæˆ (ç°‡æ•°=41, d_cut=2)\n",
      "   [Breadth] RCé˜ˆå€¼ t=14 å®Œæˆ (ç°‡æ•°=45, d_cut=1)\n",
      "   [Breadth] RCé˜ˆå€¼ t=15 å®Œæˆ (ç°‡æ•°=49, d_cut=0)\n",
      "\n",
      "============================================\n",
      " ğŸš€ å¼€å§‹å¤„ç†: Intensity\n",
      "============================================\n",
      "   ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: f:\\Desktop\\ç§‘ç ”é¡¹ç›®\\1.è´Ÿè´£ç§‘ç ”é¡¹ç›®\\Climate Policy\\CAMPF_Supplementary\\data\\4-2-group_overlapping_cluster\\Intensity\n",
      "   æ­£åœ¨è®¡ç®—èšç±»æ ‘ï¼ˆComplete Linkageï¼‰å¹¶æ£€æŸ¥é‡å¿ƒ...\n",
      "   [Intensity é‡å¿ƒ] å·¦: 8117 | å³: 7680\n",
      "   >>> âœ… é‡å¿ƒæ­£å¸¸ã€‚\n",
      "   ğŸ¯ å¾…å¤„ç†é˜ˆå€¼RC (11ä¸ª): [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "      è§£é‡Šï¼šæ¯ä¸ª t è¡¨ç¤º Overlap âˆˆ (t-1, +âˆ]ï¼ˆç­‰ä»· Overlap â‰¥ tï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Done   3 out of  11 | elapsed:   21.9s remaining:   58.5s\n",
      "[Parallel(n_jobs=-3)]: Done   6 out of  11 | elapsed:   24.8s remaining:   20.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Intensity] RCé˜ˆå€¼ t=5 å®Œæˆ (ç°‡æ•°=5, d_cut=10)\n",
      "   [Intensity] RCé˜ˆå€¼ t=6 å®Œæˆ (ç°‡æ•°=8, d_cut=9)\n",
      "   [Intensity] RCé˜ˆå€¼ t=7 å®Œæˆ (ç°‡æ•°=12, d_cut=8)\n",
      "   [Intensity] RCé˜ˆå€¼ t=8 å®Œæˆ (ç°‡æ•°=14, d_cut=7)\n",
      "   [Intensity] RCé˜ˆå€¼ t=9 å®Œæˆ (ç°‡æ•°=16, d_cut=6)\n",
      "   [Intensity] RCé˜ˆå€¼ t=10 å®Œæˆ (ç°‡æ•°=19, d_cut=5)\n",
      "   [Intensity] RCé˜ˆå€¼ t=11 å®Œæˆ (ç°‡æ•°=28, d_cut=4)\n",
      "   [Intensity] RCé˜ˆå€¼ t=12 å®Œæˆ (ç°‡æ•°=35, d_cut=3)\n",
      "   [Intensity] RCé˜ˆå€¼ t=13 å®Œæˆ (ç°‡æ•°=40, d_cut=2)\n",
      "   [Intensity] RCé˜ˆå€¼ t=14 å®Œæˆ (ç°‡æ•°=46, d_cut=1)\n",
      "   [Intensity] RCé˜ˆå€¼ t=15 å®Œæˆ (ç°‡æ•°=49, d_cut=0)\n",
      "\n",
      "âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Done   9 out of  11 | elapsed:   25.2s remaining:    5.5s\n",
      "[Parallel(n_jobs=-3)]: Done  11 out of  11 | elapsed:   25.3s finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, leaves_list\n",
    "from scipy.spatial.distance import squareform\n",
    "from pathlib import Path\n",
    "import colorsys\n",
    "import warnings\n",
    "from joblib import Parallel, delayed  # âœ… å¹¶è¡Œ\n",
    "\n",
    "# ç¦ç”¨è­¦å‘Š\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. åŸºç¡€é…ç½®\n",
    "# ==========================================\n",
    "current_dir = Path.cwd()\n",
    "data_dir = current_dir.parent / \"data\"\n",
    "base_output_dir = data_dir / \"4-2-group_overlapping_cluster\"\n",
    "base_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_times_black_font():\n",
    "    return FontProperties(family='Times New Roman', size=26, weight='black')\n",
    "\n",
    "T_BLACK = get_times_black_font()\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'font.weight': 'black',\n",
    "    'axes.labelweight': 'black',\n",
    "    'axes.titleweight': 'black',\n",
    "    'axes.unicode_minus': False,\n",
    "    'figure.dpi': 300\n",
    "})\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.rcParams['axes.axisbelow'] = False\n",
    "\n",
    "def lighten_color_slightly(hex_color: str) -> str:\n",
    "    r, g, b = plt.cm.colors.to_rgb(hex_color)\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "    return plt.cm.colors.to_hex(colorsys.hls_to_rgb(h, min(1.0, l + 0.12), s * 0.88))\n",
    "\n",
    "def get_palette(k: int) -> list:\n",
    "    colors = ['#1f77b4', '#2ca02c', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "    return (colors * (k // len(colors) + 1))[:k]\n",
    "\n",
    "# âœ… å·¦å¼€å³é—­é˜ˆå€¼ï¼šOverlap âˆˆ (t-1, +âˆ]ï¼Œå¯¹æ•´æ•° overlap ç­‰ä»·äº Overlap >= t\n",
    "THRESHOLDS_RC = list(range(5, 16))  # 5..15\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ•°æ®å¤„ç†ï¼ˆä¿æŒä¸å˜ï¼šå…±è¯†çŸ©é˜µ=é‡å æ¬¡æ•°ï¼‰\n",
    "# ==========================================\n",
    "def build_overlap_matrix(input_file: Path) -> tuple[pd.DataFrame, int]:\n",
    "    if not input_file.exists():\n",
    "        return None, 0\n",
    "\n",
    "    df = pd.read_csv(input_file, encoding='utf-8-sig')\n",
    "\n",
    "    if 'L2æ”¿ç­–' not in df.columns and 'L2æ”¿ç­–ä¸­æ–‡å' in df.columns:\n",
    "        df.rename(columns={'L2æ”¿ç­–ä¸­æ–‡å': 'L2æ”¿ç­–'}, inplace=True)\n",
    "\n",
    "    def safe_col(d, name):\n",
    "        return d[name].iloc[:, 0] if isinstance(d[name], pd.DataFrame) else d[name]\n",
    "\n",
    "    df_clean = pd.DataFrame({\n",
    "        'P': safe_col(df, 'L2æ”¿ç­–').astype(str),\n",
    "        'C': safe_col(df, 'å›½å®¶').astype(str),\n",
    "        'G': safe_col(df, 'èšç±»ID')\n",
    "    }).drop_duplicates()\n",
    "\n",
    "    policies = sorted(df_clean['P'].unique())\n",
    "    countries = sorted(df_clean['C'].unique())\n",
    "\n",
    "    res_map = {\n",
    "        p: dict(zip(\n",
    "            df_clean[df_clean['P'] == p]['C'],\n",
    "            df_clean[df_clean['P'] == p]['G']\n",
    "        )) for p in policies\n",
    "    }\n",
    "\n",
    "    n = len(countries)\n",
    "    matrix = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    for i, c1 in enumerate(countries):\n",
    "        for j in range(i, n):\n",
    "            c2 = countries[j]\n",
    "            common = sum(\n",
    "                1 for p in policies\n",
    "                if c1 in res_map[p]\n",
    "                and c2 in res_map[p]\n",
    "                and (res_map[p][c1] == res_map[p][c2] if i != j else True)\n",
    "            )\n",
    "            matrix[i, j] = matrix[j, i] = common\n",
    "\n",
    "    return pd.DataFrame(matrix, index=countries, columns=countries), len(policies)\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ ¸å¿ƒç»˜å›¾ï¼ˆå¯è§†åŒ–ä¸å˜ï¼‰- æ”¹ä¸ºâ€œé˜ˆå€¼åˆ‡å‰²â€\n",
    "#    âœ… æ”¹åŠ¨ç‚¹ï¼šé˜ˆå€¼é‡‡ç”¨ å·¦å¼€å³é—­ => Overlap >= t\n",
    "# ==========================================\n",
    "def draw_and_save_threshold(\n",
    "    threshold_rc: int,\n",
    "    Z: np.ndarray,\n",
    "    df_o: pd.DataFrame,\n",
    "    max_o: int,\n",
    "    output_folder: Path,\n",
    "    type_name: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    å•ä¸ªé˜ˆå€¼å¤„ç†é€»è¾‘ï¼šé˜ˆå€¼åˆ‡å‰²(Complete Linkage) -> é¢œè‰² -> å¯¼å‡ºCSV -> ç»˜å›¾\n",
    "\n",
    "    âœ… å·¦å¼€å³é—­ï¼ˆå¯¹æ•´æ•° overlapï¼‰ï¼šOverlap âˆˆ (t-1, +âˆ]  <=>  Overlap >= t\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # âœ… å·¦å¼€å³é—­ï¼šOverlap >= t\n",
    "        min_overlap = threshold_rc\n",
    "\n",
    "        # distance = max_o - overlap\n",
    "        # overlap >= min_overlap  <=>  distance <= max_o - min_overlap\n",
    "        d_cut = max_o - min_overlap\n",
    "\n",
    "        # 1) é˜ˆå€¼åˆ‡å‰²ï¼šcriterion='distance'\n",
    "        n = df_o.shape[0]\n",
    "        if min_overlap > max_o:\n",
    "            # ä¸å¯èƒ½æ»¡è¶³ overlap >= min_overlapï¼šå…¨éƒ¨å•ç‚¹ç°‡\n",
    "            labels_raw = np.arange(1, n + 1, dtype=int)\n",
    "        else:\n",
    "            # SciPy çš„ distance cut æ˜¯ <= tï¼ˆå³é—­ï¼‰ï¼Œç¬¦åˆâ€œå³é—­â€è¦æ±‚\n",
    "            labels_raw = fcluster(Z, t=float(d_cut), criterion='distance')\n",
    "\n",
    "        # é‡æ–°æ˜ å°„ä¸º 0..k-1ï¼ˆä¿è¯é…è‰²ä¸è¾¹æ¡†é€»è¾‘ç¨³å®šï¼‰\n",
    "        uniq = sorted(set(labels_raw.tolist()))\n",
    "        relabel = {old: i for i, old in enumerate(uniq)}\n",
    "        labels = np.array([relabel[x] for x in labels_raw], dtype=int)\n",
    "        k = len(uniq)\n",
    "\n",
    "        palette = get_palette(max(k, 1))\n",
    "        row_colors = pd.Series(\n",
    "            [lighten_color_slightly(palette[l % len(palette)]) for l in labels],\n",
    "            index=df_o.index\n",
    "        )\n",
    "\n",
    "        # 2) å¯¼å‡º CSVï¼ˆåŠ å…¥æ ¸å¿ƒ-è¾¹ç¼˜ + é˜ˆå€¼æ ¡éªŒä¿¡æ¯ï¼‰\n",
    "        csv_filename = f'{type_name}_consensus_threshold_rc={threshold_rc}.csv'\n",
    "\n",
    "        groups = {}\n",
    "        for country, cid in zip(df_o.index.tolist(), labels.astype(int).tolist()):\n",
    "            groups.setdefault(cid, []).append(country)\n",
    "\n",
    "        rows = []\n",
    "        for cid in sorted(groups):\n",
    "            members = sorted(groups[cid])\n",
    "            sub = df_o.loc[members, members].values\n",
    "\n",
    "            if len(members) > 1:\n",
    "                off = sub[~np.eye(len(members), dtype=bool)]\n",
    "                min_off = int(np.min(off))\n",
    "                mean_off = float(np.mean(off))\n",
    "                is_core = True\n",
    "                # Complete Linkage + distance cut ç†è®ºä¸Šåº”æ»¡è¶³ï¼šç°‡å†…ä»»æ„ä¸¤å›½ overlap >= min_overlap\n",
    "                pass_check = (min_off >= min_overlap)\n",
    "            else:\n",
    "                min_off = np.nan\n",
    "                mean_off = np.nan\n",
    "                is_core = False\n",
    "                pass_check = True  # å•ç‚¹æ— â€œä»»æ„ä¸¤å›½â€çº¦æŸ\n",
    "\n",
    "            rows.append({\n",
    "                'Thresholdæ¡ä»¶': f'Overlap âˆˆ ({threshold_rc - 1}, +âˆ]  (ç­‰ä»·: Overlap â‰¥ {threshold_rc})',\n",
    "                'ç­‰ä»·æœ€å°é‡å ': min_overlap,\n",
    "                'å…±è¯†ID': cid + 1,\n",
    "                'æ ¸å¿ƒ-è¾¹ç¼˜': 'Core' if is_core else 'Periphery',\n",
    "                'é…è‰²HEX': lighten_color_slightly(palette[cid % len(palette)]),\n",
    "                'å›½å®¶æ•°': len(members),\n",
    "                'å›½å®¶åˆ—è¡¨': ', '.join(members),\n",
    "                'ç°‡å†…æœ€å°é‡å (éå¯¹è§’)': min_off,\n",
    "                'ç°‡å†…å¹³å‡é‡å (éå¯¹è§’)': (f'{mean_off:.2f}' if len(members) > 1 else ''),\n",
    "                'é˜ˆå€¼æ ¡éªŒ(æœ€å°é‡å >=ç­‰ä»·é—¨æ§›)': pass_check\n",
    "            })\n",
    "\n",
    "        pd.DataFrame(rows).to_csv(output_folder / csv_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        # 3) ç»˜å›¾ï¼ˆä¸¥æ ¼æ²¿ç”¨ä½ çš„ Style Referenceï¼Œä¸æ”¹ä»»ä½•å¯è§†åŒ–å‚æ•°ï¼‰\n",
    "        png_filename = f'{type_name}_consensus_threshold_rc={threshold_rc}.png'\n",
    "        size = max(18, len(df_o) * 0.72)\n",
    "\n",
    "        g = sns.clustermap(\n",
    "            df_o,\n",
    "            row_linkage=Z,\n",
    "            col_linkage=Z,\n",
    "            cmap='RdYlBu_r',\n",
    "            row_colors=row_colors,\n",
    "            col_colors=row_colors,\n",
    "            dendrogram_ratio=0.1,\n",
    "            linewidths=0.5,\n",
    "            figsize=(size, size),\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            vmin=0,\n",
    "            vmax=max_o,\n",
    "            annot_kws={'fontsize': 28, 'weight': 'black', 'fontproperties': T_BLACK},\n",
    "            cbar_kws={'ticks': [0, int(max_o*0.25), int(max_o*0.5), int(max_o*0.75), max_o]},\n",
    "            tree_kws={'linewidths': 6}\n",
    "        )\n",
    "\n",
    "        # è°ƒæ•´ Colorbar ä½ç½®\n",
    "        hm_pos = g.ax_heatmap.get_position()\n",
    "        pad = 0.04; cbar_w = 0.018; cbar_h = hm_pos.height * 0.78\n",
    "        cbar_y = hm_pos.y0 + hm_pos.height * 0.11\n",
    "        g.cax.set_position([hm_pos.x1 + pad, cbar_y, cbar_w, cbar_h])\n",
    "        g.cax.yaxis.set_label_position('right')\n",
    "\n",
    "        g.cax.set_axisbelow(False)\n",
    "        for coll in g.cax.collections: coll.set_zorder(0)\n",
    "        g.cax.yaxis.set_ticks_position('both')\n",
    "        g.cax.tick_params(axis='y', direction='in', length=12, width=3, colors='black', left=True, right=True, pad=6)\n",
    "\n",
    "        for t in g.cax.yaxis.get_major_ticks():\n",
    "            for ln in (t.tick1line, t.tick2line):\n",
    "                ln.set_zorder(10); ln.set_clip_on(False)\n",
    "        for lab in g.cax.get_yticklabels(): lab.set_zorder(11)\n",
    "\n",
    "        g.cax.set_ylabel(f'Overlap Count ({type_name})', fontproperties=T_BLACK, fontsize=32, weight='black', labelpad=20)\n",
    "        plt.setp(g.cax.get_yticklabels(), fontproperties=T_BLACK, fontsize=28, weight='black')\n",
    "\n",
    "        # è½´æ ‡ç­¾\n",
    "        for lab in g.ax_heatmap.get_xticklabels():\n",
    "            lab.set_fontproperties(T_BLACK); lab.set_rotation(45); lab.set_ha('right')\n",
    "        for lab in g.ax_heatmap.get_yticklabels():\n",
    "            lab.set_fontproperties(T_BLACK); lab.set_rotation(0)\n",
    "\n",
    "        # ç²—è¾¹æ¡†\n",
    "        ax = g.ax_heatmap\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True); spine.set_linewidth(4); spine.set_edgecolor('black')\n",
    "\n",
    "        # ç»˜åˆ¶èšç±»é»‘æ¡†ï¼ˆåŸºäºå½“å‰é˜ˆå€¼æ ‡ç­¾ï¼‰\n",
    "        order = g.dendrogram_row.reordered_ind\n",
    "        ordered_labels = labels[order].astype(int).tolist()\n",
    "        ranges = {}\n",
    "        for idx, cid in enumerate(ordered_labels):\n",
    "            if cid not in ranges: ranges[cid] = (idx, idx)\n",
    "            else: ranges[cid] = (ranges[cid][0], idx)\n",
    "\n",
    "        boundary_color = '#4D4D4D'\n",
    "        for _, (s, e) in ranges.items():\n",
    "            ax.plot([s, s], [s, e + 1], color=boundary_color, linewidth=6, clip_on=False, zorder=20)\n",
    "            ax.plot([e + 1, e + 1], [s, e + 1], color=boundary_color, linewidth=6, clip_on=False, zorder=20)\n",
    "            ax.plot([s, e + 1], [s, s], color=boundary_color, linewidth=6, clip_on=False, zorder=20)\n",
    "            ax.plot([s, e + 1], [e + 1, e + 1], color=boundary_color, linewidth=6, clip_on=False, zorder=20)\n",
    "\n",
    "        plt.savefig(output_folder / png_filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close('all')\n",
    "\n",
    "        return f\"[{type_name}] RCé˜ˆå€¼ t={threshold_rc} å®Œæˆ (ç°‡æ•°={k}, d_cut={d_cut})\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[{type_name}] RCé˜ˆå€¼ t={threshold_rc} å¤±è´¥: {str(e)}\"\n",
    "\n",
    "# ==========================================\n",
    "# 4. Complete Linkage + è·ç¦»è½¬æ¢ + å¼ºåˆ¶å·¦åæ’åº\n",
    "# ==========================================\n",
    "def compute_force_left_linkage_complete(df_o: pd.DataFrame, max_o: int, type_name: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å…³é”®æ”¹é€ ç‚¹ï¼š\n",
    "    - distance = max_o - overlap\n",
    "    - linkage(method='complete')ï¼Œä¿è¯ç°‡å†…ä»»æ„ä¸¤ç‚¹è·ç¦»<=cut\n",
    "      => ç­‰ä»·äº overlap >= max_o - cut\n",
    "    \"\"\"\n",
    "    print(f\"   æ­£åœ¨è®¡ç®—èšç±»æ ‘ï¼ˆComplete Linkageï¼‰å¹¶æ£€æŸ¥é‡å¿ƒ...\")\n",
    "\n",
    "    dist = (max_o - df_o).astype(float)\n",
    "    np.fill_diagonal(dist.values, 0.0)  # è·ç¦»çŸ©é˜µå¯¹è§’å¼ºåˆ¶ä¸º0\n",
    "    condensed = squareform(dist.values, checks=False)\n",
    "\n",
    "    Z = linkage(condensed, method='complete')\n",
    "\n",
    "    weights = df_o.sum(axis=1).values\n",
    "\n",
    "    # å±€éƒ¨æ’åºï¼ˆç»´æŒä½ åŸæ¥çš„â€œå·¦åâ€é£æ ¼ï¼‰\n",
    "    w_map = {i: v for i, v in enumerate(weights)}\n",
    "    for i, (c1, c2, _, _) in enumerate(Z):\n",
    "        c1, c2 = int(c1), int(c2)\n",
    "        if w_map.get(c1, 0) < w_map.get(c2, 0):\n",
    "            Z[i, 0], Z[i, 1] = c2, c1\n",
    "        w_map[len(weights) + i] = w_map.get(c1, 0) + w_map.get(c2, 0)\n",
    "\n",
    "    # å…¨å±€é‡å¿ƒç¿»è½¬\n",
    "    leaves = leaves_list(Z)\n",
    "    mid = len(leaves) // 2\n",
    "    left_weight = weights[leaves[:mid]].sum()\n",
    "    right_weight = weights[leaves[mid:]].sum()\n",
    "\n",
    "    print(f\"   [{type_name} é‡å¿ƒ] å·¦: {left_weight:.0f} | å³: {right_weight:.0f}\")\n",
    "\n",
    "    if right_weight > left_weight:\n",
    "        print(f\"   >>> âš ï¸ æ£€æµ‹åˆ°é‡å¿ƒåœ¨å³ï¼Œæ‰§è¡Œé•œåƒç¿»è½¬...\")\n",
    "        Z[:, [0, 1]] = Z[:, [1, 0]]\n",
    "    else:\n",
    "        print(f\"   >>> âœ… é‡å¿ƒæ­£å¸¸ã€‚\")\n",
    "\n",
    "    return Z\n",
    "\n",
    "# ==========================================\n",
    "# 5. ä¸»æµç¨‹ï¼ˆé˜ˆå€¼æ‰«æï¼šå·¦å¼€å³é—­ï¼‰\n",
    "# ==========================================\n",
    "def process_type_pipeline(file_path: Path, type_name: str):\n",
    "    print(f\"\\n============================================\")\n",
    "    print(f\" ğŸš€ å¼€å§‹å¤„ç†: {type_name}\")\n",
    "    print(f\"============================================\")\n",
    "\n",
    "    type_output_dir = base_output_dir / type_name\n",
    "    type_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"   ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: {type_output_dir}\")\n",
    "\n",
    "    df_o, max_o = build_overlap_matrix(file_path)\n",
    "    if df_o is None:\n",
    "        print(f\"   âŒ è·³è¿‡ï¼Œæ–‡ä»¶æœªæ‰¾åˆ°\")\n",
    "        return\n",
    "\n",
    "    # âœ… Complete Linkage + è·ç¦»è½¬æ¢\n",
    "    Z = compute_force_left_linkage_complete(df_o, max_o, type_name)\n",
    "\n",
    "    # âœ… é˜ˆå€¼åˆ—è¡¨ï¼ˆå·¦å¼€å³é—­ï¼‰\n",
    "    print(f\"   ğŸ¯ å¾…å¤„ç†é˜ˆå€¼RC ({len(THRESHOLDS_RC)}ä¸ª): {THRESHOLDS_RC}\")\n",
    "    print(f\"      è§£é‡Šï¼šæ¯ä¸ª t è¡¨ç¤º Overlap âˆˆ (t-1, +âˆ]ï¼ˆç­‰ä»· Overlap â‰¥ tï¼‰\")\n",
    "\n",
    "    results = Parallel(n_jobs=-3, verbose=5)(\n",
    "        delayed(draw_and_save_threshold)(\n",
    "            t_rc, Z, df_o, max_o, type_output_dir, type_name\n",
    "        ) for t_rc in THRESHOLDS_RC\n",
    "    )\n",
    "\n",
    "    for res in results:\n",
    "        print(f\"   {res}\")\n",
    "\n",
    "def main():\n",
    "    files = {\n",
    "        \"Breadth\": data_dir / \"3-1-L2_Policy_Clustering_Breadth.csv\",\n",
    "        \"Intensity\": data_dir / \"3-1-L2_Policy_Clustering_Intensity.csv\"\n",
    "    }\n",
    "\n",
    "    for type_name, path in files.items():\n",
    "        process_type_pipeline(path, type_name)\n",
    "\n",
    "    print(\"\\nâœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0a32d",
   "metadata": {},
   "source": [
    "### ä¸åŒèšç±»çš„å…±è¯†å›½å®¶è¯†åˆ«è¡¨æ ¼æ±‡æ€»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c21bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.cluster.hierarchy import fcluster, dendrogram as scipy_dendrogram\n",
    "\n",
    "# è·¯å¾„é…ç½®\n",
    "current_dir = Path.cwd()\n",
    "data_dir = current_dir.parent / \"data\"\n",
    "mapping_output_path = data_dir / \"4-2-Consensus_Policy_Cluster_Mapping.csv\"\n",
    "stats_output_path = data_dir / \"4-2-Consensus_Cluster_Statistics.csv\"\n",
    "\n",
    "records = []\n",
    "cluster_stats = []\n",
    "\n",
    "# æ ¸å¿ƒé€»è¾‘ï¼šéå†æ‰€æœ‰Kå€¼ç”Ÿæˆæ˜ å°„\n",
    "for k_value in valid_k_values:\n",
    "    # è·å–åŸå§‹èšç±»é¡ºåº\n",
    "    temp_dend = scipy_dendrogram(row_linkage, no_plot=True)\n",
    "    original_order = temp_dend['leaves']\n",
    "    \n",
    "    # åè½¬é¡ºåº\n",
    "    reversed_order = original_order[::-1]\n",
    "    \n",
    "    # è·å–åŸå§‹èšç±»æ ‡ç­¾ï¼ˆ0-basedï¼‰\n",
    "    original_labels = fcluster(row_linkage, t=k_value, criterion='maxclust') - 1\n",
    "    reordered_original_labels = [original_labels[i] for i in reversed_order]\n",
    "    \n",
    "    # åˆ›å»ºæ–°çš„èšç±»IDæ˜ å°„ï¼šæŒ‰åè½¬åçš„é¡ºåºä»1å¼€å§‹é‡æ–°ç¼–å·\n",
    "    new_cluster_id = 1\n",
    "    old_to_new_map = {}\n",
    "    \n",
    "    for old_label in reordered_original_labels:\n",
    "        if old_label not in old_to_new_map:\n",
    "            old_to_new_map[old_label] = new_cluster_id\n",
    "            new_cluster_id += 1\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªå›½å®¶åˆ†é…æ–°çš„èšç±»ID\n",
    "    cluster_counts = {}\n",
    "    for country, old_cluster_id in zip(countries, original_labels):\n",
    "        new_cid = old_to_new_map[old_cluster_id]  # ä½¿ç”¨æ˜ å°„åçš„ID\n",
    "        records.append({\"Kå€¼\": k_value, \"å…±è¯†èšç±»ID\": new_cid, \"å›½å®¶\": country})\n",
    "        cluster_counts[new_cid] = cluster_counts.get(new_cid, 0) + 1\n",
    "    \n",
    "    # ç»Ÿè®¡æ¯ä¸ªèšç±»çš„å›½å®¶æ•°\n",
    "    for cid, count in sorted(cluster_counts.items()):\n",
    "        cluster_stats.append({\"Kå€¼\": k_value, \"å…±è¯†èšç±»ID\": cid, \"å›½å®¶æ•°\": count})\n",
    "\n",
    "# æ˜ å°„è¡¨\n",
    "df_mapping = pd.DataFrame(records).sort_values([\"Kå€¼\", \"å…±è¯†èšç±»ID\", \"å›½å®¶\"]).reset_index(drop=True)\n",
    "df_mapping.to_csv(mapping_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ç»Ÿè®¡è¡¨\n",
    "df_stats = pd.DataFrame(cluster_stats)\n",
    "df_stats.to_csv(stats_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ“ æ˜ å°„è¡¨å·²ä¿å­˜ï¼š{mapping_output_path}\")\n",
    "print(f\"âœ“ ç»Ÿè®¡è¡¨å·²ä¿å­˜ï¼š{stats_output_path}\")\n",
    "print(f\"æ€»è®°å½•æ•°ï¼š{len(df_mapping)} | Kå€¼èŒƒå›´ï¼š{valid_k_values}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
