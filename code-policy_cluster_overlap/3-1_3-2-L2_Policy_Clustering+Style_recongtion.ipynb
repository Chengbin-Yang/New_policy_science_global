{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65d9f19",
   "metadata": {},
   "source": [
    "### Cluster generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44676d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®ç›®å½•: f:\\Desktop\\ç§‘ç ”é¡¹ç›®\\1.è´Ÿè´£ç§‘ç ”é¡¹ç›®\\Climate Policy\\CAMPF_Supplementary\\data\n",
      "æ­£åœ¨æ‰§è¡Œ Breadth èšç±» (2005-2023)...\n",
      "âœ… å·²ä¿å­˜: 3-1-L2_Policy_Clustering_Breadth.csv\n",
      "æ­£åœ¨æ‰§è¡Œ Intensity èšç±» (2005-2023)...\n",
      "âœ… å·²ä¿å­˜: 3-1-L2_Policy_Clustering_Intensity.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cell 1: èšç±»åˆ†æ (Clustering)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Š\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. æ ¸å¿ƒèšç±»ç®—æ³•\n",
    "def auto_select_k_and_cluster(X_data: np.ndarray, max_k: int = 10) -> np.ndarray:\n",
    "    \"\"\"è‡ªåŠ¨é€‰æ‹©æœ€ä½³Kå€¼å¹¶è¿”å›èšç±»æ ‡ç­¾ (åŸºäºè‚˜éƒ¨æ³•åˆ™-æœ€å¤§é™å¹…)\"\"\"\n",
    "    n_samples = len(X_data)\n",
    "    if n_samples < 3:\n",
    "        return np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    Z = linkage(X_data, method='ward')\n",
    "    \n",
    "    wcss_list = []\n",
    "    valid_ks = list(range(2, min(max_k, n_samples) + 1))\n",
    "    \n",
    "    for k in valid_ks:\n",
    "        labels = fcluster(Z, k, criterion='maxclust')\n",
    "        wcss = 0\n",
    "        for i in range(1, k + 1):\n",
    "            cluster_points = X_data[labels == i]\n",
    "            if len(cluster_points) > 0:\n",
    "                center = cluster_points.mean(axis=0)\n",
    "                wcss += np.sum((cluster_points - center) ** 2)\n",
    "        wcss_list.append(wcss)\n",
    "    \n",
    "    if len(wcss_list) < 2:\n",
    "        best_k = valid_ks[0]\n",
    "    else:\n",
    "        deltas = [wcss_list[i] - wcss_list[i+1] for i in range(len(wcss_list)-1)]\n",
    "        best_idx = np.argmax(deltas)\n",
    "        best_k = valid_ks[best_idx + 1]\n",
    "    \n",
    "    return fcluster(Z, best_k, criterion='maxclust') - 1\n",
    "\n",
    "def run_clustering_single_metric(df_source: pd.DataFrame, l2_name: str, need_scale: bool = False) -> pd.Series:\n",
    "    \"\"\"å¤„ç†å•æ¡L2æ”¿ç­–\"\"\"\n",
    "    # === ä¿®æ”¹ç‚¹1ï¼šä¸¥æ ¼é™åˆ¶å¹´ä»½ 2005-2023 ===\n",
    "    df = df_source[(df_source['TIME_PERIOD'] >= 2005) & (df_source['TIME_PERIOD'] <= 2023)]\n",
    "    \n",
    "    X = df.set_index(['REF_AREA', 'TIME_PERIOD'])[l2_name].unstack().fillna(0)\n",
    "    \n",
    "    if X.empty:\n",
    "        return pd.Series(dtype=int)\n",
    "\n",
    "    if need_scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        if X.values.max() == X.values.min():\n",
    "            X_vals = X.values\n",
    "        else:\n",
    "            X_vals = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_vals = X.values\n",
    "        \n",
    "    labels = auto_select_k_and_cluster(X_vals)\n",
    "    return pd.Series(labels, index=X.index, name='ClusterID')\n",
    "\n",
    "# 2. æ•°æ® IO\n",
    "def load_data(data_dir: Path):\n",
    "    df_b = pd.read_parquet(data_dir / \"2-1-country_breadth.parquet\")\n",
    "    df_i = pd.read_parquet(data_dir / \"2-1-country_intensity.parquet\")\n",
    "    l2_list = [c for c in df_b.columns if c not in {'REF_AREA', 'TIME_PERIOD'}]\n",
    "    return df_b, df_i, l2_list\n",
    "\n",
    "def save_clustered_data(cluster_results: Dict[str, pd.Series], df_source: pd.DataFrame, output_path: Path):\n",
    "    records = []\n",
    "    # åŒæ ·ç¡®ä¿æºæ•°æ®ä¹Ÿæ˜¯ 2005-2023ï¼Œä»¥ä¾¿ä¿å­˜æ—¶ä¸€è‡´\n",
    "    df_source_filtered = df_source[(df_source['TIME_PERIOD'] >= 2005) & (df_source['TIME_PERIOD'] <= 2023)]\n",
    "    \n",
    "    for l2, clusters in cluster_results.items():\n",
    "        if clusters.empty: continue\n",
    "        \n",
    "        sub = df_source_filtered[['REF_AREA', 'TIME_PERIOD', l2]].copy()\n",
    "        sub.columns = ['å›½å®¶', 'å¹´ä»½', 'å æ¯”'] # ç»Ÿä¸€åˆ—å\n",
    "        sub['èšç±»ID'] = sub['å›½å®¶'].map(clusters)\n",
    "        sub['L2æ”¿ç­–'] = l2\n",
    "        sub['L2æ”¿ç­–ä¸­æ–‡å'] = l2 # å ä½ï¼Œç¡®ä¿åˆ—å­˜åœ¨\n",
    "        \n",
    "        sub = sub.dropna(subset=['èšç±»ID'])\n",
    "        sub['èšç±»ID'] = sub['èšç±»ID'].astype(int)\n",
    "        records.append(sub)\n",
    "        \n",
    "    if records:\n",
    "        final_df = pd.concat(records)\n",
    "        final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ… å·²ä¿å­˜: {output_path.name}\")\n",
    "\n",
    "# 3. æ‰§è¡Œ\n",
    "base_dir = Path.cwd().parent\n",
    "data_dir = base_dir / \"data\" if (base_dir / \"data\").exists() else Path.cwd() / \"data\"\n",
    "print(f\"æ•°æ®ç›®å½•: {data_dir}\")\n",
    "\n",
    "df_b, df_i, l2_list = load_data(data_dir)\n",
    "\n",
    "print(\"æ­£åœ¨æ‰§è¡Œ Breadth èšç±» (2005-2023)...\")\n",
    "b_results = {l2: run_clustering_single_metric(df_b, l2, need_scale=False) for l2 in l2_list}\n",
    "save_clustered_data(b_results, df_b, data_dir / \"3-1-L2_Policy_Clustering_Breadth.csv\")\n",
    "\n",
    "print(\"æ­£åœ¨æ‰§è¡Œ Intensity èšç±» (2005-2023)...\")\n",
    "i_results = {l2: run_clustering_single_metric(df_i, l2, need_scale=True) for l2 in l2_list}\n",
    "save_clustered_data(i_results, df_i, data_dir / \"3-1-L2_Policy_Clustering_Intensity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f8dd0",
   "metadata": {},
   "source": [
    "### Table generateï¼ˆç”Ÿæˆå¯ç¼–è¾‘ç‰ˆçš„PPTï¼Œå’Œè€å¸ˆè®¨è®ºä¿®æ”¹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a56c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹æ‰§è¡Œï¼šç”Ÿæˆæ•°æ® -> å…¨é‡æ’åº -> é”å®š ID\n",
      "âœ… CSV å·²ä¿å­˜: 3-2-Automated_Recognition_Mode.csv\n",
      "âœ… é¡ºåºå·²ä¸¥æ ¼é”å®šè‡³ JSON: 3-2-fixed_ppt_order.json\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cell 1: ç”Ÿæˆå…¨é‡æ•°æ® -> æ¨¡æ‹Ÿ Cell 3 æ’åº -> é”å®šé¡ºåº\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# --- è·¯å¾„é…ç½® ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "OUTPUT_CSV = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "OUTPUT_JSON = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"3-2-fixed_ppt_order.json\"\n",
    "\n",
    "# --- 1. ç‰¹å¾è®¡ç®— (æ¥è‡ª Cell 12) ---\n",
    "def calculate_cluster_metrics(df_sub: pd.DataFrame, is_intensity: bool) -> Dict[int, Dict[str, Any]]:\n",
    "    th_high = 6.0 if is_intensity else 0.6\n",
    "    th_med = 3.0 if is_intensity else 0.3\n",
    "    th_slope = 1.5 if is_intensity else 0.15\n",
    "\n",
    "    df_calc = df_sub[(df_sub['å¹´ä»½'] >= 2005) & (df_sub['å¹´ä»½'] <= 2023)]\n",
    "    # æ³¨æ„ï¼šgroupby é»˜è®¤ä¼šæŒ‰ key æ’åºï¼Œè¿™é‡Œä¿è¯äº† trends çš„ index æ˜¯æœ‰åºçš„\n",
    "    trends = df_calc.groupby(['èšç±»ID', 'å¹´ä»½'])['å æ¯”'].mean().unstack()\n",
    "    \n",
    "    features = {}\n",
    "    for cid, row in trends.iterrows():\n",
    "        ts = row.dropna()\n",
    "        if len(ts) < 2:\n",
    "            features[cid] = {'Starting': 'Low', 'Trend': 'Stable', 'Ending': 'Low', 'MeanStart': 0}\n",
    "            continue\n",
    "            \n",
    "        start_val = ts.iloc[:3].mean()\n",
    "        end_val = ts.iloc[-3].mean()\n",
    "        slope = end_val - start_val\n",
    "        std = ts.std()\n",
    "        \n",
    "        if start_val > th_high: s_lbl = 'High'\n",
    "        elif start_val > th_med: s_lbl = 'Medium'\n",
    "        else: s_lbl = 'Low'\n",
    "        \n",
    "        if end_val > th_high: e_lbl = 'High'\n",
    "        elif end_val > th_med: e_lbl = 'Medium'\n",
    "        else: e_lbl = 'Low'\n",
    "        \n",
    "        if slope > th_slope: t_lbl = 'Rise'\n",
    "        elif slope < -0.05: t_lbl = 'Decline'\n",
    "        elif std > (1.0 if is_intensity else 0.1): t_lbl = 'Fluctuate'\n",
    "        else: t_lbl = 'Stable'\n",
    "        \n",
    "        features[cid] = {\n",
    "            'Starting': s_lbl, 'Trend': t_lbl, 'Ending': e_lbl, 'MeanStart': start_val\n",
    "        }\n",
    "    return features\n",
    "\n",
    "# --- 2. æ’åºé€»è¾‘ (100% å¤åˆ» Cell 3) ---\n",
    "def sort_clusters_original_logic(sort_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"å®Œå…¨ä½¿ç”¨ Cell 3 çš„æ’åº Key é€»è¾‘\"\"\"\n",
    "    orders = {\n",
    "        'Starting': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "        'Ending': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "        'Trend': {'Rise': 0, 'Stable': 1, 'Fluctuate': 2, 'Decline': 3}\n",
    "    }\n",
    "    \n",
    "    df = sort_df.copy()\n",
    "    df['sort_key'] = df.apply(lambda r: (\n",
    "        orders['Starting'].get(r['Starting'], 99),\n",
    "        orders['Ending'].get(r['Ending'], 99),\n",
    "        orders['Trend'].get(r['Trend'], 99),\n",
    "        r.get('MeanStart', 0)\n",
    "    ), axis=1)\n",
    "    \n",
    "    return df.sort_values('sort_key')\n",
    "\n",
    "# --- 3. ä¸»æ‰§è¡Œæµç¨‹ ---\n",
    "def process_and_lock_strict():\n",
    "    print(\"ğŸš€ å¼€å§‹æ‰§è¡Œï¼šç”Ÿæˆæ•°æ® -> å…¨é‡æ’åº -> é”å®š ID\")\n",
    "    \n",
    "    all_records = []\n",
    "    \n",
    "    # === æ­¥éª¤ A: ç”Ÿæˆæ‰€æœ‰æ•°æ® (æ¨¡æ‹Ÿ Cell 12 çš„å†™å…¥è¿‡ç¨‹) ===\n",
    "    tasks = [\n",
    "        (\"3-1-L2_Policy_Clustering_Breadth.csv\", \"Breadth\", False),\n",
    "        (\"3-1-L2_Policy_Clustering_Intensity.csv\", \"Intensity\", True)\n",
    "    ]\n",
    "\n",
    "    for fname, m_type, is_intensity in tasks:\n",
    "        fpath = DATA_DIR / fname\n",
    "        if not fpath.exists():\n",
    "            print(f\"âš ï¸ è·³è¿‡æ–‡ä»¶: {fname}\")\n",
    "            continue\n",
    "\n",
    "        df_raw = pd.read_csv(fpath, encoding='utf-8-sig')\n",
    "        \n",
    "        # ä¸¥æ ¼æŒ‰ç…§ Cell 12 çš„å¾ªç¯é¡ºåºç”Ÿæˆè®°å½•\n",
    "        for l2 in df_raw['L2æ”¿ç­–ä¸­æ–‡å'].dropna().unique():\n",
    "            l2_data = df_raw[df_raw['L2æ”¿ç­–ä¸­æ–‡å'] == l2]\n",
    "            feats_map = calculate_cluster_metrics(l2_data, is_intensity)\n",
    "            \n",
    "            # æ¨¡æ‹Ÿ Cell 12 çš„ iterrows å±•å¼€\n",
    "            for _, row in l2_data.iterrows():\n",
    "                cid = row['èšç±»ID']\n",
    "                if cid in feats_map:\n",
    "                    f = feats_map[cid]\n",
    "                    all_records.append({\n",
    "                        'L2æ”¿ç­–ä¸­æ–‡å': l2,\n",
    "                        'å›½å®¶': row['å›½å®¶'],\n",
    "                        'èšç±»ID': cid,\n",
    "                        'Starting': f['Starting'],\n",
    "                        'Trend': f['Trend'],\n",
    "                        'Ending': f['Ending'],\n",
    "                        'MeanStart': f['MeanStart'],\n",
    "                        'Type': m_type\n",
    "                    })\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ•°æ®\")\n",
    "        return\n",
    "\n",
    "    # === æ­¥éª¤ B: æ„å»º DataFrame å¹¶å»é‡ (æ¨¡æ‹Ÿ Cell 12 çš„è¾“å‡º) ===\n",
    "    df_full = pd.DataFrame(all_records).drop_duplicates(subset=['L2æ”¿ç­–ä¸­æ–‡å', 'å›½å®¶', 'Type'])\n",
    "    \n",
    "    # ä¿å­˜ CSV (è¿™æ˜¯ Cell 3 è¯»å–çš„å¯¹è±¡)\n",
    "    df_full.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… CSV å·²ä¿å­˜: {OUTPUT_CSV.name}\")\n",
    "\n",
    "    # === æ­¥éª¤ C: æ¨¡æ‹Ÿ Cell 3 çš„è¯»å–å’Œæ’åºè¿‡ç¨‹ ===\n",
    "    # ä¸ºäº†ç»å¯¹ä¸¥è°¨ï¼Œæˆ‘ä»¬ç›´æ¥å¯¹ df_full è¿›è¡Œæ“ä½œï¼Œå°±åƒ Cell 3 è¯»è¿›æ¥ä¸€æ ·\n",
    "    order_map = {}\n",
    "    \n",
    "    # 1. ç­›é€‰ metric (Cell 3 æ˜¯åˆ†å¼€è·‘çš„ï¼Œè¿™é‡Œæˆ‘ä»¬å¾ªç¯è·‘)\n",
    "    for m_type in ['Breadth', 'Intensity']:\n",
    "        df_metric = df_full[df_full['Type'] == m_type].copy()\n",
    "        if df_metric.empty: continue\n",
    "        \n",
    "        # 2. å…¨é‡æ’åº (Cell 3 çš„å…³é”®æ­¥éª¤: sort_clusters(df_meta))\n",
    "        # è¿™ä¼šæ ¹æ® Starting/Ending ç­‰å¯¹æ•´ä¸ªè¡¨æ’åº\n",
    "        df_metric_sorted = sort_clusters_original_logic(df_metric)\n",
    "        \n",
    "        # 3. å¾ªç¯ L2 æå– ID (Cell 3 çš„ç»˜å›¾å¾ªç¯)\n",
    "        for l2 in df_metric_sorted['L2æ”¿ç­–ä¸­æ–‡å'].unique():\n",
    "            # æå–è¯¥ L2 ä¸‹çš„æ‰€æœ‰è¡Œ\n",
    "            l2_sort_info = df_metric_sorted[df_metric_sorted['L2æ”¿ç­–ä¸­æ–‡å'] == l2]\n",
    "            \n",
    "            # Cell 3 é‡Œçš„é€»è¾‘: l2_sort_info.sort_values('sort_key')['èšç±»ID'].unique()\n",
    "            # è™½ç„¶ df_metric_sorted å·²ç»æ˜¯æ’å¥½åºçš„ï¼Œä½† Cell 3 é‡Œä¸ºäº†ä¿é™©åˆ sort äº†ä¸€æ¬¡\n",
    "            # æˆ‘ä»¬è¿™é‡Œä¹Ÿç…§åšï¼Œç¡®ä¿è¡Œä¸ºä¸€è‡´\n",
    "            final_sorted_df = sort_clusters_original_logic(l2_sort_info)\n",
    "            sorted_ids = final_sorted_df['èšç±»ID'].unique().tolist()\n",
    "            \n",
    "            order_map[f\"{m_type}_{l2}\"] = sorted_ids\n",
    "\n",
    "    # === æ­¥éª¤ D: é”å®š JSON ===\n",
    "    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(order_map, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"âœ… é¡ºåºå·²ä¸¥æ ¼é”å®šè‡³ JSON: {OUTPUT_JSON.name}\")\n",
    "\n",
    "# è¿è¡Œ\n",
    "process_and_lock_strict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b1a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> å¼€å§‹ç»˜å›¾: Breadth\n",
      "  -> Saved: Buildings_â€“_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Buildings_â€“_non_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Climate_governance_Breadth_Sorted.png\n",
      "  -> Saved: Electricity_â€“_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Electricity_â€“_non_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Fossil_fuel_production_policies_Breadth_Sorted.png\n",
      "  -> Saved: GHG_emissions_data_and_reporting_Breadth_Sorted.png\n",
      "  -> Saved: GHG_emissions_targets_Breadth_Sorted.png\n",
      "  -> Saved: Industry_â€“_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Industry_â€“_non_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: International_climate_co-operation_Breadth_Sorted.png\n",
      "  -> Saved: International_public_finance_Breadth_Sorted.png\n",
      "  -> Saved: Public_Research,_Development_and_Demonstration_Breadth_Sorted.png\n",
      "  -> Saved: Transport_â€“_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Transport_â€“_non_market-based_instruments_Breadth_Sorted.png\n",
      "\n",
      ">>> å¼€å§‹ç»˜å›¾: Intensity\n",
      "  -> Saved: Buildings_â€“_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Buildings_â€“_non_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Climate_governance_Intensity_Sorted.png\n",
      "  -> Saved: Electricity_â€“_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Electricity_â€“_non_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Fossil_fuel_production_policies_Intensity_Sorted.png\n",
      "  -> Saved: GHG_emissions_data_and_reporting_Intensity_Sorted.png\n",
      "  -> Saved: GHG_emissions_targets_Intensity_Sorted.png\n",
      "  -> Saved: Industry_â€“_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Industry_â€“_non_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: International_climate_co-operation_Intensity_Sorted.png\n",
      "  -> Saved: International_public_finance_Intensity_Sorted.png\n",
      "  -> Saved: Public_Research,_Development_and_Demonstration_Intensity_Sorted.png\n",
      "  -> Saved: Transport_â€“_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Transport_â€“_non_market-based_instruments_Intensity_Sorted.png\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cell 2: è¯»å– JSON é¡ºåºç»˜å›¾\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "\n",
    "# --- åŸºç¡€é…ç½® ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "ORDER_FILE = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"3-2-fixed_ppt_order.json\"\n",
    "FEATURE_FILE = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "OUTPUT_DIR = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\"\n",
    "\n",
    "# --- æ ·å¼è®¾ç½® ---\n",
    "def setup_mpl_single2():\n",
    "    mpl.rc('font', size=25)\n",
    "    mpl.rcParams.update({\n",
    "        'legend.fontsize': 'small', 'xtick.labelsize': 'small', 'ytick.labelsize': 'small',\n",
    "        'lines.linewidth': 2, 'axes.linewidth': 2, 'xtick.major.pad': '12', 'ytick.major.pad': '12',\n",
    "        'xtick.direction': 'in', 'ytick.direction': 'in', 'xtick.top': False, 'ytick.right': False,\n",
    "        'mathtext.default': 'regular', 'axes.titlesize': 'small'\n",
    "    })\n",
    "setup_mpl_single2()\n",
    "\n",
    "NATURE_COLORS = [\n",
    "    '#E64B35', \"#6917C2\", '#00A087', '#3C5488', '#F39B7F', \n",
    "    '#8491B4', '#91D1C2', '#DC0000', '#7E6148', '#B09C85', \n",
    "    '#E18727', '#20854E', '#0072B5', '#BC3C29', '#6F99AD'\n",
    "]\n",
    "\n",
    "def lighten_color(color, amount=0.7):\n",
    "    try:\n",
    "        c = mcolors.to_rgb(color)\n",
    "        return tuple([c[i] + (1 - c[i]) * amount for i in range(3)])\n",
    "    except: return (0.5, 0.5, 0.5)\n",
    "\n",
    "# --- ç»˜å›¾æ ¸å¿ƒ ---\n",
    "def plot_fixed_order(l2_name, df_data, df_feats, sorted_ids, metric_label):\n",
    "    # æ•°æ®å‡†å¤‡\n",
    "    df_plot = df_data[(df_data['å¹´ä»½'] >= 2005) & (df_data['å¹´ä»½'] <= 2023)]\n",
    "    years = sorted(df_plot['å¹´ä»½'].unique())\n",
    "    matrix = df_plot.pivot(index='å¹´ä»½', columns='å›½å®¶', values='å æ¯”').reindex(years)\n",
    "    overall_mean = matrix.mean(axis=1)\n",
    "    y_max = matrix.max().max() * 1.15\n",
    "\n",
    "    # ç”»å¸ƒè®¾ç½®\n",
    "    n_clusters = len(sorted_ids)\n",
    "    n_cols, n_rows = 3, (n_clusters + 2) // 3\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3.5 * n_rows), squeeze=False)\n",
    "    \n",
    "    # --- ä¸¥æ ¼æŒ‰ç…§ JSON ç»™å®šçš„ ID é¡ºåºå¾ªç¯ ---\n",
    "    for idx, cid in enumerate(sorted_ids):\n",
    "        ax = axes[idx // n_cols, idx % n_cols]\n",
    "        \n",
    "        # æ‰¾å¯¹åº”çš„å›½å®¶\n",
    "        countries = [c for c in df_plot[df_plot['èšç±»ID'] == cid]['å›½å®¶'].unique() if c in matrix.columns]\n",
    "        if not countries: continue\n",
    "\n",
    "        # æ‰¾æ ‡é¢˜ (ä»ç‰¹å¾æ–‡ä»¶é‡Œè¯»)\n",
    "        feat_row = df_feats[(df_feats['L2æ”¿ç­–ä¸­æ–‡å'] == l2_name) & (df_feats['èšç±»ID'] == cid)]\n",
    "        if not feat_row.empty:\n",
    "            r = feat_row.iloc[0]\n",
    "            title_str = f\"{r['Starting']}+{r['Trend']}+{str(r['Ending']).replace(' Share', '')}\"\n",
    "        else:\n",
    "            title_str = f\"Cluster {cid}\"\n",
    "\n",
    "        # ç»˜å›¾æ“ä½œ\n",
    "        color = NATURE_COLORS[idx % len(NATURE_COLORS)] # é¢œè‰²åªè·Ÿé¡ºåºæœ‰å…³\n",
    "        fill_color = lighten_color(color, 0.7)\n",
    "        sub_matrix = matrix[countries]\n",
    "        c_mean = sub_matrix.mean(axis=1)\n",
    "\n",
    "        ax.plot(sub_matrix.index, sub_matrix.values, color=color, alpha=0.25, lw=1.2, zorder=1)\n",
    "        ax.plot(c_mean.index, c_mean, marker='o', color=color, lw=2.5, ms=7,\n",
    "                mfc=fill_color, mec=color, mew=1.8, label='Cluster Average',\n",
    "                markevery=max(1, len(years)//10), zorder=10)\n",
    "        ax.plot(overall_mean.index, overall_mean, color='#000000', ls='--', lw=2.5,\n",
    "                label='Overall Average', alpha=0.85, zorder=9, dashes=(3, 2))\n",
    "\n",
    "        ax.set_title(f\"{title_str}\\n({len(countries)} countries)\", pad=15, ha='center')\n",
    "        ax.set_ylim(bottom=-(y_max * 0.04), top=y_max)\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "        ax.tick_params(axis='x', rotation=90, labelsize=15, pad=5.5)\n",
    "        \n",
    "        if idx % n_cols == 0: ax.set_ylabel(metric_label)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.legend(loc='best', frameon=False, fontsize=12)\n",
    "\n",
    "    # æ¸…ç†å¤šä½™å­å›¾\n",
    "    for j in range(n_clusters, n_rows * n_cols): axes[j // n_cols, j % n_cols].axis('off')\n",
    "    plt.subplots_adjust(wspace=0.25, top=0.85, bottom=0.15, hspace=0.45)\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    safe_name = l2_name.replace(\"/\", \"_\").replace(\" \", \"_\").replace(\":\", \"\")\n",
    "    out_dir_m = OUTPUT_DIR / f\"Plots_{metric_label}\"\n",
    "    out_dir_m.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir_m / f\"{safe_name}_{metric_label}_Sorted.png\"\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "    print(f\"  -> Saved: {out_path.name}\")\n",
    "\n",
    "# --- ä¸»æ‰§è¡Œ ---\n",
    "def main_plot(fname, m_label):\n",
    "    print(f\"\\n>>> å¼€å§‹ç»˜å›¾: {m_label}\")\n",
    "    if not ORDER_FILE.exists() or not FEATURE_FILE.exists():\n",
    "        print(\"âŒ è¯·å…ˆè¿è¡Œ Cell 1\")\n",
    "        return\n",
    "        \n",
    "    with open(ORDER_FILE, 'r', encoding='utf-8') as f:\n",
    "        order_map = json.load(f)\n",
    "    df_feats = pd.read_csv(FEATURE_FILE, encoding='utf-8-sig')\n",
    "    \n",
    "    fpath = DATA_DIR / fname\n",
    "    if not fpath.exists(): return\n",
    "    df_data = pd.read_csv(fpath, encoding='utf-8-sig')\n",
    "    \n",
    "    for l2 in df_data['L2æ”¿ç­–ä¸­æ–‡å'].dropna().unique():\n",
    "        key = f\"{m_label}_{l2}\"\n",
    "        if key in order_map:\n",
    "            plot_fixed_order(l2, df_data[df_data['L2æ”¿ç­–ä¸­æ–‡å']==l2], df_feats, order_map[key], m_label)\n",
    "\n",
    "# è¿è¡Œ\n",
    "main_plot(\"3-1-L2_Policy_Clustering_Breadth.csv\", \"Breadth\")\n",
    "main_plot(\"3-1-L2_Policy_Clustering_Intensity.csv\", \"Intensity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ec0743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> æ­£åœ¨ç”Ÿæˆ PPT (JSONæ’åºç‰ˆ): Analysis_Report_Breadth_Editable.pptx ...\n",
      "  [15/15] å¤„ç†: Transport â€“ non mark...\n",
      "âœ… PPT ç”Ÿæˆå®Œæ¯•: Analysis_Report_Breadth_Editable.pptx\n",
      "\n",
      ">>> æ­£åœ¨ç”Ÿæˆ PPT (JSONæ’åºç‰ˆ): Analysis_Report_Intensity_Editable.pptx ...\n",
      "  [15/15] å¤„ç†: Transport â€“ non mark...\n",
      "âœ… PPT ç”Ÿæˆå®Œæ¯•: Analysis_Report_Intensity_Editable.pptx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "from pathlib import Path\n",
    "import io\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "\n",
    "# ==========================================\n",
    "# 1. æ ·å¼ä¸åŸºç¡€é…ç½® (ä¿æŒä¸åŠ¨)\n",
    "# ==========================================\n",
    "def setup_mpl_single2():\n",
    "    # é’ˆå¯¹å°å°ºå¯¸å›¾ç‰‡(4è‹±å¯¸)ä¼˜åŒ–å­—å·\n",
    "    mpl.rc('font', size=12) \n",
    "    mpl.rcParams.update({\n",
    "        'legend.fontsize': 9, \n",
    "        'xtick.labelsize': 10, \n",
    "        'ytick.labelsize': 10,\n",
    "        'lines.linewidth': 2, \n",
    "        'axes.linewidth': 1.2,\n",
    "        'xtick.major.pad': '3', \n",
    "        'ytick.major.pad': '3',\n",
    "        'xtick.direction': 'in', \n",
    "        'ytick.direction': 'in',\n",
    "        'xtick.top': False, \n",
    "        'ytick.right': False,\n",
    "        'mathtext.default': 'regular', \n",
    "        'axes.titlesize': 11\n",
    "    })\n",
    "setup_mpl_single2()\n",
    "\n",
    "NATURE_COLORS = [\n",
    "    '#E64B35', \"#6917C2\", '#00A087', '#3C5488', '#F39B7F', \n",
    "    '#8491B4', '#91D1C2', '#DC0000', '#7E6148', '#B09C85', \n",
    "    '#E18727', '#20854E', '#0072B5', '#BC3C29', '#6F99AD'\n",
    "]\n",
    "\n",
    "def lighten_color(color, amount=0.7):\n",
    "    try:\n",
    "        c = mcolors.to_rgb(color)\n",
    "        c = tuple([c[i] + (1 - c[i]) * amount for i in range(3)])\n",
    "        return c\n",
    "    except: return color\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ ¸å¿ƒç»˜å›¾å‡½æ•° (ä¿æŒä¸åŠ¨)\n",
    "# ==========================================\n",
    "def create_single_cluster_img(l2_data, cid, years, matrix, overall_mean, y_max, metric_label, color_idx, show_ylabel):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆå•ä¸ªèšç±»çš„å°å›¾ã€‚\n",
    "    - show_ylabel: True åˆ™æ˜¾ç¤º Y è½´æ ‡é¢˜ï¼ŒFalse åˆ™éšè—ï¼ˆä½†ä¿ç•™ç©ºé—´ä»¥å¯¹é½ï¼‰\n",
    "    \"\"\"\n",
    "    countries = l2_data[l2_data['èšç±»ID'] == cid]['å›½å®¶'].unique()\n",
    "    countries = [c for c in countries if c in matrix.columns]\n",
    "    \n",
    "    if not countries: return None\n",
    "\n",
    "    # ç”»å¸ƒå°ºå¯¸ï¼šè®¾å®šä¸º PPT ä¸­å›¾ç‰‡çš„ç›®æ ‡å°ºå¯¸ (å®½é«˜æ¯”çº¦ 1.5)\n",
    "    # ç¨å¾®åŠ å¤§ä¸€ç‚¹ï¼Œåˆ©ç”¨ PPT ç©ºé—´\n",
    "    fig, ax = plt.subplots(figsize=(4.2, 2.8))\n",
    "    \n",
    "    color = NATURE_COLORS[color_idx % len(NATURE_COLORS)]\n",
    "    fill_color = lighten_color(color, 0.7)\n",
    "    \n",
    "    sub_matrix = matrix[countries]\n",
    "    c_mean = sub_matrix.mean(axis=1)\n",
    "    \n",
    "    # --- ç»˜å›¾ ---\n",
    "    ax.plot(sub_matrix.index, sub_matrix.values, color=color, alpha=0.25, lw=1.0, zorder=1)\n",
    "    ax.plot(c_mean.index, c_mean, marker='o', color=color, lw=2.0, ms=5,\n",
    "            mfc=fill_color, mec=color, mew=1.5, label='Cluster Avg',\n",
    "            markevery=max(1, len(years)//10), zorder=10)\n",
    "    ax.plot(overall_mean.index, overall_mean, color='#000000', ls='--', lw=2.0,\n",
    "            label='Overall Avg', alpha=0.85, zorder=9, dashes=(3, 2))\n",
    "    \n",
    "    # --- åæ ‡è½´è®¾ç½® ---\n",
    "    # Yè½´å‘ä¸‹å»¶ä¼¸ 4%\n",
    "    y_neg_padding = y_max * 0.04\n",
    "    ax.set_ylim(bottom=-y_neg_padding, top=y_max)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax.tick_params(axis='x', rotation=90) # å¹´ä»½ç«–æ’\n",
    "    \n",
    "    # === å…³é”®ï¼šYè½´æ ‡ç­¾æ§åˆ¶ ===\n",
    "    if show_ylabel:\n",
    "        ax.set_ylabel(metric_label)\n",
    "    else:\n",
    "        ax.set_ylabel(\"\") # ä¸å†™æ–‡å­—ï¼Œä½†ä¸‹é¢å›ºå®šè¾¹è·ä¼šä¿ç•™ä½ç½®\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # å›¾ä¾‹\n",
    "    ax.legend(loc='best', frameon=False, fontsize=8, handlelength=1.5, markerscale=0.8)\n",
    "    \n",
    "    # === å…³é”®ï¼šä½¿ç”¨å›ºå®šè¾¹è·ä»£æ›¿ tight_layout ===\n",
    "    # è¿™æ ·æ‰€æœ‰å›¾çš„ç»˜å›¾åŒºåŸŸï¼ˆåæ ‡è½´æ¡†ï¼‰å¤§å°å®Œå…¨ä¸€è‡´\n",
    "    plt.subplots_adjust(left=0.18, right=0.97, top=0.95, bottom=0.25)\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png', dpi=200, transparent=True)\n",
    "    plt.close()\n",
    "    img_stream.seek(0)\n",
    "    return img_stream\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ•°æ®åŠ è½½ä¸æ’åº (ä¿®æ”¹ï¼šä¼˜å…ˆè¯»å–JSON)\n",
    "# ==========================================\n",
    "def load_features(feature_file: Path, metric_type: str):\n",
    "    if not feature_file.exists(): return pd.DataFrame()\n",
    "    df = pd.read_csv(feature_file, encoding='utf-8-sig')\n",
    "    df = df[df['Type'] == metric_type].copy()\n",
    "    return df\n",
    "\n",
    "def load_json_order(json_file: Path):\n",
    "    if not json_file.exists():\n",
    "        print(f\"âš ï¸ JSON é¡ºåºæ–‡ä»¶æœªæ‰¾åˆ°: {json_file}\")\n",
    "        return {}\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ==========================================\n",
    "# 4. PPT ç”Ÿæˆä¸»ç¨‹åº (é›†æˆ JSON æ’åº)\n",
    "# ==========================================\n",
    "def generate_ppt_grid(csv_file: Path, feature_file: Path, json_file: Path, output_ppt: Path, metric_label: str):\n",
    "    if not csv_file.exists(): return\n",
    "\n",
    "    print(f\"\\n>>> æ­£åœ¨ç”Ÿæˆ PPT (JSONæ’åºç‰ˆ): {output_ppt.name} ...\")\n",
    "    \n",
    "    df_data = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
    "    df_data = df_data[(df_data['å¹´ä»½'] >= 2005) & (df_data['å¹´ä»½'] <= 2023)]\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    df_features = load_features(feature_file, metric_label)\n",
    "    order_map = load_json_order(json_file) # æ ¸å¿ƒï¼šåŠ è½½ JSON\n",
    "    \n",
    "    if df_features.empty: return\n",
    "\n",
    "    # PPT åˆå§‹åŒ–\n",
    "    prs = Presentation()\n",
    "    prs.slide_width = Inches(13.333)\n",
    "    prs.slide_height = Inches(7.5)\n",
    "    \n",
    "    # === å¸ƒå±€å‚æ•° ===\n",
    "    MARGIN_LEFT = 0.5\n",
    "    MARGIN_TOP = 1.1   # ç¨å¾®å¾€ä¸Šæ\n",
    "    COL_GAP = 0.15     # å‡å°åˆ—é—´è·\n",
    "    ROW_GAP = 0.4      # è¡Œé—´è·\n",
    "    \n",
    "    PLOT_WIDTH = 4.0\n",
    "    PLOT_HEIGHT = 2.6  # é«˜åº¦å¢åŠ \n",
    "    LABEL_HEIGHT = 0.35\n",
    "    \n",
    "    l2_list = df_data['L2æ”¿ç­–ä¸­æ–‡å'].dropna().unique()\n",
    "    \n",
    "    for i, l2 in enumerate(l2_list):\n",
    "        print(f\"\\r  [{i+1}/{len(l2_list)}] å¤„ç†: {l2[:20]}...\", end=\"\")\n",
    "        \n",
    "        l2_feats = df_features[df_features['L2æ”¿ç­–ä¸­æ–‡å'] == l2]\n",
    "        if l2_feats.empty: continue\n",
    "        \n",
    "        # === æ ¸å¿ƒé€»è¾‘ä¿®æ”¹ï¼šè·å–æ’åº ID ===\n",
    "        key = f\"{metric_label}_{l2}\"\n",
    "        if key in order_map:\n",
    "            sorted_cids = order_map[key] # ä¼˜å…ˆï¼šä½¿ç”¨ JSON é‡Œçš„é¡ºåº\n",
    "        else:\n",
    "            # é™çº§ï¼šå¦‚æœæ²¡æœ‰ JSON è®°å½•ï¼Œé€€å›åˆ°é»˜è®¤æ’åº (æŒ‰ Cluster ID)\n",
    "            # æˆ–è€…ä½ å¯ä»¥é€‰æ‹©åœ¨è¿™é‡ŒåŠ ä¸Šä½ çš„æ—§æ’åºé€»è¾‘ï¼Œä½†é€šå¸¸ JSON åº”è¯¥è¦†ç›–æ‰€æœ‰\n",
    "            print(f\" (âš ï¸ æ— JSONè®°å½•, ä½¿ç”¨é»˜è®¤IDé¡ºåº) \", end=\"\")\n",
    "            sorted_cids = sorted(l2_feats['èšç±»ID'].unique())\n",
    "\n",
    "        l2_sub = df_data[df_data['L2æ”¿ç­–ä¸­æ–‡å'] == l2]\n",
    "        years = sorted(l2_sub['å¹´ä»½'].unique())\n",
    "        matrix = l2_sub.pivot(index='å¹´ä»½', columns='å›½å®¶', values='å æ¯”').reindex(years)\n",
    "        overall_mean = matrix.mean(axis=1)\n",
    "        y_max = matrix.max().max() * 1.15\n",
    "        \n",
    "        # æ–°å»ºå¹»ç¯ç‰‡\n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "        \n",
    "        # A. ä¸»æ ‡é¢˜\n",
    "        tb = slide.shapes.add_textbox(Inches(0.5), Inches(0.1), Inches(12), Inches(0.8))\n",
    "        p = tb.text_frame.paragraphs[0]\n",
    "        p.text = l2\n",
    "        p.font.size = Pt(24); p.font.bold = True; p.font.color.rgb = RGBColor(44, 62, 80)\n",
    "        \n",
    "        # --- å¾ªç¯ç”Ÿæˆç½‘æ ¼ ---\n",
    "        for idx, cid in enumerate(sorted_cids):\n",
    "            # å¦‚æœ JSON é‡Œçš„ ID åœ¨ç‰¹å¾è¡¨é‡Œæ‰¾ä¸åˆ°ï¼ˆæ¯”å¦‚æ•°æ®è¿‡æ»¤äº†ï¼‰ï¼Œè·³è¿‡\n",
    "            if cid not in l2_feats['èšç±»ID'].values: continue\n",
    "\n",
    "            row, col = idx // 3, idx % 3\n",
    "            \n",
    "            curr_x = Inches(MARGIN_LEFT + col * (PLOT_WIDTH + COL_GAP))\n",
    "            curr_y = Inches(MARGIN_TOP + row * (LABEL_HEIGHT + PLOT_HEIGHT + ROW_GAP))\n",
    "            \n",
    "            if curr_y + Inches(3) > prs.slide_height: break \n",
    "            \n",
    "            # å‡†å¤‡æ–‡å­—\n",
    "            feat_row = l2_feats[l2_feats['èšç±»ID'] == cid].iloc[0]\n",
    "            s_t = feat_row['Starting']\n",
    "            t_t = feat_row['Trend']\n",
    "            e_t = str(feat_row['Ending']).replace(' Share', '') \n",
    "            \n",
    "            countries = l2_sub[l2_sub['èšç±»ID'] == cid]['å›½å®¶'].unique()\n",
    "            label_text = f\"{s_t}+{t_t}+{e_t} ({len(countries)} countries)\"\n",
    "            \n",
    "            # B. æ’å…¥æ–‡æœ¬æ¡†\n",
    "            tb_sub = slide.shapes.add_textbox(curr_x, curr_y, Inches(PLOT_WIDTH), Inches(LABEL_HEIGHT))\n",
    "            tp = tb_sub.text_frame.paragraphs[0]\n",
    "            tp.text = label_text\n",
    "            tp.font.size = Pt(14); tp.font.bold = True; tp.alignment = PP_ALIGN.CENTER\n",
    "            \n",
    "            # C. æ’å…¥å›¾ç‰‡\n",
    "            # === å…³é”®ï¼šä¼ å…¥ show_ylabel å‚æ•° ===\n",
    "            show_y = (col == 0)\n",
    "            \n",
    "            img_stream = create_single_cluster_img(\n",
    "                l2_sub, cid, years, matrix, overall_mean, y_max, metric_label, idx, \n",
    "                show_ylabel=show_y\n",
    "            )\n",
    "            \n",
    "            if img_stream:\n",
    "                pic_y = curr_y + Inches(LABEL_HEIGHT)\n",
    "                slide.shapes.add_picture(img_stream, curr_x, pic_y, width=Inches(PLOT_WIDTH))\n",
    "                \n",
    "    prs.save(output_ppt)\n",
    "    print(f\"\\nâœ… PPT ç”Ÿæˆå®Œæ¯•: {output_ppt.name}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. æ‰§è¡Œ\n",
    "# ==========================================\n",
    "def main():\n",
    "    base_dir = Path.cwd().parent\n",
    "    data_dir = base_dir / \"data\" if (base_dir / \"data\").exists() else Path.cwd() / \"data\"\n",
    "    output_dir = data_dir / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"Human_catorgy\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    feature_csv = data_dir / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "    json_file = data_dir / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"3-2-fixed_ppt_order.json\"\n",
    "    \n",
    "    # 1. Breadth PPT\n",
    "    generate_ppt_grid(\n",
    "        data_dir / \"3-1-L2_Policy_Clustering_Breadth.csv\",\n",
    "        feature_csv,\n",
    "        json_file,\n",
    "        output_dir / \"Analysis_Report_Breadth_Editable.pptx\",\n",
    "        \"Breadth\"\n",
    "    )\n",
    "    \n",
    "    # 2. Intensity PPT\n",
    "    generate_ppt_grid(\n",
    "        data_dir / \"3-1-L2_Policy_Clustering_Intensity.csv\",\n",
    "        feature_csv,\n",
    "        json_file,\n",
    "        output_dir / \"Analysis_Report_Intensity_Editable.pptx\",\n",
    "        \"Intensity\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c6e59",
   "metadata": {},
   "source": [
    "### Wait Human changeï¼ˆé‡å¤è·‘ä¸€éï¼Œæ ¸å¯¹ä¸€ä¸‹ï¼Œè¿™æ¬¡PPTç›´æ¥è·‘çš„ä¸å¯ç¼–è¾‘ç‰ˆçš„ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c7c9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“‚ å¤„ç†æ–‡ä»¶: Analysis_Report_Breadth_Editable.pptx\n",
      "ğŸ¯ ç›®æ ‡ç±»å‹: Breadth\n",
      "ğŸ” é¡ºåºå‚ç…§: 3-2-fixed_ppt_order.json\n",
      "==================================================\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ mar... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Low'] -> æ–°: ['Low', 'Stable', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ mar... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ non... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ non... (ID:2)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Climate governa... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Fluctuate', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Electricity â€“ m... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Electricity â€“ m... (ID:1)\n",
      "    æ—§: ['Medium' 'Rise' 'Medium'] -> æ–°: ['High', 'Fluctuate', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Electricity â€“ n... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Electricity â€“ n... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Electricity â€“ n... (ID:1)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Fossil fuel pro... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions d... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Fluctuate', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions d... (ID:1)\n",
      "    æ—§: ['Medium' 'Fluctuate' 'Medium'] -> æ–°: ['Medium', 'Fluctuate', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions t... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Rise', 'High (Early)']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions t... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Rise', 'High (Later)']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions t... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Industry â€“ mark... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Industry â€“ mark... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Industry â€“ mark... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Industry â€“ non ... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Industry â€“ non ... (ID:1)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] International c... (ID:1)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] International c... (ID:2)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] International p... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ mar... (ID:0)\n",
      "    æ—§: ['Low' 'Stable' 'Low'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ mar... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ mar... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ non... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ non... (ID:2)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      "âœ… ä¿å­˜æˆåŠŸï¼å…±æ›´æ–°äº† 400 è¡Œæ•°æ®ã€‚\n",
      "\n",
      "==================================================\n",
      "ğŸ“‚ å¤„ç†æ–‡ä»¶: Analysis_Report_Intensity_Editable.pptx\n",
      "ğŸ¯ ç›®æ ‡ç±»å‹: Intensity\n",
      "ğŸ” é¡ºåºå‚ç…§: 3-2-fixed_ppt_order.json\n",
      "==================================================\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ mar... (ID:0)\n",
      "    æ—§: ['Low' 'Stable' 'Low'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ mar... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ non... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ non... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Buildings â€“ non... (ID:0)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Climate governa... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Climate governa... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Electricity â€“ m... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Electricity â€“ n... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Electricity â€“ n... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Fossil fuel pro... (ID:2)\n",
      "    æ—§: ['Medium' 'Rise' 'Medium'] -> æ–°: ['High', 'Fluctuate', 'High']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions d... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'Low'] -> æ–°: ['Low', 'Fluctuate', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions d... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Fluctuate', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions d... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions t... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] GHG emissions t... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Low', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Industry â€“ mark... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Industry â€“ non ... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['High', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] International c... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] International c... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] International c... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'High'] -> æ–°: ['Medium', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] International p... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Public Research... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Medium', 'Fluctuate', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Public Research... (ID:2)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ mar... (ID:2)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Medium', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ mar... (ID:1)\n",
      "    æ—§: ['Medium' 'Rise' 'Medium'] -> æ–°: ['High', 'Rise', 'High']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ non... (ID:0)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Low', 'Rise', 'Low']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ non... (ID:1)\n",
      "    æ—§: ['Low' 'Rise' 'Medium'] -> æ–°: ['Medium', 'Rise', 'Medium']\n",
      " ğŸ”„ [æ›´æ–°] Transport â€“ non... (ID:2)\n",
      "    æ—§: ['Medium' 'Rise' 'High'] -> æ–°: ['High', 'Rise', 'High']\n",
      "âœ… ä¿å­˜æˆåŠŸï¼å…±æ›´æ–°äº† 439 è¡Œæ•°æ®ã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "from pathlib import Path\n",
    "from pptx.util import Inches\n",
    "from typing import List, Optional, Dict\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# 1. å¼ºåŠ›æ¸…æ´—å‡½æ•° (ä¿ç•™åŸé€»è¾‘)\n",
    "# ==========================================\n",
    "def normalize_title_robust(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ç»ˆææ¸…æ´—ï¼šè½¬å°å†™ã€æ›¿æ¢æ‰€æœ‰ç±»å‹æ¨ªæ ã€å»å¤šä½™ç©ºæ ¼\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): return str(text) if text is not None else \"\"\n",
    "    text = text.replace('\\u2013', '-').replace('\\u2014', '-').replace('â€“', '-')\n",
    "    text = text.replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def normalize_spaces(text: str) -> str:\n",
    "    \"\"\"æ¸…ç†æ–‡æœ¬æ¡†å†…å®¹ä¸­çš„ç‰¹æ®Šç©ºæ ¼\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def get_slide_title(slide) -> Optional[str]:\n",
    "    \"\"\"è·å–å¹»ç¯ç‰‡æ ‡é¢˜\"\"\"\n",
    "    if slide.shapes.title and slide.shapes.title.text.strip():\n",
    "        return slide.shapes.title.text.strip()\n",
    "    \n",
    "    candidates = []\n",
    "    for shape in slide.shapes:\n",
    "        if shape.has_text_frame and shape.text_frame.text.strip():\n",
    "            candidates.append((shape.top, shape.text_frame.text.strip()))\n",
    "    \n",
    "    if candidates:\n",
    "        candidates.sort(key=lambda x: x[0])\n",
    "        if candidates[0][0] < Inches(2.0):\n",
    "            return candidates[0][1]\n",
    "    return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ ¸å¿ƒï¼šåŠ è½½ JSON é”å®šé¡ºåº\n",
    "# ==========================================\n",
    "def load_fixed_order(json_path: Path) -> Dict[str, List[int]]:\n",
    "    if not json_path.exists():\n",
    "        print(f\"âŒ JSON é¡ºåºæ–‡ä»¶æœªæ‰¾åˆ°: {json_path}\")\n",
    "        return {}\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ ¸å¿ƒåŒæ­¥é€»è¾‘ (åˆ©ç”¨ JSON å›å¡«)\n",
    "# ==========================================\n",
    "def sync_ppt_with_json(ppt_path: Path, csv_path: Path, json_path: Path, target_type: str) -> None:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ“‚ å¤„ç†æ–‡ä»¶: {ppt_path.name}\")\n",
    "    print(f\"ğŸ¯ ç›®æ ‡ç±»å‹: {target_type}\")\n",
    "    print(f\"ğŸ” é¡ºåºå‚ç…§: {json_path.name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if not ppt_path.exists():\n",
    "        print(f\"âŒ PPTæ–‡ä»¶ä¸å­˜åœ¨: {ppt_path}\")\n",
    "        return\n",
    "\n",
    "    prs = Presentation(ppt_path)\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "    order_map = load_fixed_order(json_path)\n",
    "    \n",
    "    if not order_map:\n",
    "        print(\"âŒ æ— æ³•åŠ è½½é¡ºåºæ˜ å°„ï¼Œç»ˆæ­¢æ“ä½œã€‚\")\n",
    "        return\n",
    "    \n",
    "    # === å…³é”®1ï¼šåˆ›å»ºåŒ¹é…åˆ— ===\n",
    "    df['Match_Title'] = df['L2æ”¿ç­–ä¸­æ–‡å'].apply(normalize_title_robust)\n",
    "    \n",
    "    # åŒæ—¶ä¹ŸæŠŠ JSON çš„ key å¤„ç†ä¸€ä¸‹ï¼Œæ–¹ä¾¿æŸ¥æ‰¾\n",
    "    # JSON key æ ¼å¼: \"Breadth_PolicyName\" -> æˆ‘ä»¬å­˜ä¸€ä¸ªæ˜ å°„: clean_policy_name -> [ids]\n",
    "    json_lookup = {}\n",
    "    prefix = f\"{target_type}_\"\n",
    "    for key, ids in order_map.items():\n",
    "        if key.startswith(prefix):\n",
    "            raw_policy_name = key[len(prefix):] # å»æ‰ \"Breadth_\" å‰ç¼€\n",
    "            clean_name = normalize_title_robust(raw_policy_name)\n",
    "            json_lookup[clean_name] = ids\n",
    "\n",
    "    update_count = 0\n",
    "    \n",
    "    for slide_idx, slide in enumerate(prs.slides):\n",
    "        raw_ppt_title = get_slide_title(slide)\n",
    "        if not raw_ppt_title: continue\n",
    "        \n",
    "        # å¤„ç† PPT æ ‡é¢˜ï¼ˆæœ‰äº›æ ‡é¢˜å¯èƒ½åŒ…å«æ‹¬å·é‡Œçš„ metric typeï¼Œå»æ‰å®ƒï¼‰\n",
    "        title_pure = raw_ppt_title.replace(f\"({target_type})\", \"\").strip()\n",
    "        clean_ppt_title = normalize_title_robust(title_pure)\n",
    "        \n",
    "        # 1. æŸ¥æ‰¾ JSON é‡Œçš„é¡ºåº\n",
    "        if clean_ppt_title not in json_lookup:\n",
    "            # å°è¯•æ¨¡ç³ŠåŒ¹é… (æ¯”å¦‚ PPT æ ‡é¢˜ä¸å®Œæ•´)\n",
    "            found = False\n",
    "            for k in json_lookup.keys():\n",
    "                if k in clean_ppt_title or clean_ppt_title in k:\n",
    "                    clean_ppt_title = k\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                # print(f\"âš ï¸ [æ— é¡ºåºè®°å½•] PPTæ ‡é¢˜: '{raw_ppt_title}' - è·³è¿‡\")\n",
    "                continue\n",
    "        \n",
    "        sorted_cids = json_lookup[clean_ppt_title]\n",
    "        \n",
    "        # 2. è·å– PPT é‡Œçš„æ ‡ç­¾æ¡† (ä»å·¦åˆ°å³æ’åº)\n",
    "        # ç­›é€‰é€»è¾‘ï¼šæœ‰æ–‡æœ¬ï¼Œä¸”æ–‡æœ¬é‡ŒåŒ…å« '+' å· (ç‰¹å¾æ ‡è¯†)\n",
    "        label_shapes = [s for s in slide.shapes if s.has_text_frame and \"+\" in s.text_frame.text]\n",
    "        # æ’åºï¼šå…ˆæŒ‰ Top (è¡Œ)ï¼Œå†æŒ‰ Left (åˆ—)ã€‚å…è®¸å¤šè¡Œå¸ƒå±€ã€‚\n",
    "        # å®¹å·®ï¼šTop å·®å¼‚åœ¨ 0.5 è‹±å¯¸å†…è§†ä¸ºåŒä¸€è¡Œ\n",
    "        label_shapes.sort(key=lambda s: (int(s.top / Inches(0.5)), s.left))\n",
    "        \n",
    "        if len(label_shapes) != len(sorted_cids):\n",
    "            # è¿™ç§æƒ…å†µæ¯”è¾ƒå°‘è§ï¼Œå¯èƒ½æ˜¯ PPT è¢«äººæ‰‹åŠ¨åˆ æ”¹äº†\n",
    "            print(f\"âš ï¸ [æ•°é‡ä¸ç¬¦] {title_pure}: PPTæœ‰{len(label_shapes)}ä¸ªæ¡†, JSONè®°å½•æœ‰{len(sorted_cids)}ä¸ªèšç±»\")\n",
    "            continue\n",
    "            \n",
    "        # 3. é€ä¸€å›å¡«\n",
    "        for shape, cid in zip(label_shapes, sorted_cids):\n",
    "            raw_text = normalize_spaces(shape.text_frame.text)\n",
    "            \n",
    "            # è§£æ PPT æ–‡æœ¬å†…å®¹: \"High + Stable + Low (xx countries)\"\n",
    "            parts = raw_text.split('+')\n",
    "            if len(parts) >= 3:\n",
    "                p1 = parts[0].strip()\n",
    "                p2 = parts[1].strip()\n",
    "                p3_full = parts[2].strip()\n",
    "                \n",
    "                # å¤„ç† Ending éƒ¨åˆ†ï¼Œå»æ‰æ‹¬å·é‡Œçš„å›½å®¶æ•°\n",
    "                final_e = p3_full\n",
    "                last_paren = p3_full.rfind('(')\n",
    "                if last_paren != -1:\n",
    "                    final_e = p3_full[:last_paren].strip()\n",
    "                \n",
    "                # æ¸…æ´— \"Share\" åç¼€\n",
    "                new_s = p1.replace(' Share', '')\n",
    "                new_t = p2\n",
    "                new_e = final_e.replace(' Share', '')\n",
    "                \n",
    "                # 4. æ›´æ–° CSV\n",
    "                # å®šä½æ¡ä»¶ï¼šæ ‡å‡†åŒ–çš„æ”¿ç­–å + ç±»å‹ + èšç±»ID (è¿™ä¸ªIDæ¥è‡ª JSONï¼Œç»å¯¹å‡†ç¡®)\n",
    "                target_idx = df[(df['Match_Title'] == clean_ppt_title) & \n",
    "                                (df['Type'] == target_type) & \n",
    "                                (df['èšç±»ID'] == cid)].index\n",
    "                \n",
    "                if not target_idx.empty:\n",
    "                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°\n",
    "                    old_vals = df.loc[target_idx[0], ['Starting', 'Trend', 'Ending']].values\n",
    "                    new_vals = [new_s, new_t, new_e]\n",
    "                    \n",
    "                    if list(old_vals) != new_vals:\n",
    "                        print(f\" ğŸ”„ [æ›´æ–°] {title_pure[:15]}... (ID:{cid})\")\n",
    "                        print(f\"    æ—§: {old_vals} -> æ–°: {new_vals}\")\n",
    "                        \n",
    "                        df.loc[target_idx, 'Starting'] = new_s\n",
    "                        df.loc[target_idx, 'Trend'] = new_t\n",
    "                        df.loc[target_idx, 'Ending'] = new_e\n",
    "                        update_count += len(target_idx) # å¯èƒ½ä¸€æ¬¡æ›´æ–°å¤šè¡Œ(å¤šä¸ªå›½å®¶)\n",
    "\n",
    "    # æ¸…ç†\n",
    "    if 'Match_Title' in df.columns: del df['Match_Title']\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    if update_count > 0:\n",
    "        try:\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"âœ… ä¿å­˜æˆåŠŸï¼å…±æ›´æ–°äº† {update_count} è¡Œæ•°æ®ã€‚\")\n",
    "        except PermissionError:\n",
    "            print(\"âŒ ä¿å­˜å¤±è´¥ï¼è¯·å…ˆå…³é—­ CSV æ–‡ä»¶ï¼\")\n",
    "    else:\n",
    "        print(\"âœ¨ æ–‡ä»¶å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. ä¸»ç¨‹åº\n",
    "# ==========================================\n",
    "def main():\n",
    "    base_dir = Path.cwd().parent\n",
    "    data_dir = base_dir / \"data\" if (base_dir / \"data\").exists() else Path.cwd() / \"data\"\n",
    "    \n",
    "    # è·¯å¾„é…ç½®\n",
    "    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„äººå·¥ä¿®æ”¹åçš„ PPT æ”¾åœ¨è¿™ä¸ª Human_catorgy æ–‡ä»¶å¤¹ä¸‹\n",
    "    ppt_dir = data_dir / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"Human_catorgy\" /\"Final_Edit\"\n",
    "    csv_file = data_dir / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "    json_file = data_dir / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"3-2-fixed_ppt_order.json\"\n",
    "    \n",
    "    # 1. æ›´æ–° Breadth\n",
    "    sync_ppt_with_json(ppt_dir / \"Analysis_Report_Breadth_Editable.pptx\", csv_file, json_file, \"Breadth\")\n",
    "    \n",
    "    # 2. æ›´æ–° Intensity\n",
    "    sync_ppt_with_json(ppt_dir / \"Analysis_Report_Intensity_Editable.pptx\", csv_file, json_file, \"Intensity\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f04e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> å¼€å§‹ç»˜å›¾: Breadth\n",
      "  -> Saved: Buildings_â€“_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Buildings_â€“_non_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Climate_governance_Breadth_Sorted.png\n",
      "  -> Saved: Electricity_â€“_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Electricity_â€“_non_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Fossil_fuel_production_policies_Breadth_Sorted.png\n",
      "  -> Saved: GHG_emissions_data_and_reporting_Breadth_Sorted.png\n",
      "  -> Saved: GHG_emissions_targets_Breadth_Sorted.png\n",
      "  -> Saved: Industry_â€“_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Industry_â€“_non_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: International_climate_co-operation_Breadth_Sorted.png\n",
      "  -> Saved: International_public_finance_Breadth_Sorted.png\n",
      "  -> Saved: Public_Research,_Development_and_Demonstration_Breadth_Sorted.png\n",
      "  -> Saved: Transport_â€“_market-based_instruments_Breadth_Sorted.png\n",
      "  -> Saved: Transport_â€“_non_market-based_instruments_Breadth_Sorted.png\n",
      "\n",
      ">>> å¼€å§‹ç»˜å›¾: Intensity\n",
      "  -> Saved: Buildings_â€“_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Buildings_â€“_non_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Climate_governance_Intensity_Sorted.png\n",
      "  -> Saved: Electricity_â€“_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Electricity_â€“_non_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Fossil_fuel_production_policies_Intensity_Sorted.png\n",
      "  -> Saved: GHG_emissions_data_and_reporting_Intensity_Sorted.png\n",
      "  -> Saved: GHG_emissions_targets_Intensity_Sorted.png\n",
      "  -> Saved: Industry_â€“_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Industry_â€“_non_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: International_climate_co-operation_Intensity_Sorted.png\n",
      "  -> Saved: International_public_finance_Intensity_Sorted.png\n",
      "  -> Saved: Public_Research,_Development_and_Demonstration_Intensity_Sorted.png\n",
      "  -> Saved: Transport_â€“_market-based_instruments_Intensity_Sorted.png\n",
      "  -> Saved: Transport_â€“_non_market-based_instruments_Intensity_Sorted.png\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cell 2: è¯»å– JSON é¡ºåºç»˜å›¾\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "\n",
    "# --- åŸºç¡€é…ç½® ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "ORDER_FILE = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"3-2-fixed_ppt_order.json\"\n",
    "FEATURE_FILE = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "OUTPUT_DIR = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\"\n",
    "\n",
    "# --- æ ·å¼è®¾ç½® ---\n",
    "def setup_mpl_single2():\n",
    "    mpl.rc('font', size=25)\n",
    "    mpl.rcParams.update({\n",
    "        'legend.fontsize': 'small', 'xtick.labelsize': 'small', 'ytick.labelsize': 'small',\n",
    "        'lines.linewidth': 2, 'axes.linewidth': 2, 'xtick.major.pad': '12', 'ytick.major.pad': '12',\n",
    "        'xtick.direction': 'in', 'ytick.direction': 'in', 'xtick.top': False, 'ytick.right': False,\n",
    "        'mathtext.default': 'regular', 'axes.titlesize': 'small'\n",
    "    })\n",
    "setup_mpl_single2()\n",
    "\n",
    "NATURE_COLORS = [\n",
    "    '#E64B35', \"#6917C2\", '#00A087', '#3C5488', '#F39B7F', \n",
    "    '#8491B4', '#91D1C2', '#DC0000', '#7E6148', '#B09C85', \n",
    "    '#E18727', '#20854E', '#0072B5', '#BC3C29', '#6F99AD'\n",
    "]\n",
    "\n",
    "def lighten_color(color, amount=0.7):\n",
    "    try:\n",
    "        c = mcolors.to_rgb(color)\n",
    "        return tuple([c[i] + (1 - c[i]) * amount for i in range(3)])\n",
    "    except: return (0.5, 0.5, 0.5)\n",
    "\n",
    "# --- ç»˜å›¾æ ¸å¿ƒ ---\n",
    "def plot_fixed_order(l2_name, df_data, df_feats, sorted_ids, metric_label):\n",
    "    # æ•°æ®å‡†å¤‡\n",
    "    df_plot = df_data[(df_data['å¹´ä»½'] >= 2005) & (df_data['å¹´ä»½'] <= 2023)]\n",
    "    years = sorted(df_plot['å¹´ä»½'].unique())\n",
    "    matrix = df_plot.pivot(index='å¹´ä»½', columns='å›½å®¶', values='å æ¯”').reindex(years)\n",
    "    overall_mean = matrix.mean(axis=1)\n",
    "    y_max = matrix.max().max() * 1.15\n",
    "\n",
    "    # ç”»å¸ƒè®¾ç½®\n",
    "    n_clusters = len(sorted_ids)\n",
    "    n_cols, n_rows = 3, (n_clusters + 2) // 3\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3.5 * n_rows), squeeze=False)\n",
    "    \n",
    "    # --- ä¸¥æ ¼æŒ‰ç…§ JSON ç»™å®šçš„ ID é¡ºåºå¾ªç¯ ---\n",
    "    for idx, cid in enumerate(sorted_ids):\n",
    "        ax = axes[idx // n_cols, idx % n_cols]\n",
    "        \n",
    "        # æ‰¾å¯¹åº”çš„å›½å®¶\n",
    "        countries = [c for c in df_plot[df_plot['èšç±»ID'] == cid]['å›½å®¶'].unique() if c in matrix.columns]\n",
    "        if not countries: continue\n",
    "\n",
    "        # æ‰¾æ ‡é¢˜ (ä»ç‰¹å¾æ–‡ä»¶é‡Œè¯»)\n",
    "        feat_row = df_feats[(df_feats['L2æ”¿ç­–ä¸­æ–‡å'] == l2_name) & (df_feats['èšç±»ID'] == cid)]\n",
    "        if not feat_row.empty:\n",
    "            r = feat_row.iloc[0]\n",
    "            title_str = f\"{r['Starting']}+{r['Trend']}+{str(r['Ending']).replace(' Share', '')}\"\n",
    "        else:\n",
    "            title_str = f\"Cluster {cid}\"\n",
    "\n",
    "        # ç»˜å›¾æ“ä½œ\n",
    "        color = NATURE_COLORS[idx % len(NATURE_COLORS)] # é¢œè‰²åªè·Ÿé¡ºåºæœ‰å…³\n",
    "        fill_color = lighten_color(color, 0.7)\n",
    "        sub_matrix = matrix[countries]\n",
    "        c_mean = sub_matrix.mean(axis=1)\n",
    "\n",
    "        ax.plot(sub_matrix.index, sub_matrix.values, color=color, alpha=0.25, lw=1.2, zorder=1)\n",
    "        ax.plot(c_mean.index, c_mean, marker='o', color=color, lw=2.5, ms=7,\n",
    "                mfc=fill_color, mec=color, mew=1.8, label='Cluster Average',\n",
    "                markevery=max(1, len(years)//10), zorder=10)\n",
    "        ax.plot(overall_mean.index, overall_mean, color='#000000', ls='--', lw=2.5,\n",
    "                label='Overall Average', alpha=0.85, zorder=9, dashes=(3, 2))\n",
    "\n",
    "        ax.set_title(f\"{title_str}\\n({len(countries)} countries)\", pad=15, ha='center')\n",
    "        ax.set_ylim(bottom=-(y_max * 0.04), top=y_max)\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "        ax.tick_params(axis='x', rotation=90, labelsize=15, pad=5.5)\n",
    "        \n",
    "        if idx % n_cols == 0: ax.set_ylabel(metric_label)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.legend(loc='best', frameon=False, fontsize=12)\n",
    "\n",
    "    # æ¸…ç†å¤šä½™å­å›¾\n",
    "    for j in range(n_clusters, n_rows * n_cols): axes[j // n_cols, j % n_cols].axis('off')\n",
    "    plt.subplots_adjust(wspace=0.25, top=0.85, bottom=0.15, hspace=0.45)\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    safe_name = l2_name.replace(\"/\", \"_\").replace(\" \", \"_\").replace(\":\", \"\")\n",
    "    out_dir_m = OUTPUT_DIR / f\"Plots_{metric_label}\"\n",
    "    out_dir_m.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir_m / f\"{safe_name}_{metric_label}_Sorted.png\"\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "    print(f\"  -> Saved: {out_path.name}\")\n",
    "\n",
    "# --- ä¸»æ‰§è¡Œ ---\n",
    "def main_plot(fname, m_label):\n",
    "    print(f\"\\n>>> å¼€å§‹ç»˜å›¾: {m_label}\")\n",
    "    if not ORDER_FILE.exists() or not FEATURE_FILE.exists():\n",
    "        print(\"âŒ è¯·å…ˆè¿è¡Œ Cell 1\")\n",
    "        return\n",
    "        \n",
    "    with open(ORDER_FILE, 'r', encoding='utf-8') as f:\n",
    "        order_map = json.load(f)\n",
    "    df_feats = pd.read_csv(FEATURE_FILE, encoding='utf-8-sig')\n",
    "    \n",
    "    fpath = DATA_DIR / fname\n",
    "    if not fpath.exists(): return\n",
    "    df_data = pd.read_csv(fpath, encoding='utf-8-sig')\n",
    "    \n",
    "    for l2 in df_data['L2æ”¿ç­–ä¸­æ–‡å'].dropna().unique():\n",
    "        key = f\"{m_label}_{l2}\"\n",
    "        if key in order_map:\n",
    "            plot_fixed_order(l2, df_data[df_data['L2æ”¿ç­–ä¸­æ–‡å']==l2], df_feats, order_map[key], m_label)\n",
    "\n",
    "# è¿è¡Œ\n",
    "main_plot(\"3-1-L2_Policy_Clustering_Breadth.csv\", \"Breadth\")\n",
    "main_plot(\"3-1-L2_Policy_Clustering_Intensity.csv\", \"Intensity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2deac23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 3: ç”Ÿæˆ PPT (Strict Order Matching)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "# --- è·¯å¾„é…ç½® ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "# ä¼˜å…ˆæŸ¥æ‰¾çˆ¶çº§ dataï¼Œè‹¥ä¸å­˜åœ¨åˆ™æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹ data\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "ORDER_FILE = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"3-2-fixed_ppt_order.json\"\n",
    "FEATURE_FILE = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "# ä¿®æ”¹ä¸ºç›®æ ‡ç›¸å¯¹è·¯å¾„\n",
    "IMG_BASE_DIR = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\"\n",
    "IMG_BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def create_table(slide, df_policy: pd.DataFrame, sorted_ids: list, l2_name: str) -> None:\n",
    "    \"\"\"\n",
    "    åœ¨å¹»ç¯ç‰‡åº•éƒ¨åˆ›å»ºç‰¹å¾è¡¨æ ¼ï¼Œä¸¥æ ¼æŒ‰ç…§ sorted_ids é¡ºåºæ’åˆ—ã€‚\n",
    "    Args:\n",
    "        slide: pptx å¹»ç¯ç‰‡å¯¹è±¡\n",
    "        df_policy: å½“å‰æ”¿ç­–çš„æ•°æ®é›† DataFrame\n",
    "        sorted_ids: é”å®šçš„èšç±»IDåˆ—è¡¨\n",
    "        l2_name: æ”¿ç­–åç§°\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for cid in sorted_ids:\n",
    "        row_data = df_policy[df_policy['èšç±»ID'] == cid]\n",
    "        if row_data.empty: continue\n",
    "        \n",
    "        r = row_data.iloc[0]\n",
    "        countries = df_policy[df_policy['èšç±»ID'] == cid]['å›½å®¶'].unique()\n",
    "        country_str = \", \".join(countries)\n",
    "        if len(country_str) > 50: country_str = country_str[:47] + \"...\"\n",
    "        \n",
    "        rows.append([\n",
    "            f\"Cluster {cid}\",\n",
    "            f\"{r['Starting']} -> {r['Trend']} -> {r['Ending']}\",\n",
    "            f\"{r['MeanStart']:.3f}\",\n",
    "            f\"{len(countries)} ({country_str})\"\n",
    "        ])\n",
    "\n",
    "    if not rows: return\n",
    "\n",
    "    # è¡¨æ ¼å‚æ•°ï¼šè¡Œæ•°, åˆ—æ•°, å·¦, ä¸Š, å®½, é«˜\n",
    "    table = slide.shapes.add_table(len(rows) + 1, 4, Inches(0.5), Inches(4.5), Inches(9.0), Inches(0.8)).table\n",
    "\n",
    "    # è®¾ç½®åˆ—å®½\n",
    "    widths = [1.2, 2.5, 1.2, 4.1]\n",
    "    for idx, w in enumerate(widths):\n",
    "        table.columns[idx].width = Inches(w)\n",
    "\n",
    "    # è¡¨å¤´è®¾ç½®\n",
    "    headers = [\"ID\", \"Trajectory Features\", \"Mean Start\", \"Countries\"]\n",
    "    for i, h in enumerate(headers):\n",
    "        cell = table.cell(0, i)\n",
    "        cell.text = h\n",
    "        cell.fill.solid()\n",
    "        cell.fill.fore_color.rgb = RGBColor(220, 220, 220)\n",
    "        cell.text_frame.paragraphs[0].font.bold = True\n",
    "        cell.text_frame.paragraphs[0].font.size = Pt(10)\n",
    "\n",
    "    # å†…å®¹å¡«å……\n",
    "    for r_idx, row_content in enumerate(rows):\n",
    "        for c_idx, text in enumerate(row_content):\n",
    "            cell = table.cell(r_idx + 1, c_idx)\n",
    "            cell.text = str(text)\n",
    "            cell.text_frame.paragraphs[0].font.size = Pt(9)\n",
    "            cell.vertical_anchor = 3 # Middle\n",
    "\n",
    "def generate_ppt_for_metric(metric_label: str) -> None:\n",
    "    \"\"\"\n",
    "    ç”ŸæˆæŒ‡å®š Metric (Breadth/Intensity) çš„ PPT æŠ¥å‘Šã€‚\n",
    "    Args:\n",
    "        metric_label: è¯„ä¼°æŒ‡æ ‡åç§°\n",
    "    \"\"\"\n",
    "    if not ORDER_FILE.exists() or not FEATURE_FILE.exists(): return\n",
    "\n",
    "    with open(ORDER_FILE, 'r', encoding='utf-8') as f:\n",
    "        order_map = json.load(f)\n",
    "    \n",
    "    df_all = pd.read_csv(FEATURE_FILE, encoding='utf-8-sig')\n",
    "    df_feats = df_all[df_all['Type'] == metric_label]\n",
    "\n",
    "    prs = Presentation()\n",
    "    prs.slide_width, prs.slide_height = Inches(13.333), Inches(7.5)\n",
    "\n",
    "    for l2 in df_feats['L2æ”¿ç­–ä¸­æ–‡å'].unique():\n",
    "        key = f\"{metric_label}_{l2}\"\n",
    "        if key not in order_map: continue\n",
    "            \n",
    "        sorted_ids = order_map[key]\n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "        \n",
    "        # æ ‡é¢˜\n",
    "        title_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.2), Inches(12), Inches(0.5))\n",
    "        tf = title_box.text_frame\n",
    "        tf.text = f\"{l2} ({metric_label})\"\n",
    "        tf.paragraphs[0].font.bold = True\n",
    "        tf.paragraphs[0].font.size = Pt(24)\n",
    "        \n",
    "        # å›¾ç‰‡è·¯å¾„å¤„ç†\n",
    "        safe_name = l2.replace(\"/\", \"_\").replace(\" \", \"_\").replace(\":\", \"\")\n",
    "        img_path = IMG_BASE_DIR / f\"Plots_{metric_label}\" / f\"{safe_name}_{metric_label}_Sorted.png\"\n",
    "        \n",
    "        if img_path.exists():\n",
    "            slide.shapes.add_picture(str(img_path), Inches(0.5), Inches(0.8), width=Inches(12.3))\n",
    "        else:\n",
    "            # ä»…åœ¨æ‰¾ä¸åˆ°å›¾ç‰‡æ—¶åœ¨PPTå†…æç¤ºï¼Œä¸ä¸­æ–­\n",
    "            err_box = slide.shapes.add_textbox(Inches(4), Inches(3), Inches(5), Inches(1))\n",
    "            err_box.text_frame.text = \"Image Not Found\"\n",
    "\n",
    "        # ç”Ÿæˆè¡¨æ ¼\n",
    "        create_table(slide, df_feats[df_feats['L2æ”¿ç­–ä¸­æ–‡å'] == l2], sorted_ids, l2)\n",
    "\n",
    "    output_path = IMG_BASE_DIR / f\"Report_{metric_label}_Sorted.pptx\"\n",
    "    prs.save(output_path)\n",
    "\n",
    "# --- æ‰§è¡Œ ---\n",
    "generate_ppt_for_metric(\"Breadth\")\n",
    "generate_ppt_for_metric(\"Intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce724e6",
   "metadata": {},
   "source": [
    "### Update the right changeï¼ˆç¡®ä¿æ­£ç¡®åé‡æ–°æ’åºä¸€ä¸‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6cfe6",
   "metadata": {},
   "source": [
    "##### Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "906f17b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ç¬¬ä¸€æ­¥: é‡æ–°æ’åºå¹¶é”å®š JSON...\n",
      "âœ… é¡ºåºå·²æ›´æ–°è‡³: 3-2-fixed_ppt_order.json\n",
      "\n",
      "ğŸš€ ç¬¬äºŒæ­¥: ä½¿ç”¨æ–°é¡ºåºé‡æ–°ç»˜å›¾...\n",
      ">>> æ­£åœ¨å¤„ç† Breadth ...\n",
      ">>> æ­£åœ¨å¤„ç† Intensity ...\n",
      "\n",
      "âœ¨ å…¨éƒ¨å®Œæˆï¼å›¾ç‰‡å·²æŒ‰æœ€æ–°æ’åºé‡æ–°ç”Ÿæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "\n",
    "# ==========================================\n",
    "# é…ç½®ä¸è·¯å¾„\n",
    "# ==========================================\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "FEATURE_FILE = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "ORDER_FILE = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"3-2-fixed_ppt_order.json\"\n",
    "OUTPUT_DIR = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\"\n",
    "\n",
    "NATURE_COLORS = [\n",
    "    '#E64B35', \"#6917C2\", '#00A087', '#3C5488', '#F39B7F', \n",
    "    '#8491B4', '#91D1C2', '#DC0000', '#7E6148', '#B09C85', \n",
    "    '#E18727', '#20854E', '#0072B5', '#BC3C29', '#6F99AD'\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 1. æ ·å¼è®¾ç½® (ä½¿ç”¨æ‚¨æŒ‡å®šçš„é«˜çº§æ ¼å¼)\n",
    "# ==========================================\n",
    "def setup_mpl_single2():\n",
    "    mpl.rc('font', size=25)\n",
    "    mpl.rcParams.update({\n",
    "        'legend.fontsize': 'small', 'xtick.labelsize': 'small', 'ytick.labelsize': 'small',\n",
    "        'lines.linewidth': 2, 'axes.linewidth': 2, 'xtick.major.pad': '12', 'ytick.major.pad': '12',\n",
    "        'xtick.direction': 'in', 'ytick.direction': 'in', 'xtick.top': False, 'ytick.right': False,\n",
    "        'mathtext.default': 'regular', 'axes.titlesize': 'small'\n",
    "    })\n",
    "\n",
    "def lighten_color(color, amount=0.7):\n",
    "    try:\n",
    "        c = mcolors.to_rgb(color)\n",
    "        return tuple([c[i] + (1 - c[i]) * amount for i in range(3)])\n",
    "    except: return (0.5, 0.5, 0.5)\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ’åºé€»è¾‘\n",
    "# ==========================================\n",
    "def get_sort_key(row):\n",
    "    \"\"\"æ’åºä¼˜å…ˆçº§: Starting > Ending > Trend > MeanStart\"\"\"\n",
    "    orders = {\n",
    "        'Starting': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "        'Ending': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "        'Trend': {'Rise': 0, 'Stable': 1, 'Fluctuate': 2, 'Decline': 3}\n",
    "    }\n",
    "    return (\n",
    "        orders['Starting'].get(row['Starting'], 99),\n",
    "        orders['Ending'].get(row['Ending'], 99),\n",
    "        orders['Trend'].get(row['Trend'], 99),\n",
    "        row.get('MeanStart', 0)\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 3. ç»˜å›¾æ ¸å¿ƒ (åº”ç”¨ç‰¹å®šæ ¼å¼)\n",
    "# ==========================================\n",
    "def plot_fixed_order(l2_name, df_data, df_feats, sorted_ids, metric_label):\n",
    "    # æ•°æ®å‡†å¤‡\n",
    "    df_plot = df_data[(df_data['å¹´ä»½'] >= 2005) & (df_data['å¹´ä»½'] <= 2023)]\n",
    "    years = sorted(df_plot['å¹´ä»½'].unique())\n",
    "    matrix = df_plot.pivot(index='å¹´ä»½', columns='å›½å®¶', values='å æ¯”').reindex(years)\n",
    "    overall_mean = matrix.mean(axis=1)\n",
    "    y_max = matrix.max().max() * 1.15\n",
    "\n",
    "    # ç”»å¸ƒè®¾ç½®\n",
    "    n_clusters = len(sorted_ids)\n",
    "    n_cols, n_rows = 3, (n_clusters + 2) // 3\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3.5 * n_rows), squeeze=False)\n",
    "    \n",
    "    for idx, cid in enumerate(sorted_ids):\n",
    "        ax = axes[idx // n_cols, idx % n_cols]\n",
    "        \n",
    "        # æ‰¾å¯¹åº”çš„å›½å®¶\n",
    "        countries = [c for c in df_plot[df_plot['èšç±»ID'] == cid]['å›½å®¶'].unique() if c in matrix.columns]\n",
    "        if not countries: continue\n",
    "\n",
    "        # æ‰¾æ ‡é¢˜\n",
    "        feat_row = df_feats[(df_feats['L2æ”¿ç­–ä¸­æ–‡å'] == l2_name) & (df_feats['èšç±»ID'] == cid)]\n",
    "        if not feat_row.empty:\n",
    "            r = feat_row.iloc[0]\n",
    "            title_str = f\"{r['Starting']}+{r['Trend']}+{str(r['Ending']).replace(' Share', '')}\"\n",
    "        else:\n",
    "            title_str = f\"Cluster {cid}\"\n",
    "\n",
    "        # ç»˜å›¾æ“ä½œ\n",
    "        color = NATURE_COLORS[idx % len(NATURE_COLORS)]\n",
    "        fill_color = lighten_color(color, 0.7)\n",
    "        sub_matrix = matrix[countries]\n",
    "        c_mean = sub_matrix.mean(axis=1)\n",
    "\n",
    "        ax.plot(sub_matrix.index, sub_matrix.values, color=color, alpha=0.25, lw=1.2, zorder=1)\n",
    "        ax.plot(c_mean.index, c_mean, marker='o', color=color, lw=2.5, ms=7,\n",
    "                mfc=fill_color, mec=color, mew=1.8, label='Cluster Average',\n",
    "                markevery=max(1, len(years)//10), zorder=10)\n",
    "        ax.plot(overall_mean.index, overall_mean, color='#000000', ls='--', lw=2.5,\n",
    "                label='Overall Average', alpha=0.85, zorder=9, dashes=(3, 2))\n",
    "\n",
    "        ax.set_title(f\"{title_str}\\n({len(countries)} countries)\", pad=15, ha='center')\n",
    "        ax.set_ylim(bottom=-(y_max * 0.04), top=y_max)\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "        ax.tick_params(axis='x', rotation=90, labelsize=15, pad=5.5)\n",
    "        \n",
    "        if idx % n_cols == 0: ax.set_ylabel(metric_label)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.legend(loc='best', frameon=False, fontsize=12)\n",
    "\n",
    "    # æ¸…ç†å¤šä½™å­å›¾\n",
    "    for j in range(n_clusters, n_rows * n_cols): axes[j // n_cols, j % n_cols].axis('off')\n",
    "    plt.subplots_adjust(wspace=0.25, top=0.85, bottom=0.15, hspace=0.45)\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    safe_name = l2_name.replace(\"/\", \"_\").replace(\" \", \"_\").replace(\":\", \"\")\n",
    "    out_dir_m = OUTPUT_DIR / f\"Plots_{metric_label}\"\n",
    "    out_dir_m.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir_m / f\"{safe_name}_{metric_label}_Sorted.png\"\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# ==========================================\n",
    "# 4. ä¸»æ‰§è¡Œæµç¨‹ (æ’åº -> ç»˜å›¾)\n",
    "# ==========================================\n",
    "def main_resort_and_plot():\n",
    "    setup_mpl_single2() # åº”ç”¨æ ·å¼\n",
    "    \n",
    "    if not FEATURE_FILE.exists():\n",
    "        print(f\"âŒ æœªæ‰¾åˆ°ç‰¹å¾æ–‡ä»¶: {FEATURE_FILE}\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸš€ ç¬¬ä¸€æ­¥: é‡æ–°æ’åºå¹¶é”å®š JSON...\")\n",
    "    df_feats = pd.read_csv(FEATURE_FILE, encoding='utf-8-sig')\n",
    "    order_map = {}\n",
    "    \n",
    "    # 1. ç”Ÿæˆæ’åº\n",
    "    for metric in ['Breadth', 'Intensity']:\n",
    "        sub_df = df_feats[df_feats['Type'] == metric].copy()\n",
    "        if sub_df.empty: continue\n",
    "        sub_df['sort_key'] = sub_df.apply(get_sort_key, axis=1)\n",
    "        sub_df = sub_df.sort_values('sort_key')\n",
    "        \n",
    "        for l2 in sub_df['L2æ”¿ç­–ä¸­æ–‡å'].unique():\n",
    "            ids = sub_df[sub_df['L2æ”¿ç­–ä¸­æ–‡å'] == l2]['èšç±»ID'].unique().tolist()\n",
    "            order_map[f\"{metric}_{l2}\"] = ids\n",
    "            \n",
    "    with open(ORDER_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(order_map, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"âœ… é¡ºåºå·²æ›´æ–°è‡³: {ORDER_FILE.name}\")\n",
    "\n",
    "    # 2. é‡æ–°ç»˜å›¾\n",
    "    print(\"\\nğŸš€ ç¬¬äºŒæ­¥: ä½¿ç”¨æ–°é¡ºåºé‡æ–°ç»˜å›¾...\")\n",
    "    tasks = [('Breadth', '3-1-L2_Policy_Clustering_Breadth.csv'), \n",
    "             ('Intensity', '3-1-L2_Policy_Clustering_Intensity.csv')]\n",
    "             \n",
    "    for metric, fname in tasks:\n",
    "        fpath = DATA_DIR / fname\n",
    "        if not fpath.exists(): continue\n",
    "        df_raw = pd.read_csv(fpath, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\">>> æ­£åœ¨å¤„ç† {metric} ...\")\n",
    "        for l2 in df_raw['L2æ”¿ç­–ä¸­æ–‡å'].dropna().unique():\n",
    "            key = f\"{metric}_{l2}\"\n",
    "            if key in order_map:\n",
    "                plot_fixed_order(l2, df_raw[df_raw['L2æ”¿ç­–ä¸­æ–‡å'] == l2], df_feats, order_map[key], metric)\n",
    "                \n",
    "    print(\"\\nâœ¨ å…¨éƒ¨å®Œæˆï¼å›¾ç‰‡å·²æŒ‰æœ€æ–°æ’åºé‡æ–°ç”Ÿæˆã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_resort_and_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94da284",
   "metadata": {},
   "source": [
    "##### PPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632b8df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ç”Ÿæˆ: Analysis_Report_Breadth_Editable.pptx ...\n",
      "âœ… å®Œæˆ: Analysis_Report_Breadth_Editable.pptx\n",
      "æ­£åœ¨ç”Ÿæˆ: Analysis_Report_Intensity_Editable.pptx ...\n",
      "âœ… å®Œæˆ: Analysis_Report_Intensity_Editable.pptx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "from pathlib import Path\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "# --- é…ç½® ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "FEATURE_FILE = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "ORDER_FILE = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"3-2-fixed_ppt_order.json\"\n",
    "OUTPUT_DIR = DATA_DIR / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"Human_catorgy\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NATURE_COLORS = ['#E64B35', \"#6917C2\", '#00A087', '#3C5488', '#F39B7F', '#8491B4', '#91D1C2', '#DC0000', '#7E6148', '#B09C85', '#E18727', '#20854E', '#0072B5', '#BC3C29', '#6F99AD']\n",
    "\n",
    "def create_img_stream(df_sub: pd.DataFrame, cid: int, color_idx: int, y_max: float, show_y: bool) -> io.BytesIO:\n",
    "    \"\"\"ç”Ÿæˆå•ä¸ªå†…å­˜å›¾ç‰‡æµ\"\"\"\n",
    "    years = sorted(df_sub['å¹´ä»½'].unique())\n",
    "    matrix = df_sub.pivot(index='å¹´ä»½', columns='å›½å®¶', values='å æ¯”').reindex(years)\n",
    "    if matrix.empty: return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(4.2, 2.8))\n",
    "    col = NATURE_COLORS[color_idx % 15]\n",
    "    fill = tuple([c + (1 - c) * 0.7 for c in mcolors.to_rgb(col)])\n",
    "    \n",
    "    sub = matrix[df_sub[df_sub['èšç±»ID'] == cid]['å›½å®¶'].unique()]\n",
    "    ax.plot(sub.index, sub.values, color=col, alpha=0.25, lw=1)\n",
    "    ax.plot(sub.mean(axis=1), marker='o', color=col, lw=2, ms=5, mfc=fill, zorder=10)\n",
    "    \n",
    "    ax.set_ylim(bottom=-(y_max*0.04), top=y_max)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.set_ylabel(\"Metric\" if show_y else \"\")\n",
    "    ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "    plt.subplots_adjust(left=0.18, right=0.97, top=0.95, bottom=0.25)\n",
    "    \n",
    "    stream = io.BytesIO()\n",
    "    plt.savefig(stream, format='png', dpi=200, transparent=True)\n",
    "    plt.close()\n",
    "    stream.seek(0)\n",
    "    return stream\n",
    "\n",
    "def generate_ppt(metric: str, csv_name: str):\n",
    "    \"\"\"åŸºäº JSON é¡ºåºç”Ÿæˆå¯ç¼–è¾‘ PPT\"\"\"\n",
    "    out_ppt = OUTPUT_DIR / f\"Analysis_Report_{metric}_Editable.pptx\"\n",
    "    df_raw = pd.read_csv(DATA_DIR / csv_name, encoding='utf-8-sig')\n",
    "    df_raw = df_raw[(df_raw['å¹´ä»½'] >= 2005) & (df_raw['å¹´ä»½'] <= 2023)]\n",
    "    df_feats = pd.read_csv(FEATURE_FILE, encoding='utf-8-sig')\n",
    "    with open(ORDER_FILE) as f: order_map = json.load(f)\n",
    "\n",
    "    prs = Presentation()\n",
    "    prs.slide_width, prs.slide_height = Inches(13.333), Inches(7.5)\n",
    "    \n",
    "    print(f\"æ­£åœ¨ç”Ÿæˆ: {out_ppt.name} ...\")\n",
    "    for l2 in df_feats[df_feats['Type'] == metric]['L2æ”¿ç­–ä¸­æ–‡å'].unique():\n",
    "        ids = order_map.get(f\"{metric}_{l2}\", [])\n",
    "        if not ids: continue\n",
    "            \n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "        tb = slide.shapes.add_textbox(Inches(0.5), Inches(0.1), Inches(12), Inches(0.8))\n",
    "        tb.text_frame.text = l2\n",
    "        tb.text_frame.paragraphs[0].font.size = Pt(24)\n",
    "        \n",
    "        l2_data = df_raw[df_raw['L2æ”¿ç­–ä¸­æ–‡å'] == l2]\n",
    "        y_max = l2_data['å æ¯”'].max() * 1.15\n",
    "        \n",
    "        for idx, cid in enumerate(ids):\n",
    "            row, col = idx // 3, idx % 3\n",
    "            if row * 3 + col >= 6: break # ç®€å•é™åˆ¶ä¸€é¡µæœ€å¤š6å¼ \n",
    "            \n",
    "            # æ–‡æœ¬æ ‡ç­¾\n",
    "            fr = df_feats[(df_feats['L2æ”¿ç­–ä¸­æ–‡å'] == l2) & (df_feats['èšç±»ID'] == cid)].iloc[0]\n",
    "            txt = f\"{fr['Starting']}+{fr['Trend']}+{str(fr['Ending']).replace(' Share','')} ({len(l2_data[l2_data['èšç±»ID']==cid]['å›½å®¶'].unique())} countries)\"\n",
    "            \n",
    "            x, y = Inches(0.5 + col * 4.15), Inches(1.1 + row * 3.1)\n",
    "            tf = slide.shapes.add_textbox(x, y, Inches(4), Inches(0.35)).text_frame\n",
    "            tf.text = txt; tf.paragraphs[0].alignment = PP_ALIGN.CENTER; tf.paragraphs[0].font.bold = True\n",
    "            \n",
    "            # å›¾ç‰‡\n",
    "            stream = create_img_stream(l2_data, cid, idx, y_max, col == 0)\n",
    "            if stream: slide.shapes.add_picture(stream, x, y + Inches(0.35), width=Inches(4))\n",
    "\n",
    "    prs.save(out_ppt)\n",
    "    print(f\"âœ… å®Œæˆ: {out_ppt.name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_ppt(\"Breadth\", \"3-1-L2_Policy_Clustering_Breadth.csv\")\n",
    "    generate_ppt(\"Intensity\", \"3-1-L2_Policy_Clustering_Intensity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c5329",
   "metadata": {},
   "source": [
    "### æ›´æ–°äººå·¥ç¼–å†™æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5849285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# ==========================================\n",
    "# 1. è·¯å¾„é…ç½®\n",
    "# ==========================================\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "CONFIG_FILE = DATA_DIR / \"config_mappings.json\"\n",
    "\n",
    "# è¾“å…¥æ–‡ä»¶\n",
    "FILE_3_2 = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "FILE_3_1_B = DATA_DIR / \"3-1-L2_Policy_Clustering_Breadth.csv\"\n",
    "FILE_3_1_I = DATA_DIR / \"3-1-L2_Policy_Clustering_Intensity.csv\"\n",
    "\n",
    "# è¾“å‡ºæ–‡ä»¶\n",
    "OUTPUT_FILE = DATA_DIR / \"3-3-Visualizing_Models_by_Country.csv\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. è¾…åŠ©å‡½æ•°\n",
    "# ==========================================\n",
    "def load_mappings(json_path: Path) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    åŠ è½½æ˜ å°„æ–‡ä»¶ï¼Œæ„å»º L2åç§°->ä»£ç  å’Œ L2åç§°->L1åç§° çš„æ˜ å°„å­—å…¸ã€‚\n",
    "    \n",
    "    Args:\n",
    "        json_path: config_mappings.json çš„è·¯å¾„\n",
    "    Returns:\n",
    "        (l2_name_to_code, l2_name_to_l1)\n",
    "    \"\"\"\n",
    "    if not json_path.exists():\n",
    "        return {}, {}\n",
    "    \n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 1. æ„å»º L2 Name -> L2 Code (åè½¬ level2_names)\n",
    "    # åŸå§‹: {\"LEV2_XXX\": \"Name\"} -> ç›®æ ‡: {\"Name\": \"LEV2_XXX\"}\n",
    "    l2_name_to_code = {v: k for k, v in config.get('level2_names', {}).items()}\n",
    "    \n",
    "    # 2. æ„å»º L2 Name -> L1 Name\n",
    "    # éå† level_mapping (ç»“æ„ä¸º LEV3 -> {L2:..., L1:...})\n",
    "    l2_name_to_l1 = {}\n",
    "    for item in config.get('level_mapping', {}).values():\n",
    "        l2_name = item.get('L2')\n",
    "        l1_name = item.get('L1')\n",
    "        if l2_name and l1_name:\n",
    "            l2_name_to_l1[l2_name] = l1_name\n",
    "            \n",
    "    return l2_name_to_code, l2_name_to_l1\n",
    "\n",
    "def format_ending(val: str) -> str:\n",
    "    \"\"\"æ ¼å¼åŒ– Ending å­—æ®µï¼Œç¡®ä¿ä»¥ ' Share' ç»“å°¾\"\"\"\n",
    "    s_val = str(val)\n",
    "    if s_val.lower() == 'nan': return \"\"\n",
    "    return s_val if s_val.endswith(\" Share\") else f\"{s_val} Share\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. ä¸»é€»è¾‘\n",
    "# ==========================================\n",
    "def generate_final_table():\n",
    "    \"\"\"è¯»å– 3-1 å’Œ 3-2 æ•°æ®ï¼Œç»“åˆæ˜ å°„æ–‡ä»¶ï¼Œç”ŸæˆåŒ…å« Type çš„æœ€ç»ˆå®½è¡¨\"\"\"\n",
    "    \n",
    "    # 1. å‡†å¤‡æ•°æ®ä¸æ˜ å°„\n",
    "    l2_code_map, l2_l1_map = load_mappings(CONFIG_FILE)\n",
    "    \n",
    "    if not FILE_3_2.exists():\n",
    "        return\n",
    "    \n",
    "    df_features = pd.read_csv(FILE_3_2, encoding='utf-8-sig')\n",
    "    \n",
    "    tasks = [\n",
    "        (FILE_3_1_B, \"Breadth\"),\n",
    "        (FILE_3_1_I, \"Intensity\")\n",
    "    ]\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    for fpath, type_label in tasks:\n",
    "        if not fpath.exists(): continue\n",
    "            \n",
    "        # è¯»å–é¢æ¿æ•°æ® (3-1)\n",
    "        df_panel = pd.read_csv(fpath, encoding='utf-8-sig')\n",
    "        \n",
    "        # ç­›é€‰å¯¹åº”ç±»å‹çš„ç‰¹å¾æ•°æ® (3-2)\n",
    "        df_feats_sub = df_features[df_features['Type'] == type_label].copy()\n",
    "        \n",
    "        # åˆå¹¶: é¢æ¿æ•°æ® + ç‰¹å¾æ•°æ®\n",
    "        # è¿æ¥é”®: L2æ”¿ç­–ä¸­æ–‡å, å›½å®¶, èšç±»ID\n",
    "        merged = pd.merge(\n",
    "            df_panel, \n",
    "            df_feats_sub[['L2æ”¿ç­–ä¸­æ–‡å', 'å›½å®¶', 'èšç±»ID', 'Starting', 'Trend', 'Ending']], \n",
    "            on=['L2æ”¿ç­–ä¸­æ–‡å', 'å›½å®¶', 'èšç±»ID'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # æ˜ç¡®æŒ‡å®š Type\n",
    "        merged['Type'] = type_label\n",
    "        \n",
    "        # æ˜ å°„æ–°å­—æ®µ\n",
    "        merged['L2æ”¿ç­–'] = merged['L2æ”¿ç­–ä¸­æ–‡å'].map(l2_code_map)\n",
    "        merged['L1åˆ†ç±»'] = merged['L2æ”¿ç­–ä¸­æ–‡å'].map(l2_l1_map)\n",
    "        merged['L1åˆ†ç±»ä¸­æ–‡å'] = merged['L1åˆ†ç±»'] # ç¤ºä¾‹ä¸­ L1 ä¸­æ–‡åä¸è‹±æ–‡åä¸€è‡´\n",
    "        \n",
    "        # æ ¼å¼åŒ– Ending\n",
    "        merged['Ending'] = merged['Ending'].apply(format_ending)\n",
    "        \n",
    "        # æ”¶é›†ç»“æœ\n",
    "        all_data.append(merged)\n",
    "\n",
    "    if not all_data:\n",
    "        return\n",
    "\n",
    "    # 2. åˆå¹¶ä¸å­—æ®µæ•´ç†\n",
    "    df_final = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # å®šä¹‰æœ€ç»ˆåˆ—é¡ºåº (åŒ…å« Type)\n",
    "    target_cols = [\n",
    "        'L2æ”¿ç­–', 'L2æ”¿ç­–ä¸­æ–‡å', 'L1åˆ†ç±»', 'L1åˆ†ç±»ä¸­æ–‡å', \n",
    "        'èšç±»ID', 'å›½å®¶', 'å¹´ä»½', 'å æ¯”', \n",
    "        'Starting', 'Trend', 'Ending', 'Type'\n",
    "    ]\n",
    "    \n",
    "    # ç¡®ä¿åˆ—å­˜åœ¨ï¼Œå¹¶é‡æ–°æ’åº\n",
    "    final_cols = [c for c in target_cols if c in df_final.columns]\n",
    "    df_final = df_final[final_cols]\n",
    "    \n",
    "    # æ’åºä¼˜åŒ–ä½“éªŒ\n",
    "    df_final = df_final.sort_values(by=['Type', 'L2æ”¿ç­–', 'å›½å®¶', 'å¹´ä»½'])\n",
    "    \n",
    "    # 3. ä¿å­˜\n",
    "    df_final.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_final_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
