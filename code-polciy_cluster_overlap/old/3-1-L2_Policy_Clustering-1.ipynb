{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65d9f19",
   "metadata": {},
   "source": [
    "### Cluster generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44676d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 1: èšç±»åˆ†æ (Clustering)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Š\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. æ ¸å¿ƒèšç±»ç®—æ³•\n",
    "def auto_select_k_and_cluster(X_data: np.ndarray, max_k: int = 10) -> np.ndarray:\n",
    "    \"\"\"è‡ªåŠ¨é€‰æ‹©æœ€ä½³Kå€¼å¹¶è¿”å›èšç±»æ ‡ç­¾ (åŸºäºè‚˜éƒ¨æ³•åˆ™-æœ€å¤§é™å¹…)\"\"\"\n",
    "    n_samples = len(X_data)\n",
    "    if n_samples < 3:\n",
    "        return np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    Z = linkage(X_data, method='ward')\n",
    "    \n",
    "    wcss_list = []\n",
    "    valid_ks = list(range(2, min(max_k, n_samples) + 1))\n",
    "    \n",
    "    for k in valid_ks:\n",
    "        labels = fcluster(Z, k, criterion='maxclust')\n",
    "        wcss = 0\n",
    "        for i in range(1, k + 1):\n",
    "            cluster_points = X_data[labels == i]\n",
    "            if len(cluster_points) > 0:\n",
    "                center = cluster_points.mean(axis=0)\n",
    "                wcss += np.sum((cluster_points - center) ** 2)\n",
    "        wcss_list.append(wcss)\n",
    "    \n",
    "    if len(wcss_list) < 2:\n",
    "        best_k = valid_ks[0]\n",
    "    else:\n",
    "        deltas = [wcss_list[i] - wcss_list[i+1] for i in range(len(wcss_list)-1)]\n",
    "        best_idx = np.argmax(deltas)\n",
    "        best_k = valid_ks[best_idx + 1]\n",
    "    \n",
    "    return fcluster(Z, best_k, criterion='maxclust') - 1\n",
    "\n",
    "def run_clustering_single_metric(df_source: pd.DataFrame, l2_name: str, need_scale: bool = False) -> pd.Series:\n",
    "    \"\"\"å¤„ç†å•æ¡L2æ”¿ç­–\"\"\"\n",
    "    # === ä¿®æ”¹ç‚¹1ï¼šä¸¥æ ¼é™åˆ¶å¹´ä»½ 2005-2023 ===\n",
    "    df = df_source[(df_source['TIME_PERIOD'] >= 2005) & (df_source['TIME_PERIOD'] <= 2023)]\n",
    "    \n",
    "    X = df.set_index(['REF_AREA', 'TIME_PERIOD'])[l2_name].unstack().fillna(0)\n",
    "    \n",
    "    if X.empty:\n",
    "        return pd.Series(dtype=int)\n",
    "\n",
    "    if need_scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        if X.values.max() == X.values.min():\n",
    "            X_vals = X.values\n",
    "        else:\n",
    "            X_vals = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_vals = X.values\n",
    "        \n",
    "    labels = auto_select_k_and_cluster(X_vals)\n",
    "    return pd.Series(labels, index=X.index, name='ClusterID')\n",
    "\n",
    "# 2. æ•°æ® IO\n",
    "def load_data(data_dir: Path):\n",
    "    df_b = pd.read_parquet(data_dir / \"2-1-country_breadth.parquet\")\n",
    "    df_i = pd.read_parquet(data_dir / \"2-1-country_intensity.parquet\")\n",
    "    l2_list = [c for c in df_b.columns if c not in {'REF_AREA', 'TIME_PERIOD'}]\n",
    "    return df_b, df_i, l2_list\n",
    "\n",
    "def save_clustered_data(cluster_results: Dict[str, pd.Series], df_source: pd.DataFrame, output_path: Path):\n",
    "    records = []\n",
    "    # åŒæ ·ç¡®ä¿æºæ•°æ®ä¹Ÿæ˜¯ 2005-2023ï¼Œä»¥ä¾¿ä¿å­˜æ—¶ä¸€è‡´\n",
    "    df_source_filtered = df_source[(df_source['TIME_PERIOD'] >= 2005) & (df_source['TIME_PERIOD'] <= 2023)]\n",
    "    \n",
    "    for l2, clusters in cluster_results.items():\n",
    "        if clusters.empty: continue\n",
    "        \n",
    "        sub = df_source_filtered[['REF_AREA', 'TIME_PERIOD', l2]].copy()\n",
    "        sub.columns = ['å›½å®¶', 'å¹´ä»½', 'å æ¯”'] # ç»Ÿä¸€åˆ—å\n",
    "        sub['èšç±»ID'] = sub['å›½å®¶'].map(clusters)\n",
    "        sub['L2æ”¿ç­–'] = l2\n",
    "        sub['L2æ”¿ç­–ä¸­æ–‡å'] = l2 # å ä½ï¼Œç¡®ä¿åˆ—å­˜åœ¨\n",
    "        \n",
    "        sub = sub.dropna(subset=['èšç±»ID'])\n",
    "        sub['èšç±»ID'] = sub['èšç±»ID'].astype(int)\n",
    "        records.append(sub)\n",
    "        \n",
    "    if records:\n",
    "        final_df = pd.concat(records)\n",
    "        final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ… å·²ä¿å­˜: {output_path.name}\")\n",
    "\n",
    "# 3. æ‰§è¡Œ\n",
    "base_dir = Path.cwd().parent\n",
    "data_dir = base_dir / \"data\" if (base_dir / \"data\").exists() else Path.cwd() / \"data\"\n",
    "print(f\"æ•°æ®ç›®å½•: {data_dir}\")\n",
    "\n",
    "df_b, df_i, l2_list = load_data(data_dir)\n",
    "\n",
    "print(\"æ­£åœ¨æ‰§è¡Œ Breadth èšç±» (2005-2023)...\")\n",
    "b_results = {l2: run_clustering_single_metric(df_b, l2, need_scale=False) for l2 in l2_list}\n",
    "save_clustered_data(b_results, df_b, data_dir / \"3-1-L2_Policy_Clustering_Breadth.csv\")\n",
    "\n",
    "print(\"æ­£åœ¨æ‰§è¡Œ Intensity èšç±» (2005-2023)...\")\n",
    "i_results = {l2: run_clustering_single_metric(df_i, l2, need_scale=True) for l2 in l2_list}\n",
    "save_clustered_data(i_results, df_i, data_dir / \"3-1-L2_Policy_Clustering_Intensity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f8dd0",
   "metadata": {},
   "source": [
    "### Table generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a56c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 1: ç”Ÿæˆå…¨é‡æ•°æ® -> æ¨¡æ‹Ÿ Cell 3 æ’åº -> é”å®šé¡ºåº\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# --- è·¯å¾„é…ç½® ---\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "OUTPUT_CSV = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "OUTPUT_JSON = DATA_DIR / \"fixed_ppt_order.json\"\n",
    "\n",
    "# --- 1. ç‰¹å¾è®¡ç®— (æ¥è‡ª Cell 12) ---\n",
    "def calculate_cluster_metrics(df_sub: pd.DataFrame, is_intensity: bool) -> Dict[int, Dict[str, Any]]:\n",
    "    th_high = 6.0 if is_intensity else 0.6\n",
    "    th_med = 3.0 if is_intensity else 0.3\n",
    "    th_slope = 1.5 if is_intensity else 0.15\n",
    "\n",
    "    df_calc = df_sub[(df_sub['å¹´ä»½'] >= 2005) & (df_sub['å¹´ä»½'] <= 2023)]\n",
    "    # æ³¨æ„ï¼šgroupby é»˜è®¤ä¼šæŒ‰ key æ’åºï¼Œè¿™é‡Œä¿è¯äº† trends çš„ index æ˜¯æœ‰åºçš„\n",
    "    trends = df_calc.groupby(['èšç±»ID', 'å¹´ä»½'])['å æ¯”'].mean().unstack()\n",
    "    \n",
    "    features = {}\n",
    "    for cid, row in trends.iterrows():\n",
    "        ts = row.dropna()\n",
    "        if len(ts) < 2:\n",
    "            features[cid] = {'Starting': 'Low', 'Trend': 'Stable', 'Ending': 'Low', 'MeanStart': 0}\n",
    "            continue\n",
    "            \n",
    "        start_val = ts.iloc[:3].mean()\n",
    "        end_val = ts.iloc[-3].mean()\n",
    "        slope = end_val - start_val\n",
    "        std = ts.std()\n",
    "        \n",
    "        if start_val > th_high: s_lbl = 'High'\n",
    "        elif start_val > th_med: s_lbl = 'Medium'\n",
    "        else: s_lbl = 'Low'\n",
    "        \n",
    "        if end_val > th_high: e_lbl = 'High'\n",
    "        elif end_val > th_med: e_lbl = 'Medium'\n",
    "        else: e_lbl = 'Low'\n",
    "        \n",
    "        if slope > th_slope: t_lbl = 'Rise'\n",
    "        elif slope < -0.05: t_lbl = 'Decline'\n",
    "        elif std > (1.0 if is_intensity else 0.1): t_lbl = 'Fluctuate'\n",
    "        else: t_lbl = 'Stable'\n",
    "        \n",
    "        features[cid] = {\n",
    "            'Starting': s_lbl, 'Trend': t_lbl, 'Ending': e_lbl, 'MeanStart': start_val\n",
    "        }\n",
    "    return features\n",
    "\n",
    "# --- 2. æ’åºé€»è¾‘ (100% å¤åˆ» Cell 3) ---\n",
    "def sort_clusters_original_logic(sort_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"å®Œå…¨ä½¿ç”¨ Cell 3 çš„æ’åº Key é€»è¾‘\"\"\"\n",
    "    orders = {\n",
    "        'Starting': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "        'Ending': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "        'Trend': {'Rise': 0, 'Stable': 1, 'Fluctuate': 2, 'Decline': 3}\n",
    "    }\n",
    "    \n",
    "    df = sort_df.copy()\n",
    "    df['sort_key'] = df.apply(lambda r: (\n",
    "        orders['Starting'].get(r['Starting'], 99),\n",
    "        orders['Ending'].get(r['Ending'], 99),\n",
    "        orders['Trend'].get(r['Trend'], 99),\n",
    "        r.get('MeanStart', 0)\n",
    "    ), axis=1)\n",
    "    \n",
    "    return df.sort_values('sort_key')\n",
    "\n",
    "# --- 3. ä¸»æ‰§è¡Œæµç¨‹ ---\n",
    "def process_and_lock_strict():\n",
    "    print(\"ğŸš€ å¼€å§‹æ‰§è¡Œï¼šç”Ÿæˆæ•°æ® -> å…¨é‡æ’åº -> é”å®š ID\")\n",
    "    \n",
    "    all_records = []\n",
    "    \n",
    "    # === æ­¥éª¤ A: ç”Ÿæˆæ‰€æœ‰æ•°æ® (æ¨¡æ‹Ÿ Cell 12 çš„å†™å…¥è¿‡ç¨‹) ===\n",
    "    tasks = [\n",
    "        (\"3-1-L2_Policy_Clustering_Breadth.csv\", \"Breadth\", False),\n",
    "        (\"3-1-L2_Policy_Clustering_Intensity.csv\", \"Intensity\", True)\n",
    "    ]\n",
    "\n",
    "    for fname, m_type, is_intensity in tasks:\n",
    "        fpath = DATA_DIR / fname\n",
    "        if not fpath.exists():\n",
    "            print(f\"âš ï¸ è·³è¿‡æ–‡ä»¶: {fname}\")\n",
    "            continue\n",
    "\n",
    "        df_raw = pd.read_csv(fpath, encoding='utf-8-sig')\n",
    "        \n",
    "        # ä¸¥æ ¼æŒ‰ç…§ Cell 12 çš„å¾ªç¯é¡ºåºç”Ÿæˆè®°å½•\n",
    "        for l2 in df_raw['L2æ”¿ç­–ä¸­æ–‡å'].dropna().unique():\n",
    "            l2_data = df_raw[df_raw['L2æ”¿ç­–ä¸­æ–‡å'] == l2]\n",
    "            feats_map = calculate_cluster_metrics(l2_data, is_intensity)\n",
    "            \n",
    "            # æ¨¡æ‹Ÿ Cell 12 çš„ iterrows å±•å¼€\n",
    "            for _, row in l2_data.iterrows():\n",
    "                cid = row['èšç±»ID']\n",
    "                if cid in feats_map:\n",
    "                    f = feats_map[cid]\n",
    "                    all_records.append({\n",
    "                        'L2æ”¿ç­–ä¸­æ–‡å': l2,\n",
    "                        'å›½å®¶': row['å›½å®¶'],\n",
    "                        'èšç±»ID': cid,\n",
    "                        'Starting': f['Starting'],\n",
    "                        'Trend': f['Trend'],\n",
    "                        'Ending': f['Ending'],\n",
    "                        'MeanStart': f['MeanStart'],\n",
    "                        'Type': m_type\n",
    "                    })\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ•°æ®\")\n",
    "        return\n",
    "\n",
    "    # === æ­¥éª¤ B: æ„å»º DataFrame å¹¶å»é‡ (æ¨¡æ‹Ÿ Cell 12 çš„è¾“å‡º) ===\n",
    "    df_full = pd.DataFrame(all_records).drop_duplicates(subset=['L2æ”¿ç­–ä¸­æ–‡å', 'å›½å®¶', 'Type'])\n",
    "    \n",
    "    # ä¿å­˜ CSV (è¿™æ˜¯ Cell 3 è¯»å–çš„å¯¹è±¡)\n",
    "    df_full.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… CSV å·²ä¿å­˜: {OUTPUT_CSV.name}\")\n",
    "\n",
    "    # === æ­¥éª¤ C: æ¨¡æ‹Ÿ Cell 3 çš„è¯»å–å’Œæ’åºè¿‡ç¨‹ ===\n",
    "    # ä¸ºäº†ç»å¯¹ä¸¥è°¨ï¼Œæˆ‘ä»¬ç›´æ¥å¯¹ df_full è¿›è¡Œæ“ä½œï¼Œå°±åƒ Cell 3 è¯»è¿›æ¥ä¸€æ ·\n",
    "    order_map = {}\n",
    "    \n",
    "    # 1. ç­›é€‰ metric (Cell 3 æ˜¯åˆ†å¼€è·‘çš„ï¼Œè¿™é‡Œæˆ‘ä»¬å¾ªç¯è·‘)\n",
    "    for m_type in ['Breadth', 'Intensity']:\n",
    "        df_metric = df_full[df_full['Type'] == m_type].copy()\n",
    "        if df_metric.empty: continue\n",
    "        \n",
    "        # 2. å…¨é‡æ’åº (Cell 3 çš„å…³é”®æ­¥éª¤: sort_clusters(df_meta))\n",
    "        # è¿™ä¼šæ ¹æ® Starting/Ending ç­‰å¯¹æ•´ä¸ªè¡¨æ’åº\n",
    "        df_metric_sorted = sort_clusters_original_logic(df_metric)\n",
    "        \n",
    "        # 3. å¾ªç¯ L2 æå– ID (Cell 3 çš„ç»˜å›¾å¾ªç¯)\n",
    "        for l2 in df_metric_sorted['L2æ”¿ç­–ä¸­æ–‡å'].unique():\n",
    "            # æå–è¯¥ L2 ä¸‹çš„æ‰€æœ‰è¡Œ\n",
    "            l2_sort_info = df_metric_sorted[df_metric_sorted['L2æ”¿ç­–ä¸­æ–‡å'] == l2]\n",
    "            \n",
    "            # Cell 3 é‡Œçš„é€»è¾‘: l2_sort_info.sort_values('sort_key')['èšç±»ID'].unique()\n",
    "            # è™½ç„¶ df_metric_sorted å·²ç»æ˜¯æ’å¥½åºçš„ï¼Œä½† Cell 3 é‡Œä¸ºäº†ä¿é™©åˆ sort äº†ä¸€æ¬¡\n",
    "            # æˆ‘ä»¬è¿™é‡Œä¹Ÿç…§åšï¼Œç¡®ä¿è¡Œä¸ºä¸€è‡´\n",
    "            final_sorted_df = sort_clusters_original_logic(l2_sort_info)\n",
    "            sorted_ids = final_sorted_df['èšç±»ID'].unique().tolist()\n",
    "            \n",
    "            order_map[f\"{m_type}_{l2}\"] = sorted_ids\n",
    "\n",
    "    # === æ­¥éª¤ D: é”å®š JSON ===\n",
    "    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(order_map, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"âœ… é¡ºåºå·²ä¸¥æ ¼é”å®šè‡³ JSON: {OUTPUT_JSON.name}\")\n",
    "\n",
    "# è¿è¡Œ\n",
    "process_and_lock_strict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c6e59",
   "metadata": {},
   "source": [
    "### Wait Human change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce724e6",
   "metadata": {},
   "source": [
    "### Wait Human change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b78036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¤„ç†æ•°æ®å¹¶ä¿å­˜è‡³: f:\\Desktop\\ç§‘ç ”é¡¹ç›®\\1.è´Ÿè´£ç§‘ç ”é¡¹ç›®\\Climate Policy\\CAMPF_Supplementary\\data\\3-2-Automated_Recognition_Mode.csv\n",
      "âœ… æˆåŠŸä¿å­˜ 1470 æ¡ç‰¹å¾è®°å½•ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cell 12: è®¡ç®—ç‰¹å¾å¹¶ä¿å­˜æ–‡ä»¶ (Generate Metadata)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# è·¯å¾„é…ç½®\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\" if (BASE_DIR / \"data\").exists() else Path.cwd() / \"data\"\n",
    "OUTPUT_FILE = DATA_DIR / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "\n",
    "def calculate_cluster_metrics(df_sub: pd.DataFrame, is_intensity: bool) -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æŒ‡å®šL2æ”¿ç­–ä¸‹å„èšç±»çš„å½¢æ€ç‰¹å¾ (High/Low, Rising/Stableç­‰)ã€‚\n",
    "    \"\"\"\n",
    "    # é˜ˆå€¼è®¾å®š\n",
    "    th_high = 6.0 if is_intensity else 0.6\n",
    "    th_med = 3.0 if is_intensity else 0.3\n",
    "    th_slope = 1.5 if is_intensity else 0.15\n",
    "\n",
    "    # ç­›é€‰å¹´ä»½å¹¶è®¡ç®—å‡å€¼æ›²çº¿\n",
    "    df_calc = df_sub[(df_sub['å¹´ä»½'] >= 2005) & (df_sub['å¹´ä»½'] <= 2023)]\n",
    "    trends = df_calc.groupby(['èšç±»ID', 'å¹´ä»½'])['å æ¯”'].mean().unstack()\n",
    "    \n",
    "    features = {}\n",
    "    for cid, row in trends.iterrows():\n",
    "        ts = row.dropna()\n",
    "        if len(ts) < 2:\n",
    "            features[cid] = {'Starting': 'Low', 'Trend': 'Stable', 'Ending': 'Low', 'MeanStart': 0}\n",
    "            continue\n",
    "            \n",
    "        start_val = ts.iloc[:3].mean()\n",
    "        end_val = ts.iloc[-3].mean()\n",
    "        slope = end_val - start_val\n",
    "        std = ts.std()\n",
    "        \n",
    "        # åˆ¤å®š Starting\n",
    "        if start_val > th_high: s_lbl = 'High'\n",
    "        elif start_val > th_med: s_lbl = 'Medium'\n",
    "        else: s_lbl = 'Low'\n",
    "        \n",
    "        # åˆ¤å®š Ending\n",
    "        if end_val > th_high: e_lbl = 'High'\n",
    "        elif end_val > th_med: e_lbl = 'Medium'\n",
    "        else: e_lbl = 'Low'\n",
    "        \n",
    "        # åˆ¤å®š Trend\n",
    "        if slope > th_slope: t_lbl = 'Rise'\n",
    "        elif slope < -0.05: t_lbl = 'Decline'\n",
    "        elif std > (1.0 if is_intensity else 0.1): t_lbl = 'Fluctuate'\n",
    "        else: t_lbl = 'Stable'\n",
    "        \n",
    "        features[cid] = {\n",
    "            'Starting': s_lbl, 'Trend': t_lbl, 'Ending': e_lbl, 'MeanStart': start_val\n",
    "        }\n",
    "    return features\n",
    "\n",
    "def generate_metadata(file_name: str, metric_type: str) -> pd.DataFrame:\n",
    "    \"\"\"è¯»å–èšç±»ç»“æœï¼Œç”ŸæˆåŒ…å«ç‰¹å¾æ ‡ç­¾çš„å…ƒæ•°æ®è¡¨\"\"\"\n",
    "    file_path = DATA_DIR / file_name\n",
    "    if not file_path.exists():\n",
    "        print(f\"âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°: {file_name}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "    is_intensity = (metric_type == 'Intensity')\n",
    "    records = []\n",
    "\n",
    "    for l2 in df['L2æ”¿ç­–ä¸­æ–‡å'].unique():\n",
    "        l2_data = df[df['L2æ”¿ç­–ä¸­æ–‡å'] == l2]\n",
    "        feats_map = calculate_cluster_metrics(l2_data, is_intensity)\n",
    "        \n",
    "        # å°†ç‰¹å¾æ˜ å°„å›è¯¥èšç±»çš„æ‰€æœ‰å›½å®¶\n",
    "        # ä¸ºäº†å»é‡ï¼Œæˆ‘ä»¬åªéœ€ä¿ç•™ (L2, ClusterID) çš„å”¯ä¸€ç»„åˆå³å¯ï¼Œ\n",
    "        # ä½†ä¸ºäº†åç»­æ–¹ä¾¿ï¼Œè¿™é‡Œä¿ç•™å›½å®¶å­—æ®µå¹¶åœ¨è¿”å›æ—¶å»é‡\n",
    "        for _, row in l2_data.iterrows():\n",
    "            cid = row['èšç±»ID']\n",
    "            if cid in feats_map:\n",
    "                f = feats_map[cid]\n",
    "                records.append({\n",
    "                    'L2æ”¿ç­–ä¸­æ–‡å': l2,\n",
    "                    'å›½å®¶': row['å›½å®¶'],\n",
    "                    'èšç±»ID': cid,\n",
    "                    'Starting': f['Starting'],\n",
    "                    'Trend': f['Trend'],\n",
    "                    'Ending': f['Ending'],\n",
    "                    'MeanStart': f['MeanStart'],\n",
    "                    'Type': metric_type\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(records).drop_duplicates(subset=['L2æ”¿ç­–ä¸­æ–‡å', 'å›½å®¶', 'Type'])\n",
    "\n",
    "# --- ä¸»æ‰§è¡Œæµç¨‹ ---\n",
    "print(f\"æ­£åœ¨å¤„ç†æ•°æ®å¹¶ä¿å­˜è‡³: {OUTPUT_FILE}\")\n",
    "df_breadth = generate_metadata(\"3-1-L2_Policy_Clustering_Breadth.csv\", \"Breadth\")\n",
    "df_intensity = generate_metadata(\"3-1-L2_Policy_Clustering_Intensity.csv\", \"Intensity\")\n",
    "\n",
    "if not df_breadth.empty and not df_intensity.empty:\n",
    "    full_df = pd.concat([df_breadth, df_intensity], ignore_index=True)\n",
    "    full_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… æˆåŠŸä¿å­˜ {len(full_df)} æ¡ç‰¹å¾è®°å½•ã€‚\")\n",
    "else:\n",
    "    print(\"âŒ ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æºæ–‡ä»¶ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99bd64e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“‚ å¤„ç†æ–‡ä»¶: Analysis_Report_Breadth_Editable.pptx\n",
      "ğŸ¯ ç›®æ ‡ç±»å‹: Breadth\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalize_spaces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    177\u001b[39m     sync_ppt_robust(ppt_dir / \u001b[33m\"\u001b[39m\u001b[33mAnalysis_Report_Intensity_Editable.pptx\u001b[39m\u001b[33m\"\u001b[39m, csv_file, \u001b[33m\"\u001b[39m\u001b[33mIntensity\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    171\u001b[39m csv_file = data_dir / \u001b[33m\"\u001b[39m\u001b[33m3-2-Automated_Recognition_Mode.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# 1. Breadth\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43msync_ppt_robust\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppt_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnalysis_Report_Breadth_Editable.pptx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBreadth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# 2. Intensity\u001b[39;00m\n\u001b[32m    177\u001b[39m sync_ppt_robust(ppt_dir / \u001b[33m\"\u001b[39m\u001b[33mAnalysis_Report_Intensity_Editable.pptx\u001b[39m\u001b[33m\"\u001b[39m, csv_file, \u001b[33m\"\u001b[39m\u001b[33mIntensity\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36msync_ppt_robust\u001b[39m\u001b[34m(ppt_path, csv_path, target_type)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# 4. é€ä¸€æ›´æ–°\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m shape, cid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(label_shapes, sorted_cids):\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     raw_text = \u001b[43mnormalize_spaces\u001b[49m(shape.text_frame.text)\n\u001b[32m    114\u001b[39m     parts = raw_text.split(\u001b[33m'\u001b[39m\u001b[33m+\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts) >= \u001b[32m3\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'normalize_spaces' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "from pathlib import Path\n",
    "from pptx.util import Inches\n",
    "from typing import List, Optional\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. å¼ºåŠ›æ¸…æ´—å‡½æ•° (è§£å†³æ¨ªæ ä¸åŒ¹é…é—®é¢˜)\n",
    "# ==========================================\n",
    "def normalize_title_robust(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ç»ˆææ¸…æ´—ï¼š\n",
    "    1. è½¬å°å†™\n",
    "    2. æŠŠæ‰€æœ‰ç±»å‹çš„æ¨ªæ  (En-dash, Em-dash) ç»Ÿä¸€æ›¿æ¢ä¸ºæ™®é€šå‡å· '-'\n",
    "    3. å»é™¤å¤šä½™ç©ºæ ¼\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): return str(text) if text is not None else \"\"\n",
    "    \n",
    "    # æ›¿æ¢å„ç§å¥‡æ€ªçš„æ¨ªæ ä¸ºæ™®é€šå‡å·\n",
    "    text = text.replace('\\u2013', '-').replace('\\u2014', '-').replace('â€“', '-')\n",
    "    \n",
    "    # æ›¿æ¢ä¸æ¢è¡Œç©ºæ ¼\n",
    "    text = text.replace('\\xa0', ' ').replace('\\n', ' ')\n",
    "    \n",
    "    # å»é™¤å¤šä½™ç©ºæ ¼å¹¶è½¬å°å†™\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def get_slide_title(slide) -> Optional[str]:\n",
    "    # ä¼˜å…ˆå–æ ‡å‡†æ ‡é¢˜\n",
    "    if slide.shapes.title and slide.shapes.title.text.strip():\n",
    "        return slide.shapes.title.text.strip() # è¿”å›åŸå§‹æ–‡æœ¬ï¼Œæ¸…æ´—åœ¨åé¢åš\n",
    "    \n",
    "    # å›é€€ï¼šå–æœ€ä¸Šé¢çš„æ–‡æœ¬æ¡†\n",
    "    candidates = []\n",
    "    for shape in slide.shapes:\n",
    "        if shape.has_text_frame and shape.text_frame.text.strip():\n",
    "            candidates.append((shape.top, shape.text_frame.text.strip()))\n",
    "    \n",
    "    if candidates:\n",
    "        candidates.sort(key=lambda x: x[0])\n",
    "        if candidates[0][0] < Inches(2.0):\n",
    "            return candidates[0][1]\n",
    "    return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. ç¨³å®šæ’åºé€»è¾‘ (ä»…åŸºäºæ•°å€¼)\n",
    "# ==========================================\n",
    "def get_sorted_cluster_ids(df_sub: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ MeanStart (æ•°å€¼) ä½œä¸ºå”¯ä¸€çš„æ’åºé”šç‚¹ã€‚\n",
    "    \"\"\"\n",
    "    df_sub = df_sub.copy()\n",
    "    # æ’åºé”®ï¼šMeanStart (ä¸»), èšç±»ID (æ¬¡)\n",
    "    df_sub['sort_key'] = df_sub.apply(lambda r: (r.get('MeanStart', 0), r.get('èšç±»ID', 0)), axis=1)\n",
    "    return df_sub.sort_values('sort_key')['èšç±»ID'].unique().tolist()\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ ¸å¿ƒåŒæ­¥é€»è¾‘\n",
    "# ==========================================\n",
    "def sync_ppt_robust(ppt_path: Path, csv_path: Path, target_type: str) -> None:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ“‚ å¤„ç†æ–‡ä»¶: {ppt_path.name}\")\n",
    "    print(f\"ğŸ¯ ç›®æ ‡ç±»å‹: {target_type}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if not ppt_path.exists():\n",
    "        print(\"âŒ æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "        return\n",
    "\n",
    "    prs = Presentation(ppt_path)\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "    \n",
    "    # === å…³é”®ï¼šåˆ›å»ºä¸€åˆ—æ ‡å‡†åŒ–çš„æ ‡é¢˜ç”¨äºåŒ¹é… ===\n",
    "    # è¿™æ · CSV é‡Œçš„ \"Buildings â€“ market\" å’Œ PPT é‡Œçš„ \"Buildings - market\" å°±èƒ½åŒ¹é…ä¸Šäº†\n",
    "    df['Match_Title'] = df['L2æ”¿ç­–ä¸­æ–‡å'].apply(normalize_title_robust)\n",
    "    \n",
    "    update_count = 0\n",
    "    \n",
    "    for slide_idx, slide in enumerate(prs.slides):\n",
    "        raw_ppt_title = get_slide_title(slide)\n",
    "        if not raw_ppt_title: continue\n",
    "        \n",
    "        # æ ‡å‡†åŒ– PPT æ ‡é¢˜\n",
    "        clean_ppt_title = normalize_title_robust(raw_ppt_title)\n",
    "        \n",
    "        # 1. å°è¯•åŒ¹é…\n",
    "        mask = (df['Match_Title'] == clean_ppt_title) & (df['Type'] == target_type)\n",
    "        target_rows = df[mask]\n",
    "        \n",
    "        if target_rows.empty:\n",
    "            # è°ƒè¯•ï¼šå¦‚æœæ˜¯é‚£ä¸ªå‡ºé—®é¢˜çš„æ”¿ç­–ï¼Œæ‰“å°ä¸€ä¸‹çœ‹ä¸ºä»€ä¹ˆæ²¡åŒ¹é…ä¸Š\n",
    "            if \"market\" in clean_ppt_title and \"instrument\" in clean_ppt_title:\n",
    "                print(f\"âš ï¸ [æœªåŒ¹é…] PPTæ ‡é¢˜: '{raw_ppt_title}' (æ¸…æ´—å: '{clean_ppt_title}')\")\n",
    "            continue\n",
    "            \n",
    "        # 2. è·å–ç¨³å®šçš„ ID é¡ºåº\n",
    "        sorted_cids = get_sorted_cluster_ids(target_rows)\n",
    "        \n",
    "        # 3. è·å– PPT æ ‡ç­¾æ¡† (Left -> Right)\n",
    "        label_shapes = [s for s in slide.shapes if s.has_text_frame and \"+\" in s.text_frame.text]\n",
    "        label_shapes.sort(key=lambda s: (int(s.top / Inches(1)), s.left))\n",
    "        \n",
    "        if len(label_shapes) != len(sorted_cids): \n",
    "            # æ¡†æ•°é‡å¯¹ä¸ä¸Š\n",
    "            if \"market\" in clean_ppt_title:\n",
    "                 print(f\"âš ï¸ [æ•°é‡ä¸ç¬¦] {raw_ppt_title}: PPTæœ‰{len(label_shapes)}ä¸ªæ¡†, CSVæœ‰{len(sorted_cids)}ä¸ªèšç±»\")\n",
    "            continue\n",
    "            \n",
    "        # 4. é€ä¸€æ›´æ–°\n",
    "        for shape, cid in zip(label_shapes, sorted_cids):\n",
    "            raw_text = normalize_spaces(shape.text_frame.text)\n",
    "            parts = raw_text.split('+')\n",
    "            \n",
    "            if len(parts) >= 3:\n",
    "                p1 = parts[0].strip(); p2 = parts[1].strip(); p3_full = parts[2].strip()\n",
    "                \n",
    "                final_e = p3_full\n",
    "                last_paren = p3_full.rfind('(')\n",
    "                if last_paren != -1:\n",
    "                    if \"countr\" in p3_full[last_paren:].lower():\n",
    "                        final_e = p3_full[:last_paren].strip()\n",
    "                \n",
    "                new_s = p1.replace(' Share', '')\n",
    "                new_t = p2\n",
    "                new_e = final_e.replace(' Share', '')\n",
    "                \n",
    "                # 5. æ‰¹é‡é”å®šå¹¶æ›´æ–°\n",
    "                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”¨æ ‡å‡†åŒ–åçš„ Match_Title æ¥å®šä½è¡Œ\n",
    "                target_idx = df[(df['Match_Title'] == clean_ppt_title) & \n",
    "                                (df['Type'] == target_type) & \n",
    "                                (df['èšç±»ID'] == cid)].index\n",
    "                \n",
    "                if not target_idx.empty:\n",
    "                    # æ£€æŸ¥å¹¶æ›´æ–°\n",
    "                    old_vals = df.loc[target_idx[0], ['Starting', 'Trend', 'Ending']].values\n",
    "                    new_vals = [new_s, new_t, new_e]\n",
    "                    \n",
    "                    # åªè¦ä»»ä½•ä¸€ä¸ªå­—æ®µå˜äº†ï¼Œå°±æ›´æ–°\n",
    "                    if list(old_vals) != new_vals:\n",
    "                        print(f\"  ğŸ”„ [æ›´æ–°] {raw_ppt_title[:20]}... (ID:{cid})\")\n",
    "                        print(f\"     æ—§: {old_vals}\")\n",
    "                        print(f\"     æ–°: {new_vals}\")\n",
    "                        \n",
    "                        df.loc[target_idx, 'Starting'] = new_s\n",
    "                        df.loc[target_idx, 'Trend'] = new_t\n",
    "                        df.loc[target_idx, 'Ending'] = new_e\n",
    "                        update_count += len(target_idx)\n",
    "\n",
    "    # æ¸…ç†è¾…åŠ©åˆ—\n",
    "    if 'Match_Title' in df.columns: del df['Match_Title']\n",
    "    \n",
    "    if update_count > 0:\n",
    "        try:\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"âœ… ä¿å­˜æˆåŠŸï¼å…±æ›´æ–°äº† {update_count} è¡Œæ•°æ®ã€‚\")\n",
    "        except PermissionError:\n",
    "            print(\"âŒ ä¿å­˜å¤±è´¥ï¼è¯·å…³é—­ Excel æ–‡ä»¶ï¼\")\n",
    "    else:\n",
    "        print(\"âœ¨ æ²¡æœ‰æ£€æµ‹åˆ°éœ€è¦æ›´æ–°çš„å†…å®¹ã€‚\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. æ‰§è¡Œ\n",
    "# ==========================================\n",
    "def main():\n",
    "    base_dir = Path.cwd().parent\n",
    "    data_dir = base_dir / \"data\" if (base_dir / \"data\").exists() else Path.cwd() / \"data\"\n",
    "    \n",
    "    ppt_dir = data_dir / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"Human_catorgy\"\n",
    "    csv_file = data_dir / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "    \n",
    "    # 1. Breadth\n",
    "    sync_ppt_robust(ppt_dir / \"Analysis_Report_Breadth_Editable.pptx\", csv_file, \"Breadth\")\n",
    "    \n",
    "    # 2. Intensity\n",
    "    sync_ppt_robust(ppt_dir / \"Analysis_Report_Intensity_Editable.pptx\", csv_file, \"Intensity\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb030c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸš€ å¼€å§‹å¤„ç†: Analysis_Report_Breadth_Editable.pptx\n",
      "ğŸ”’ æ¨¡å¼: ç»å¯¹ç¨³å®šæ’åº (é˜²æ­¢æ­»å¾ªç¯)\n",
      "==================================================\n",
      "âœ¨ æ£€æŸ¥äº† 15 é¡µï¼Œæ•°æ®å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°ï¼\n",
      "\n",
      "==================================================\n",
      "ğŸš€ å¼€å§‹å¤„ç†: Analysis_Report_Intensity_Editable.pptx\n",
      "ğŸ”’ æ¨¡å¼: ç»å¯¹ç¨³å®šæ’åº (é˜²æ­¢æ­»å¾ªç¯)\n",
      "==================================================\n",
      "âœ¨ æ£€æŸ¥äº† 15 é¡µï¼Œæ•°æ®å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°ï¼\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "from pathlib import Path\n",
    "from pptx.util import Inches\n",
    "from typing import List, Optional\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. å¼ºåŠ›æ¸…æ´—å·¥å…·\n",
    "# ==========================================\n",
    "def deep_clean(text):\n",
    "    \"\"\"\n",
    "    æ ¸å¼¹çº§æ¸…æ´—ï¼šå»é™¤æ‰€æœ‰çœ‹ä¸è§çš„å­—ç¬¦ï¼Œç»Ÿä¸€è½¬æˆæ ‡å‡†æ ¼å¼ã€‚\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # æ›¿æ¢ä¸æ¢è¡Œç©ºæ ¼(\\xa0), åˆ¶è¡¨ç¬¦, æ¢è¡Œç¬¦\n",
    "    text = text.replace('\\xa0', ' ').replace('\\t', ' ').replace('\\n', ' ')\n",
    "    # æ›¿æ¢å„ç§å¥‡æ€ªçš„æ¨ªæ \n",
    "    text = text.replace('\\u2013', '-').replace('\\u2014', '-').replace('â€“', '-')\n",
    "    # å»é™¤ ' Share' åç¼€ (ä¸åˆ†å¤§å°å†™)\n",
    "    text = re.sub(r'(?i)\\s*share$', '', text)\n",
    "    # å‹ç¼©å¤šä½™ç©ºæ ¼å¹¶å»é™¤é¦–å°¾ç©ºæ ¼\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def normalize_title_robust(text: str) -> str:\n",
    "    return deep_clean(text).lower()\n",
    "\n",
    "def get_slide_title(slide) -> Optional[str]:\n",
    "    if slide.shapes.title and slide.shapes.title.text.strip():\n",
    "        return slide.shapes.title.text.strip()\n",
    "    candidates = []\n",
    "    for shape in slide.shapes:\n",
    "        if shape.has_text_frame and shape.text_frame.text.strip():\n",
    "            candidates.append((shape.top, shape.text_frame.text.strip()))\n",
    "    if candidates:\n",
    "        candidates.sort(key=lambda x: x[0])\n",
    "        if candidates[0][0] < Inches(2.0):\n",
    "            return candidates[0][1]\n",
    "    return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. ç»å¯¹ç¨³å®šçš„æ’åºé€»è¾‘ (æ ¸å¿ƒä¿®æ”¹)\n",
    "# ==========================================\n",
    "def get_strictly_stable_ids(df_sub: pd.DataFrame, metric_type: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    ã€ç»å¯¹ç¨³å®šæ’åºã€‘\n",
    "    ä¸å†è¯»å– 'Ending' æˆ– 'Trend' æ–‡æœ¬è¿›è¡Œæ’åºï¼Œé˜²æ­¢ä¿®æ”¹æ–‡æœ¬å¯¼è‡´é¡ºåºè·³å˜çš„æ­»å¾ªç¯ã€‚\n",
    "    \n",
    "    ç­–ç•¥ï¼š\n",
    "    1. å…ˆæŒ‰ Calculated Starting (ç”± MeanStart ç®—å‡ºæ¥çš„ç±»åˆ«) åˆ†å¤§ç»„ã€‚\n",
    "    2. ç»„å†…åªæŒ‰ MeanStart æ•°å€¼æ’åºã€‚\n",
    "    è¿™æ ·æ— è®ºä½ æ€ä¹ˆæ”¹æ–‡å­—ï¼Œæ’åºé¡ºåºæ°¸è¿œå›ºå®šã€‚\n",
    "    \"\"\"\n",
    "    df = df_sub.copy()\n",
    "    \n",
    "    # é˜ˆå€¼ (å‚è€ƒè‡ªç»˜å›¾ä»£ç )\n",
    "    if metric_type == 'Intensity':\n",
    "        th_high, th_med = 6.0, 3.0\n",
    "    else: # Breadth\n",
    "        th_high, th_med = 0.6, 0.3\n",
    "\n",
    "    # åŠ¨æ€è®¡ç®—èµ·å§‹ç­‰çº§ (0=Low, 1=Med, 2=High)\n",
    "    def get_start_rank(val):\n",
    "        if val > th_high: return 2\n",
    "        if val > th_med: return 1\n",
    "        return 0\n",
    "\n",
    "    df['calc_start_rank'] = df['MeanStart'].apply(get_start_rank)\n",
    "    \n",
    "    # æ’åºé”®ï¼š(èµ·å§‹ç­‰çº§, èµ·å§‹æ•°å€¼, ID)\n",
    "    # æ³¨æ„ï¼šè¿™é‡Œå®Œå…¨æŠ›å¼ƒäº† Ending å’Œ Trendï¼Œç¡®ä¿â€œé”šç‚¹â€ä¸åŠ¨\n",
    "    df['sort_key'] = df.apply(lambda r: (\n",
    "        r['calc_start_rank'], \n",
    "        r.get('MeanStart', 0),\n",
    "        r.get('èšç±»ID', 0)\n",
    "    ), axis=1)\n",
    "    \n",
    "    return df.sort_values('sort_key')['èšç±»ID'].unique().tolist()\n",
    "\n",
    "# ==========================================\n",
    "# 3. åŒæ­¥ä¸»ç¨‹åº (å¸¦è¯¦ç»†è°ƒè¯•)\n",
    "# ==========================================\n",
    "def sync_ppt_final(ppt_path: Path, csv_path: Path, target_type: str) -> None:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸš€ å¼€å§‹å¤„ç†: {ppt_path.name}\")\n",
    "    print(f\"ğŸ”’ æ¨¡å¼: ç»å¯¹ç¨³å®šæ’åº (é˜²æ­¢æ­»å¾ªç¯)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if not ppt_path.exists():\n",
    "        print(\"âŒ PPT æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "        return\n",
    "\n",
    "    prs = Presentation(ppt_path)\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "    \n",
    "    # é¢„å¤„ç† CSV æ ‡é¢˜åˆ—\n",
    "    df['Match_Title'] = df['L2æ”¿ç­–ä¸­æ–‡å'].apply(normalize_title_robust)\n",
    "    \n",
    "    update_count = 0\n",
    "    total_slides_checked = 0\n",
    "    \n",
    "    for slide_idx, slide in enumerate(prs.slides):\n",
    "        raw_ppt_title = get_slide_title(slide)\n",
    "        if not raw_ppt_title: continue\n",
    "        \n",
    "        clean_ppt_title = normalize_title_robust(raw_ppt_title)\n",
    "        \n",
    "        # 1. åŒ¹é…æ•°æ®\n",
    "        mask = (df['Match_Title'] == clean_ppt_title) & (df['Type'] == target_type)\n",
    "        target_rows = df[mask]\n",
    "        \n",
    "        if target_rows.empty: continue\n",
    "        total_slides_checked += 1\n",
    "            \n",
    "        # 2. è·å–ç¨³å®š ID åˆ—è¡¨\n",
    "        sorted_cids = get_strictly_stable_ids(target_rows, target_type)\n",
    "        \n",
    "        # 3. è·å– PPT æ–‡æœ¬æ¡† (Top -> Left)\n",
    "        label_shapes = [s for s in slide.shapes if s.has_text_frame and \"+\" in s.text_frame.text]\n",
    "        label_shapes.sort(key=lambda s: (int(s.top / Inches(1)), s.left))\n",
    "        \n",
    "        # 4. æ•°é‡æ£€æŸ¥\n",
    "        if len(label_shapes) != len(sorted_cids):\n",
    "            if \"market\" in clean_ppt_title:\n",
    "                print(f\"âš ï¸ [è·³è¿‡] {raw_ppt_title}: PPTæ¡†æ•°é‡({len(label_shapes)}) != CSVè¡Œæ•°({len(sorted_cids)})\")\n",
    "            continue\n",
    "            \n",
    "        # 5. é€ä¸€æ¯”å¯¹ä¸æ›´æ–°\n",
    "        for shape, cid in zip(label_shapes, sorted_cids):\n",
    "            # --- è§£æ PPT ---\n",
    "            raw_text = shape.text_frame.text\n",
    "            clean_text = deep_clean(raw_text) # \"High+Rise+Low (5 countries)\"\n",
    "            \n",
    "            parts = clean_text.split('+')\n",
    "            if len(parts) < 3: continue\n",
    "            \n",
    "            p1 = parts[0].strip() # Starting\n",
    "            p2 = parts[1].strip() # Trend\n",
    "            p3_full = parts[2].strip() # Ending + (Count)\n",
    "            \n",
    "            # æå– Ending (å»æ‰æ‹¬å·éƒ¨åˆ†)\n",
    "            final_e = p3_full\n",
    "            if '(' in p3_full:\n",
    "                 final_e = p3_full.split('(')[0].strip()\n",
    "            \n",
    "            # --- è§£æ CSV ---\n",
    "            target_idx = df[(df['Match_Title'] == clean_ppt_title) & \n",
    "                            (df['Type'] == target_type) & \n",
    "                            (df['èšç±»ID'] == cid)].index\n",
    "            \n",
    "            if target_idx.empty: continue\n",
    "            \n",
    "            # è·å–å½“å‰ CSV é‡Œçš„å€¼ (å¹¶è¿›è¡ŒåŒæ ·çš„æ·±å±‚æ¸…æ´—)\n",
    "            curr_s = deep_clean(df.loc[target_idx[0], 'Starting'])\n",
    "            curr_t = deep_clean(df.loc[target_idx[0], 'Trend'])\n",
    "            curr_e = deep_clean(df.loc[target_idx[0], 'Ending'])\n",
    "            \n",
    "            # --- æ ¸å¿ƒå¯¹æ¯” ---\n",
    "            # åªæœ‰å½“æ¸…æ´—åçš„å†…å®¹çœŸçš„ä¸ä¸€æ ·æ—¶ï¼Œæ‰è§¦å‘æ›´æ–°\n",
    "            has_change = False\n",
    "            if curr_s != p1: has_change = True\n",
    "            if curr_t != p2: has_change = True\n",
    "            if curr_e != final_e: has_change = True\n",
    "            \n",
    "            if has_change:\n",
    "                print(f\" ğŸ”„ [å‘ç°å·®å¼‚] {raw_ppt_title[:15]}... (ID:{cid})\")\n",
    "                print(f\"    PPT: {p1} + {p2} + {final_e}\")\n",
    "                print(f\"    CSV: {curr_s} + {curr_t} + {curr_e}\")\n",
    "                \n",
    "                df.loc[target_idx, 'Starting'] = p1\n",
    "                df.loc[target_idx, 'Trend'] = p2\n",
    "                df.loc[target_idx, 'Ending'] = final_e\n",
    "                update_count += 1\n",
    "\n",
    "    # æ¸…ç†ä¸´æ—¶åˆ—\n",
    "    if 'Match_Title' in df.columns: del df['Match_Title']\n",
    "    if 'calc_start_rank' in df.columns: del df['calc_start_rank']\n",
    "    if 'sort_key' in df.columns: del df['sort_key']\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    if update_count > 0:\n",
    "        try:\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"âœ… ä¿å­˜æˆåŠŸï¼æ›´æ–°äº† {update_count} å¤„ã€‚\")\n",
    "        except PermissionError:\n",
    "            print(\"âŒ ä¿å­˜å¤±è´¥ï¼è¯·å…³é—­ Excel æ–‡ä»¶ï¼\")\n",
    "    else:\n",
    "        print(f\"âœ¨ æ£€æŸ¥äº† {total_slides_checked} é¡µï¼Œæ•°æ®å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°ï¼\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. æ‰§è¡Œ\n",
    "# ==========================================\n",
    "def main():\n",
    "    base_dir = Path.cwd().parent\n",
    "    data_dir = base_dir / \"data\" if (base_dir / \"data\").exists() else Path.cwd() / \"data\"\n",
    "    \n",
    "    ppt_dir = data_dir / \"3-1-(3-2)Sorted_L2_Policy_Clustering_pic\" / \"Human_catorgy\"\n",
    "    csv_file = data_dir / \"3-2-Automated_Recognition_Mode.csv\"\n",
    "    \n",
    "    sync_ppt_final(ppt_dir / \"Analysis_Report_Breadth_Editable.pptx\", csv_file, \"Breadth\")\n",
    "    sync_ppt_final(ppt_dir / \"Analysis_Report_Intensity_Editable.pptx\", csv_file, \"Intensity\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_policy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
