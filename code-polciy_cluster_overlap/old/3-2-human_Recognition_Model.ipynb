{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68e2efb",
   "metadata": {},
   "source": [
    "### 人工识别结果输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3299f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: F:\\Desktop\\CAMPF_Supplementary\\data_origin\\3-2-XJC_YCB_Thinking_TO_json.csv\n",
      "Output file: F:\\Desktop\\CAMPF_Supplementary\\data\\3-2-Human_Recognition_Mode_txt.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "data_dir = current_dir\n",
    "input_file = data_dir.parent / \"data_origin/3-2-XJC_YCB_Thinking_TO_json.csv\"\n",
    "output_file = data_dir.parent / \"data/3-2-Human_Recognition_Mode_txt.json\"\n",
    "\n",
    "# 读入 CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "result = {}\n",
    "\n",
    "# 遍历行\n",
    "for _, row in df.iterrows():\n",
    "    L1_name = row.get('L1_name_EN', '')\n",
    "    L2_name = row.get('L2_name_EN', '')\n",
    "\n",
    "    if not L1_name or not L2_name:\n",
    "        continue\n",
    "\n",
    "    if L1_name not in result:\n",
    "        result[L1_name] = {}\n",
    "    result[L1_name][L2_name] = {}\n",
    "\n",
    "    # 遍历列 0,1,2\n",
    "    for col in ['0', '1', '2']:\n",
    "        val = row.get(col, '')\n",
    "        if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "            val = ''\n",
    "        else:\n",
    "            val = str(val)\n",
    "\n",
    "        parts = val.split('+')\n",
    "        if len(parts) == 3:\n",
    "            result[L1_name][L2_name][col] = {\n",
    "                \"Starting\": parts[0],\n",
    "                \"Trend\": parts[1],\n",
    "                \"Ending\": parts[2]\n",
    "            }\n",
    "        else:\n",
    "            result[L1_name][L2_name][col] = {\n",
    "                \"Starting\": \"\",\n",
    "                \"Trend\": \"\",\n",
    "                \"Ending\": \"\"\n",
    "            }\n",
    "\n",
    "# 确保输出目录存在\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 先序列化为字符串，再做全局替换，把 '_' 换成 ','\n",
    "json_text = json.dumps(result, ensure_ascii=False, indent=2)\n",
    "json_text = json_text.replace('_', ',')\n",
    "\n",
    "# 写入文件\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(json_text)\n",
    "\n",
    "print(f\"Input file: {input_file.resolve()}\")\n",
    "print(f\"Output file: {output_file.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696aa2c",
   "metadata": {},
   "source": [
    "### JSON附加至国家级政策聚类数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e183841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成：共 735 行，匹配成功 735 行\n",
      "输出文件：F:\\Desktop\\CAMPF_Supplementary\\data\\3-2-Human_Recognition_Mode_Countries.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "# 可能存在的 L1 相关列（如果存在，就用 JSON 的 L1 覆盖；否则新增 L1_name_EN）\n",
    "L1_COLUMNS_CANDIDATES = [\"L1_name_EN\", \"L1_name_CN\", \"L1\", \"L1分类\", \"L1分类中文名\"]\n",
    "\n",
    "def build_l2_lookup_from_json(recognition_data: dict):\n",
    "    \"\"\"\n",
    "    将 JSON 构造成：{ L2政策名(与CSV一致): {\"L1\": 顶层统领L1, \"clusters\": {\"0\":{}, \"1\":{}, \"2\":{}} } }\n",
    "    假设 JSON 结构为：顶层 L1 -> L2 -> 0/1/2（与你上传的文件一致）。\n",
    "    \"\"\"\n",
    "    lookup = {}\n",
    "    for l1, by_l2 in recognition_data.items():\n",
    "        if not isinstance(by_l2, dict):\n",
    "            continue\n",
    "        for l2_name, clusters in by_l2.items():\n",
    "            if isinstance(clusters, dict):\n",
    "                # 只保留 0/1/2 三个簇（若存在）\n",
    "                cdict = {k: clusters.get(k, {}) for k in (\"0\", \"1\", \"2\") if k in clusters}\n",
    "                if cdict:\n",
    "                    lookup[str(l2_name).strip()] = {\"L1\": str(l1).strip(), \"clusters\": cdict}\n",
    "    return lookup\n",
    "\n",
    "def process_policy_data(json_path, input_csv_path, output_csv_path):\n",
    "    # 读取 JSON（顶层为 L1，第二层为 L2；与你上传的文件一致）\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        recognition_data = json.load(f)\n",
    "\n",
    "    # 构建 L2 -> {L1, clusters} 查找表\n",
    "    l2_lookup = build_l2_lookup_from_json(recognition_data)\n",
    "\n",
    "    # 读取 CSV（不提取/依赖任何 L1 列）\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # 去重：按 L2政策中文名 + 国家（若存在这两列）\n",
    "    subset_cols = [c for c in [\"L2政策中文名\", \"国家\"] if c in df.columns]\n",
    "    if subset_cols:\n",
    "        df = df.drop_duplicates(subset=subset_cols, keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    # 确保结果字段存在\n",
    "    for c in [\"Starting\", \"Trend\", \"Ending\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"\n",
    "    # 若没有任何 L1 列，则新增 L1_name_EN\n",
    "    if not any(col in df.columns for col in L1_COLUMNS_CANDIDATES):\n",
    "        df[\"L1_name_EN\"] = \"\"\n",
    "\n",
    "    matched = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        l2_cn = str(row.get(\"L2政策中文名\", \"\")).strip()\n",
    "        if not l2_cn:\n",
    "            continue\n",
    "\n",
    "        # 用 L2 中文名去 JSON 查 L1 与 clusters\n",
    "        entry = l2_lookup.get(l2_cn)\n",
    "        if not entry:\n",
    "            continue\n",
    "\n",
    "        # 覆盖/写入 L1（来自 JSON 统领）\n",
    "        l1_from_json = entry.get(\"L1\", \"\")\n",
    "        if l1_from_json:\n",
    "            written = False\n",
    "            for col in L1_COLUMNS_CANDIDATES:\n",
    "                if col in df.columns:\n",
    "                    df.at[idx, col] = l1_from_json\n",
    "                    written = True\n",
    "            if not written:  # 所有候选列都不存在则写到 L1_name_EN\n",
    "                df.at[idx, \"L1_name_EN\"] = l1_from_json\n",
    "\n",
    "        # 仍按 CSV 的 聚类ID 取 Starting/Trend/Ending\n",
    "        cluster_raw = row.get(\"聚类ID\", \"\")\n",
    "        if isinstance(cluster_raw, float) and math.isnan(cluster_raw):\n",
    "            continue\n",
    "        s = str(cluster_raw).strip()\n",
    "        cluster_id = str(int(s)) if s.isdigit() else s\n",
    "\n",
    "        if cluster_id in entry[\"clusters\"]:\n",
    "            info = entry[\"clusters\"][cluster_id] or {}\n",
    "            df.at[idx, \"Starting\"] = str(info.get(\"Starting\", \"\"))\n",
    "            df.at[idx, \"Trend\"] = str(info.get(\"Trend\", \"\"))\n",
    "            df.at[idx, \"Ending\"] = str(info.get(\"Ending\", \"\"))\n",
    "            matched += 1\n",
    "\n",
    "    # 保存（其它列保持不变）\n",
    "    Path(output_csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"处理完成：共 {len(df)} 行，匹配成功 {matched} 行\")\n",
    "    print(f\"输出文件：{Path(output_csv_path).resolve()}\")\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "json_file = current_dir.parent / \"data/3-2-Human_Recognition_Mode_txt.json\"\n",
    "input_csv = current_dir.parent / \"data/3-1-L2_Policy_Clustering_countries.csv\"\n",
    "output_csv = current_dir.parent / \"data/3-2-Human_Recognition_Mode_Countries.csv\"\n",
    "\n",
    "# 执行\n",
    "process_policy_data(json_file, input_csv, output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
