{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1850a9e",
   "metadata": {},
   "source": [
    "#### API爬取文献信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739068ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path: f:\\Desktop\\科研项目\\1.负责科研项目\\Climate Policy\\CAMPF_Supplementary_V2\\data_origin\\OpenAlex_paper\n",
      "\n",
      "[Start] Climate_Change_Policy_and_Economics (T10471)\n",
      "Total Estimation: 211646\n",
      "   ...Fetched 1000 / 211646 (0.5%)\n",
      "   ...Fetched 2000 / 211646 (0.9%)\n",
      "   ...Fetched 3000 / 211646 (1.4%)\n",
      "   ...Fetched 4000 / 211646 (1.9%)\n",
      "   ...Fetched 5000 / 211646 (2.4%)\n",
      "   ...Fetched 6000 / 211646 (2.8%)\n",
      "   ...Fetched 7000 / 211646 (3.3%)\n",
      "   ...Fetched 8000 / 211646 (3.8%)\n",
      "   ...Fetched 9000 / 211646 (4.3%)\n",
      "   ...Fetched 10000 / 211646 (4.7%)\n",
      "   ...Fetched 11000 / 211646 (5.2%)\n",
      "   ...Fetched 12000 / 211646 (5.7%)\n",
      "   ...Fetched 13000 / 211646 (6.1%)\n",
      "   ...Fetched 14000 / 211646 (6.6%)\n",
      "   ...Fetched 15000 / 211646 (7.1%)\n",
      "   ...Fetched 16000 / 211646 (7.6%)\n",
      "   ...Fetched 17000 / 211646 (8.0%)\n",
      "   ...Fetched 18000 / 211646 (8.5%)\n",
      "   ...Fetched 19000 / 211646 (9.0%)\n",
      "   ...Fetched 20000 / 211646 (9.4%)\n",
      "   ...Fetched 21000 / 211646 (9.9%)\n",
      "   ...Fetched 22000 / 211646 (10.4%)\n",
      "   ...Fetched 23000 / 211646 (10.9%)\n",
      "   ...Fetched 24000 / 211646 (11.3%)\n",
      "   ...Fetched 25000 / 211646 (11.8%)\n",
      "   ...Fetched 26000 / 211646 (12.3%)\n",
      "   ...Fetched 27000 / 211646 (12.8%)\n",
      "   ...Fetched 28000 / 211646 (13.2%)\n",
      "   ...Fetched 29000 / 211646 (13.7%)\n",
      "   ...Fetched 30000 / 211646 (14.2%)\n",
      "   ...Fetched 31000 / 211646 (14.6%)\n",
      "   ...Fetched 32000 / 211646 (15.1%)\n",
      "   ...Fetched 33000 / 211646 (15.6%)\n",
      "   ...Fetched 34000 / 211646 (16.1%)\n",
      "   ...Fetched 35000 / 211646 (16.5%)\n",
      "   ...Fetched 36000 / 211646 (17.0%)\n",
      "   ...Fetched 37000 / 211646 (17.5%)\n",
      "   ...Fetched 38000 / 211646 (18.0%)\n",
      "   ...Fetched 39000 / 211646 (18.4%)\n",
      "   ...Fetched 40000 / 211646 (18.9%)\n",
      "   ...Fetched 41000 / 211646 (19.4%)\n",
      "   ...Fetched 42000 / 211646 (19.8%)\n",
      "   ...Fetched 43000 / 211646 (20.3%)\n",
      "   ...Fetched 44000 / 211646 (20.8%)\n",
      "   ...Fetched 45000 / 211646 (21.3%)\n",
      "   ...Fetched 46000 / 211646 (21.7%)\n",
      "   ...Fetched 47000 / 211646 (22.2%)\n",
      "   ...Fetched 48000 / 211646 (22.7%)\n",
      "   ...Fetched 49000 / 211646 (23.2%)\n",
      "   ...Fetched 50000 / 211646 (23.6%)\n",
      "   ...Fetched 51000 / 211646 (24.1%)\n",
      "   ...Fetched 52000 / 211646 (24.6%)\n",
      "   ...Fetched 53000 / 211646 (25.0%)\n",
      "   ...Fetched 54000 / 211646 (25.5%)\n",
      "   ...Fetched 55000 / 211646 (26.0%)\n",
      "   ...Fetched 56000 / 211646 (26.5%)\n",
      "   ...Fetched 57000 / 211646 (26.9%)\n",
      "   ...Fetched 58000 / 211646 (27.4%)\n",
      "   ...Fetched 59000 / 211646 (27.9%)\n",
      "   ...Fetched 60000 / 211646 (28.3%)\n",
      "   ...Fetched 61000 / 211646 (28.8%)\n",
      "   ...Fetched 62000 / 211646 (29.3%)\n",
      "   ...Fetched 63000 / 211646 (29.8%)\n",
      "   ...Fetched 64000 / 211646 (30.2%)\n",
      "   ...Fetched 65000 / 211646 (30.7%)\n",
      "   ...Fetched 66000 / 211646 (31.2%)\n",
      "   ...Fetched 67000 / 211646 (31.7%)\n",
      "   ...Fetched 68000 / 211646 (32.1%)\n",
      "   ...Fetched 69000 / 211646 (32.6%)\n",
      "   ...Fetched 70000 / 211646 (33.1%)\n",
      "   ...Fetched 71000 / 211646 (33.5%)\n",
      "   ...Fetched 72000 / 211646 (34.0%)\n",
      "   ...Fetched 73000 / 211646 (34.5%)\n",
      "   ...Fetched 74000 / 211646 (35.0%)\n",
      "   ...Fetched 75000 / 211646 (35.4%)\n",
      "   ...Fetched 76000 / 211646 (35.9%)\n",
      "   ...Fetched 77000 / 211646 (36.4%)\n",
      "   ...Fetched 78000 / 211646 (36.9%)\n",
      "   ...Fetched 79000 / 211646 (37.3%)\n",
      "   ...Fetched 80000 / 211646 (37.8%)\n",
      "   ...Fetched 81000 / 211646 (38.3%)\n",
      "   ...Fetched 82000 / 211646 (38.7%)\n",
      "   ...Fetched 83000 / 211646 (39.2%)\n",
      "   ...Fetched 84000 / 211646 (39.7%)\n",
      "   ...Fetched 85000 / 211646 (40.2%)\n",
      "   ...Fetched 86000 / 211646 (40.6%)\n",
      "   ...Fetched 87000 / 211646 (41.1%)\n",
      "   ...Fetched 88000 / 211646 (41.6%)\n",
      "   ...Fetched 89000 / 211646 (42.1%)\n",
      "   ...Fetched 90000 / 211646 (42.5%)\n",
      "   ...Fetched 91000 / 211646 (43.0%)\n",
      "   ...Fetched 92000 / 211646 (43.5%)\n",
      "   ...Fetched 93000 / 211646 (43.9%)\n",
      "   ...Fetched 94000 / 211646 (44.4%)\n",
      "   ...Fetched 95000 / 211646 (44.9%)\n",
      "   ...Fetched 96000 / 211646 (45.4%)\n",
      "   ...Fetched 97000 / 211646 (45.8%)\n",
      "   ...Fetched 98000 / 211646 (46.3%)\n",
      "   ...Fetched 99000 / 211646 (46.8%)\n",
      "   ...Fetched 100000 / 211646 (47.2%)\n",
      "   ...Fetched 101000 / 211646 (47.7%)\n",
      "   ...Fetched 102000 / 211646 (48.2%)\n",
      "   ...Fetched 103000 / 211646 (48.7%)\n",
      "   ...Fetched 104000 / 211646 (49.1%)\n",
      "   ...Fetched 105000 / 211646 (49.6%)\n",
      "   ...Fetched 106000 / 211646 (50.1%)\n",
      "   ...Fetched 107000 / 211646 (50.6%)\n",
      "   ...Fetched 108000 / 211646 (51.0%)\n",
      "   ...Fetched 109000 / 211646 (51.5%)\n",
      "   ...Fetched 110000 / 211646 (52.0%)\n",
      "   ...Fetched 111000 / 211646 (52.4%)\n",
      "   ...Fetched 112000 / 211646 (52.9%)\n",
      "   ...Fetched 113000 / 211646 (53.4%)\n",
      "   ...Fetched 114000 / 211646 (53.9%)\n",
      "   ...Fetched 115000 / 211646 (54.3%)\n",
      "   ...Fetched 116000 / 211646 (54.8%)\n",
      "   ...Fetched 117000 / 211646 (55.3%)\n",
      "   ...Fetched 118000 / 211646 (55.8%)\n",
      "   ...Fetched 119000 / 211646 (56.2%)\n",
      "   ...Fetched 120000 / 211646 (56.7%)\n",
      "   ...Fetched 121000 / 211646 (57.2%)\n",
      "   ...Fetched 122000 / 211646 (57.6%)\n",
      "   ...Fetched 123000 / 211646 (58.1%)\n",
      "   ...Fetched 124000 / 211646 (58.6%)\n",
      "   ...Fetched 125000 / 211646 (59.1%)\n",
      "   ...Fetched 126000 / 211646 (59.5%)\n",
      "   ...Fetched 127000 / 211646 (60.0%)\n",
      "   ...Fetched 128000 / 211646 (60.5%)\n",
      "   ...Fetched 129000 / 211646 (61.0%)\n",
      "   ...Fetched 130000 / 211646 (61.4%)\n",
      "   ...Fetched 131000 / 211646 (61.9%)\n",
      "   ...Fetched 132000 / 211646 (62.4%)\n",
      "   ...Fetched 133000 / 211646 (62.8%)\n",
      "   ...Fetched 134000 / 211646 (63.3%)\n",
      "   ...Fetched 135000 / 211646 (63.8%)\n",
      "   ...Fetched 136000 / 211646 (64.3%)\n",
      "   ...Fetched 137000 / 211646 (64.7%)\n",
      "   ...Fetched 138000 / 211646 (65.2%)\n",
      "   ...Fetched 139000 / 211646 (65.7%)\n",
      "   ...Fetched 140000 / 211646 (66.1%)\n",
      "   ...Fetched 141000 / 211646 (66.6%)\n",
      "   ...Fetched 142000 / 211646 (67.1%)\n",
      "   ...Fetched 143000 / 211646 (67.6%)\n",
      "   ...Fetched 144000 / 211646 (68.0%)\n",
      "   ...Fetched 145000 / 211646 (68.5%)\n",
      "   ...Fetched 146000 / 211646 (69.0%)\n",
      "   ...Fetched 147000 / 211646 (69.5%)\n",
      "   ...Fetched 148000 / 211646 (69.9%)\n",
      "   ...Fetched 149000 / 211646 (70.4%)\n",
      "   ...Fetched 150000 / 211646 (70.9%)\n",
      "   ...Fetched 151000 / 211646 (71.3%)\n",
      "   ...Fetched 152000 / 211646 (71.8%)\n",
      "   ...Fetched 153000 / 211646 (72.3%)\n",
      "   ...Fetched 154000 / 211646 (72.8%)\n",
      "   ...Fetched 155000 / 211646 (73.2%)\n",
      "   ...Fetched 156000 / 211646 (73.7%)\n",
      "   ...Fetched 157000 / 211646 (74.2%)\n",
      "   ...Fetched 158000 / 211646 (74.7%)\n",
      "   ...Fetched 159000 / 211646 (75.1%)\n",
      "   ...Fetched 160000 / 211646 (75.6%)\n",
      "   ...Fetched 161000 / 211646 (76.1%)\n",
      "   ...Fetched 162000 / 211646 (76.5%)\n",
      "   ...Fetched 163000 / 211646 (77.0%)\n",
      "   ...Fetched 164000 / 211646 (77.5%)\n",
      "   ...Fetched 165000 / 211646 (78.0%)\n",
      "   ...Fetched 166000 / 211646 (78.4%)\n",
      "   ...Fetched 167000 / 211646 (78.9%)\n",
      "   ...Fetched 168000 / 211646 (79.4%)\n",
      "   ...Fetched 169000 / 211646 (79.9%)\n",
      "   ...Fetched 170000 / 211646 (80.3%)\n",
      "   ...Fetched 171000 / 211646 (80.8%)\n",
      "   ...Fetched 172000 / 211646 (81.3%)\n",
      "   ...Fetched 173000 / 211646 (81.7%)\n",
      "   ...Fetched 174000 / 211646 (82.2%)\n",
      "   ...Fetched 175000 / 211646 (82.7%)\n",
      "   ...Fetched 176000 / 211646 (83.2%)\n",
      "   ...Fetched 177000 / 211646 (83.6%)\n",
      "   ...Fetched 178000 / 211646 (84.1%)\n",
      "   ...Fetched 179000 / 211646 (84.6%)\n",
      "   ...Fetched 180000 / 211646 (85.0%)\n",
      "   ...Fetched 181000 / 211646 (85.5%)\n",
      "   ...Fetched 182000 / 211646 (86.0%)\n",
      "   ...Fetched 183000 / 211646 (86.5%)\n",
      "   ...Fetched 184000 / 211646 (86.9%)\n",
      "   ...Fetched 185000 / 211646 (87.4%)\n",
      "   ...Fetched 186000 / 211646 (87.9%)\n",
      "   ...Fetched 187000 / 211646 (88.4%)\n",
      "   ...Fetched 188000 / 211646 (88.8%)\n",
      "   ...Fetched 189000 / 211646 (89.3%)\n",
      "   ...Fetched 190000 / 211646 (89.8%)\n",
      "   ...Fetched 191000 / 211646 (90.2%)\n",
      "   ...Fetched 192000 / 211646 (90.7%)\n",
      "   ...Fetched 193000 / 211646 (91.2%)\n",
      "   ...Fetched 194000 / 211646 (91.7%)\n",
      "   ...Fetched 195000 / 211646 (92.1%)\n",
      "   ...Fetched 196000 / 211646 (92.6%)\n",
      "   ...Fetched 197000 / 211646 (93.1%)\n",
      "   ...Fetched 198000 / 211646 (93.6%)\n",
      "   ...Fetched 199000 / 211646 (94.0%)\n",
      "   ...Fetched 200000 / 211646 (94.5%)\n",
      "   ...Fetched 201000 / 211646 (95.0%)\n",
      "   ...Fetched 202000 / 211646 (95.4%)\n",
      "   ...Fetched 203000 / 211646 (95.9%)\n",
      "   ...Fetched 204000 / 211646 (96.4%)\n",
      "   ...Fetched 205000 / 211646 (96.9%)\n",
      "   ...Fetched 206000 / 211646 (97.3%)\n",
      "   ...Fetched 207000 / 211646 (97.8%)\n",
      "   ...Fetched 208000 / 211646 (98.3%)\n",
      "   ...Fetched 209000 / 211646 (98.7%)\n",
      "   ...Fetched 210000 / 211646 (99.2%)\n",
      "   ...Fetched 211000 / 211646 (99.7%)\n",
      "[Done] Saved 211646 records to ..\\..\\data_origin\\OpenAlex_paper\\Climate_Change_Policy_and_Economics\\Raw_Works.jsonl\n",
      "\n",
      "[Start] Sustainable_Development_and_Env_Policy (T12013)\n",
      "Total Estimation: 110195\n",
      "   ...Fetched 1000 / 110195 (0.9%)\n",
      "   ...Fetched 2000 / 110195 (1.8%)\n",
      "   ...Fetched 3000 / 110195 (2.7%)\n",
      "   ...Fetched 4000 / 110195 (3.6%)\n",
      "   ...Fetched 5000 / 110195 (4.5%)\n",
      "   ...Fetched 6000 / 110195 (5.4%)\n",
      "   ...Fetched 7000 / 110195 (6.4%)\n",
      "   ...Fetched 8000 / 110195 (7.3%)\n",
      "   ...Fetched 9000 / 110195 (8.2%)\n",
      "   ...Fetched 10000 / 110195 (9.1%)\n",
      "   ...Fetched 11000 / 110195 (10.0%)\n",
      "   ...Fetched 12000 / 110195 (10.9%)\n",
      "   ...Fetched 13000 / 110195 (11.8%)\n",
      "   ...Fetched 14000 / 110195 (12.7%)\n",
      "   ...Fetched 15000 / 110195 (13.6%)\n",
      "   ...Fetched 16000 / 110195 (14.5%)\n",
      "   ...Fetched 17000 / 110195 (15.4%)\n",
      "   ...Fetched 18000 / 110195 (16.3%)\n",
      "   ...Fetched 19000 / 110195 (17.2%)\n",
      "   ...Fetched 20000 / 110195 (18.1%)\n",
      "   ...Fetched 21000 / 110195 (19.1%)\n",
      "   ...Fetched 22000 / 110195 (20.0%)\n",
      "   ...Fetched 23000 / 110195 (20.9%)\n",
      "   ...Fetched 24000 / 110195 (21.8%)\n",
      "   ...Fetched 25000 / 110195 (22.7%)\n",
      "   ...Fetched 26000 / 110195 (23.6%)\n",
      "   ...Fetched 27000 / 110195 (24.5%)\n",
      "   ...Fetched 28000 / 110195 (25.4%)\n",
      "   ...Fetched 29000 / 110195 (26.3%)\n",
      "   ...Fetched 30000 / 110195 (27.2%)\n",
      "   ...Fetched 31000 / 110195 (28.1%)\n",
      "   ...Fetched 32000 / 110195 (29.0%)\n",
      "   ...Fetched 33000 / 110195 (29.9%)\n",
      "   ...Fetched 34000 / 110195 (30.9%)\n",
      "   ...Fetched 35000 / 110195 (31.8%)\n",
      "   ...Fetched 36000 / 110195 (32.7%)\n",
      "   ...Fetched 37000 / 110195 (33.6%)\n",
      "   ...Fetched 38000 / 110195 (34.5%)\n",
      "   ...Fetched 39000 / 110195 (35.4%)\n",
      "   ...Fetched 40000 / 110195 (36.3%)\n",
      "   ...Fetched 41000 / 110195 (37.2%)\n",
      "   ...Fetched 42000 / 110195 (38.1%)\n",
      "   ...Fetched 43000 / 110195 (39.0%)\n",
      "   ...Fetched 44000 / 110195 (39.9%)\n",
      "   ...Fetched 45000 / 110195 (40.8%)\n",
      "   ...Fetched 46000 / 110195 (41.7%)\n",
      "   ...Fetched 47000 / 110195 (42.7%)\n",
      "   ...Fetched 48000 / 110195 (43.6%)\n",
      "   ...Fetched 49000 / 110195 (44.5%)\n",
      "   ...Fetched 50000 / 110195 (45.4%)\n",
      "   ...Fetched 51000 / 110195 (46.3%)\n",
      "   ...Fetched 52000 / 110195 (47.2%)\n",
      "   ...Fetched 53000 / 110195 (48.1%)\n",
      "   ...Fetched 54000 / 110195 (49.0%)\n",
      "   ...Fetched 55000 / 110195 (49.9%)\n",
      "   ...Fetched 56000 / 110195 (50.8%)\n",
      "   ...Fetched 57000 / 110195 (51.7%)\n",
      "   ...Fetched 58000 / 110195 (52.6%)\n",
      "   ...Fetched 59000 / 110195 (53.5%)\n",
      "   ...Fetched 60000 / 110195 (54.4%)\n",
      "   ...Fetched 61000 / 110195 (55.4%)\n",
      "   ...Fetched 62000 / 110195 (56.3%)\n",
      "   ...Fetched 63000 / 110195 (57.2%)\n",
      "   ...Fetched 64000 / 110195 (58.1%)\n",
      "   ...Fetched 65000 / 110195 (59.0%)\n",
      "   ...Fetched 66000 / 110195 (59.9%)\n",
      "   ...Fetched 67000 / 110195 (60.8%)\n",
      "   ...Fetched 68000 / 110195 (61.7%)\n",
      "   ...Fetched 69000 / 110195 (62.6%)\n",
      "   ...Fetched 70000 / 110195 (63.5%)\n",
      "   ...Fetched 71000 / 110195 (64.4%)\n",
      "   ...Fetched 72000 / 110195 (65.3%)\n",
      "   ...Fetched 73000 / 110195 (66.2%)\n",
      "   ...Fetched 74000 / 110195 (67.2%)\n",
      "   ...Fetched 75000 / 110195 (68.1%)\n",
      "   ...Fetched 76000 / 110195 (69.0%)\n",
      "   ...Fetched 77000 / 110195 (69.9%)\n",
      "   ...Fetched 78000 / 110195 (70.8%)\n",
      "   ...Fetched 79000 / 110195 (71.7%)\n",
      "   ...Fetched 80000 / 110195 (72.6%)\n",
      "   ...Fetched 81000 / 110195 (73.5%)\n",
      "   ...Fetched 82000 / 110195 (74.4%)\n",
      "   ...Fetched 83000 / 110195 (75.3%)\n",
      "   ...Fetched 84000 / 110195 (76.2%)\n",
      "   ...Fetched 85000 / 110195 (77.1%)\n",
      "   ...Fetched 86000 / 110195 (78.0%)\n",
      "   ...Fetched 87000 / 110195 (79.0%)\n",
      "   ...Fetched 88000 / 110195 (79.9%)\n",
      "   ...Fetched 89000 / 110195 (80.8%)\n",
      "   ...Fetched 90000 / 110195 (81.7%)\n",
      "   ...Fetched 91000 / 110195 (82.6%)\n",
      "   ...Fetched 92000 / 110195 (83.5%)\n",
      "   ...Fetched 93000 / 110195 (84.4%)\n",
      "   ...Fetched 94000 / 110195 (85.3%)\n",
      "   ...Fetched 95000 / 110195 (86.2%)\n",
      "   ...Fetched 96000 / 110195 (87.1%)\n",
      "   ...Fetched 97000 / 110195 (88.0%)\n",
      "   ...Fetched 98000 / 110195 (88.9%)\n",
      "   ...Fetched 99000 / 110195 (89.8%)\n",
      "   ...Fetched 100000 / 110195 (90.7%)\n",
      "   ...Fetched 101000 / 110195 (91.7%)\n",
      "   ...Fetched 102000 / 110195 (92.6%)\n",
      "   ...Fetched 103000 / 110195 (93.5%)\n",
      "   ...Fetched 104000 / 110195 (94.4%)\n",
      "   ...Fetched 105000 / 110195 (95.3%)\n",
      "   ...Fetched 106000 / 110195 (96.2%)\n",
      "   ...Fetched 107000 / 110195 (97.1%)\n",
      "   ...Fetched 108000 / 110195 (98.0%)\n",
      "   ...Fetched 109000 / 110195 (98.9%)\n",
      "   ...Fetched 110000 / 110195 (99.8%)\n",
      "[Done] Saved 110195 records to ..\\..\\data_origin\\OpenAlex_paper\\Sustainable_Development_and_Env_Policy\\Raw_Works.jsonl\n",
      "\n",
      "[Start] Climate_Adaptation_and_Migration (T12656)\n",
      "Total Estimation: 72402\n",
      "   ...Fetched 1000 / 72402 (1.4%)\n",
      "   ...Fetched 2000 / 72402 (2.8%)\n",
      "   ...Fetched 3000 / 72402 (4.1%)\n",
      "   ...Fetched 4000 / 72402 (5.5%)\n",
      "   ...Fetched 5000 / 72402 (6.9%)\n",
      "   ...Fetched 6000 / 72402 (8.3%)\n",
      "   ...Fetched 7000 / 72402 (9.7%)\n",
      "   ...Fetched 8000 / 72402 (11.0%)\n",
      "   ...Fetched 9000 / 72402 (12.4%)\n",
      "   ...Fetched 10000 / 72402 (13.8%)\n",
      "   ...Fetched 11000 / 72402 (15.2%)\n",
      "   ...Fetched 12000 / 72402 (16.6%)\n",
      "   ...Fetched 13000 / 72402 (18.0%)\n",
      "   ...Fetched 14000 / 72402 (19.3%)\n",
      "   ...Fetched 15000 / 72402 (20.7%)\n",
      "   ...Fetched 16000 / 72402 (22.1%)\n",
      "   ...Fetched 17000 / 72402 (23.5%)\n",
      "   ...Fetched 18000 / 72402 (24.9%)\n",
      "   ...Fetched 19000 / 72402 (26.2%)\n",
      "   ...Fetched 20000 / 72402 (27.6%)\n",
      "   ...Fetched 21000 / 72402 (29.0%)\n",
      "   ...Fetched 22000 / 72402 (30.4%)\n",
      "   ...Fetched 23000 / 72402 (31.8%)\n",
      "   ...Fetched 24000 / 72402 (33.1%)\n",
      "   ...Fetched 25000 / 72402 (34.5%)\n",
      "   ...Fetched 26000 / 72402 (35.9%)\n",
      "   ...Fetched 27000 / 72402 (37.3%)\n",
      "   ...Fetched 28000 / 72402 (38.7%)\n",
      "   ...Fetched 29000 / 72402 (40.1%)\n",
      "   ...Fetched 30000 / 72402 (41.4%)\n",
      "   ...Fetched 31000 / 72402 (42.8%)\n",
      "   ...Fetched 32000 / 72402 (44.2%)\n",
      "   ...Fetched 33000 / 72402 (45.6%)\n",
      "   ...Fetched 34000 / 72402 (47.0%)\n",
      "   ...Fetched 35000 / 72402 (48.3%)\n",
      "   ...Fetched 36000 / 72402 (49.7%)\n",
      "   ...Fetched 37000 / 72402 (51.1%)\n",
      "   ...Fetched 38000 / 72402 (52.5%)\n",
      "   ...Fetched 39000 / 72402 (53.9%)\n",
      "   ...Fetched 40000 / 72402 (55.2%)\n",
      "   ...Fetched 41000 / 72402 (56.6%)\n",
      "   ...Fetched 42000 / 72402 (58.0%)\n",
      "   ...Fetched 43000 / 72402 (59.4%)\n",
      "   ...Fetched 44000 / 72402 (60.8%)\n",
      "   ...Fetched 45000 / 72402 (62.2%)\n",
      "   ...Fetched 46000 / 72402 (63.5%)\n",
      "   ...Fetched 47000 / 72402 (64.9%)\n",
      "   ...Fetched 48000 / 72402 (66.3%)\n",
      "   ...Fetched 49000 / 72402 (67.7%)\n",
      "   ...Fetched 50000 / 72402 (69.1%)\n",
      "   ...Fetched 51000 / 72402 (70.4%)\n",
      "   ...Fetched 52000 / 72402 (71.8%)\n",
      "   ...Fetched 53000 / 72402 (73.2%)\n",
      "   ...Fetched 54000 / 72402 (74.6%)\n",
      "   ...Fetched 55000 / 72402 (76.0%)\n",
      "   ...Fetched 56000 / 72402 (77.3%)\n",
      "   ...Fetched 57000 / 72402 (78.7%)\n",
      "   ...Fetched 58000 / 72402 (80.1%)\n",
      "   ...Fetched 59000 / 72402 (81.5%)\n",
      "   ...Fetched 60000 / 72402 (82.9%)\n",
      "   ...Fetched 61000 / 72402 (84.3%)\n",
      "   ...Fetched 62000 / 72402 (85.6%)\n",
      "   ...Fetched 63000 / 72402 (87.0%)\n",
      "   ...Fetched 64000 / 72402 (88.4%)\n",
      "   ...Fetched 65000 / 72402 (89.8%)\n",
      "   ...Fetched 66000 / 72402 (91.2%)\n",
      "   ...Fetched 67000 / 72402 (92.5%)\n",
      "   ...Fetched 68000 / 72402 (93.9%)\n",
      "   ...Fetched 69000 / 72402 (95.3%)\n",
      "   ...Fetched 70000 / 72402 (96.7%)\n",
      "   ...Fetched 71000 / 72402 (98.1%)\n",
      "   ...Fetched 72000 / 72402 (99.4%)\n",
      "[Done] Saved 72402 records to ..\\..\\data_origin\\OpenAlex_paper\\Climate_Adaptation_and_Migration\\Raw_Works.jsonl\n",
      "\n",
      "[Start] Climate_Communication_and_Perception (T11488)\n",
      "Total Estimation: 91313\n",
      "   ...Fetched 1000 / 91313 (1.1%)\n",
      "   ...Fetched 2000 / 91313 (2.2%)\n",
      "   ...Fetched 3000 / 91313 (3.3%)\n",
      "   ...Fetched 4000 / 91313 (4.4%)\n",
      "   ...Fetched 5000 / 91313 (5.5%)\n",
      "   ...Fetched 6000 / 91313 (6.6%)\n",
      "   ...Fetched 7000 / 91313 (7.7%)\n",
      "   ...Fetched 8000 / 91313 (8.8%)\n",
      "   ...Fetched 9000 / 91313 (9.9%)\n",
      "   ...Fetched 10000 / 91313 (11.0%)\n",
      "   ...Fetched 11000 / 91313 (12.0%)\n",
      "   ...Fetched 12000 / 91313 (13.1%)\n",
      "   ...Fetched 13000 / 91313 (14.2%)\n",
      "   ...Fetched 14000 / 91313 (15.3%)\n",
      "   ...Fetched 15000 / 91313 (16.4%)\n",
      "   ...Fetched 16000 / 91313 (17.5%)\n",
      "   ...Fetched 17000 / 91313 (18.6%)\n",
      "   ...Fetched 18000 / 91313 (19.7%)\n",
      "   ...Fetched 19000 / 91313 (20.8%)\n",
      "   ...Fetched 20000 / 91313 (21.9%)\n",
      "   ...Fetched 21000 / 91313 (23.0%)\n",
      "   ...Fetched 22000 / 91313 (24.1%)\n",
      "   ...Fetched 23000 / 91313 (25.2%)\n",
      "   ...Fetched 24000 / 91313 (26.3%)\n",
      "   ...Fetched 25000 / 91313 (27.4%)\n",
      "   ...Fetched 26000 / 91313 (28.5%)\n",
      "   ...Fetched 27000 / 91313 (29.6%)\n",
      "   ...Fetched 28000 / 91313 (30.7%)\n",
      "   ...Fetched 29000 / 91313 (31.8%)\n",
      "   ...Fetched 30000 / 91313 (32.9%)\n",
      "   ...Fetched 31000 / 91313 (33.9%)\n",
      "   ...Fetched 32000 / 91313 (35.0%)\n",
      "   ...Fetched 33000 / 91313 (36.1%)\n",
      "   ...Fetched 34000 / 91313 (37.2%)\n",
      "   ...Fetched 35000 / 91313 (38.3%)\n",
      "   ...Fetched 36000 / 91313 (39.4%)\n",
      "   ...Fetched 37000 / 91313 (40.5%)\n",
      "   ...Fetched 38000 / 91313 (41.6%)\n",
      "   ...Fetched 39000 / 91313 (42.7%)\n",
      "   ...Fetched 40000 / 91313 (43.8%)\n",
      "   ...Fetched 41000 / 91313 (44.9%)\n",
      "   ...Fetched 42000 / 91313 (46.0%)\n",
      "   ...Fetched 43000 / 91313 (47.1%)\n",
      "   ...Fetched 44000 / 91313 (48.2%)\n",
      "   ...Fetched 45000 / 91313 (49.3%)\n",
      "   ...Fetched 46000 / 91313 (50.4%)\n",
      "   ...Fetched 47000 / 91313 (51.5%)\n",
      "   ...Fetched 48000 / 91313 (52.6%)\n",
      "   ...Fetched 49000 / 91313 (53.7%)\n",
      "   ...Fetched 50000 / 91313 (54.8%)\n",
      "   ...Fetched 51000 / 91313 (55.9%)\n",
      "   ...Fetched 52000 / 91313 (56.9%)\n",
      "   ...Fetched 53000 / 91313 (58.0%)\n",
      "   ...Fetched 54000 / 91313 (59.1%)\n",
      "   ...Fetched 55000 / 91313 (60.2%)\n",
      "   ...Fetched 56000 / 91313 (61.3%)\n",
      "   ...Fetched 57000 / 91313 (62.4%)\n",
      "   ...Fetched 58000 / 91313 (63.5%)\n",
      "   ...Fetched 59000 / 91313 (64.6%)\n",
      "   ...Fetched 60000 / 91313 (65.7%)\n",
      "   ...Fetched 61000 / 91313 (66.8%)\n",
      "   ...Fetched 62000 / 91313 (67.9%)\n",
      "   ...Fetched 63000 / 91313 (69.0%)\n",
      "   ...Fetched 64000 / 91313 (70.1%)\n",
      "   ...Fetched 65000 / 91313 (71.2%)\n",
      "   ...Fetched 66000 / 91313 (72.3%)\n",
      "   ...Fetched 67000 / 91313 (73.4%)\n",
      "   ...Fetched 68000 / 91313 (74.5%)\n",
      "   ...Fetched 69000 / 91313 (75.6%)\n",
      "   ...Fetched 70000 / 91313 (76.7%)\n",
      "   ...Fetched 71000 / 91313 (77.8%)\n",
      "   ...Fetched 72000 / 91313 (78.8%)\n",
      "   ...Fetched 73000 / 91313 (79.9%)\n",
      "   ...Fetched 74000 / 91313 (81.0%)\n",
      "   ...Fetched 75000 / 91313 (82.1%)\n",
      "   ...Fetched 76000 / 91313 (83.2%)\n",
      "   ...Fetched 77000 / 91313 (84.3%)\n",
      "   ...Fetched 78000 / 91313 (85.4%)\n",
      "   ...Fetched 79000 / 91313 (86.5%)\n",
      "   ...Fetched 80000 / 91313 (87.6%)\n",
      "   ...Fetched 81000 / 91313 (88.7%)\n",
      "   ...Fetched 82000 / 91313 (89.8%)\n",
      "   ...Fetched 83000 / 91313 (90.9%)\n",
      "   ...Fetched 84000 / 91313 (92.0%)\n",
      "   ...Fetched 85000 / 91313 (93.1%)\n",
      "   ...Fetched 86000 / 91313 (94.2%)\n",
      "   ...Fetched 87000 / 91313 (95.3%)\n",
      "   ...Fetched 88000 / 91313 (96.4%)\n",
      "   ...Fetched 89000 / 91313 (97.5%)\n",
      "   ...Fetched 90000 / 91313 (98.6%)\n",
      "   ...Fetched 91000 / 91313 (99.7%)\n",
      "[Done] Saved 91313 records to ..\\..\\data_origin\\OpenAlex_paper\\Climate_Communication_and_Perception\\Raw_Works.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "EMAIL = \"2224272392@ybu.edu.cn\"\n",
    "BASE_DIR = os.path.join(\"..\", \"..\", \"data_origin\", \"OpenAlex_paper\")\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "TOPICS = {\n",
    "    \"T10471\": \"Climate_Change_Policy_and_Economics\",\n",
    "    \"T12013\": \"Sustainable_Development_and_Env_Policy\",\n",
    "    \"T12656\": \"Climate_Adaptation_and_Migration\",\n",
    "    \"T11488\": \"Climate_Communication_and_Perception\"\n",
    "}\n",
    "\n",
    "def fetch_raw_data(tid, tname):\n",
    "    print(f\"\\n[Start] {tname} ({tid})\")\n",
    "    \n",
    "    # Create directory\n",
    "    save_dir = os.path.join(BASE_DIR, tname)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    raw_path = os.path.join(save_dir, \"Raw_Works.jsonl\")\n",
    "    \n",
    "    # API Params\n",
    "    params = {\n",
    "        \"filter\": f\"topics.id:{tid}\",\n",
    "        \"per_page\": 200,\n",
    "        \"cursor\": \"*\", \n",
    "        \"sort\": \"publication_date:desc\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": f\"PythonScript/1.0 (mailto:{EMAIL})\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\"\n",
    "    }\n",
    "\n",
    "    # Reset counters for this topic\n",
    "    count = 0\n",
    "    total_count = None\n",
    "    \n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        while params[\"cursor\"]:\n",
    "            try:\n",
    "                # Request with timeout\n",
    "                resp = requests.get(BASE_URL, params=params, headers=headers, timeout=60)\n",
    "                if resp.status_code != 200:\n",
    "                    print(f\"Error: {resp.status_code}\")\n",
    "                    break\n",
    "                \n",
    "                data = resp.json()\n",
    "                \n",
    "                # Get estimation from the first page metadata\n",
    "                if total_count is None:\n",
    "                    total_count = data.get(\"meta\", {}).get(\"count\", 0)\n",
    "                    print(f\"Total Estimation: {total_count}\")\n",
    "                \n",
    "                results = data.get(\"results\", [])\n",
    "                if not results: break\n",
    "                \n",
    "                # Write to file\n",
    "                for work in results:\n",
    "                    f.write(json.dumps(work, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                count += len(results)\n",
    "                \n",
    "                # Print progress every 1000 items\n",
    "                if count % 1000 == 0:\n",
    "                    pct = (count / total_count * 100) if total_count else 0\n",
    "                    print(f\"   ...Fetched {count} / {total_count} ({pct:.1f}%)\")\n",
    "                \n",
    "                # Update cursor\n",
    "                params[\"cursor\"] = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Network error: {e}, retrying in 3s...\")\n",
    "                time.sleep(3)\n",
    "\n",
    "    print(f\"[Done] Saved {count} records to {raw_path}\")\n",
    "\n",
    "# --- Execution ---\n",
    "print(f\"Data Path: {os.path.abspath(BASE_DIR)}\")\n",
    "for tid, tname in TOPICS.items():\n",
    "    fetch_raw_data(tid, tname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598b36b",
   "metadata": {},
   "source": [
    "#### OD构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a58cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Path: f:\\Desktop\\科研项目\\1.负责科研项目\\Climate Policy\\CAMPF_Supplementary_V2\\data_origin\\OpenAlex_paper\n",
      "Total Analysis Path: f:\\Desktop\\科研项目\\1.负责科研项目\\Climate Policy\\CAMPF_Supplementary_V2\\data_origin\\OpenAlex_paper\\Total_Data_Analysis\n",
      "\n",
      "[Processing] Climate_Change_Policy_and_Economics\n",
      "   Saved: .../Climate_Change_Policy_and_Economics/OD_Build\n",
      "[Processing] Sustainable_Development_and_Env_Policy\n",
      "   Saved: .../Sustainable_Development_and_Env_Policy/OD_Build\n",
      "[Processing] Climate_Adaptation_and_Migration\n",
      "   Saved: .../Climate_Adaptation_and_Migration/OD_Build\n",
      "[Processing] Climate_Communication_and_Perception\n",
      "   Saved: .../Climate_Communication_and_Perception/OD_Build\n",
      "\n",
      "[Done] Processed 468730 unique works.\n",
      "Merged output: f:\\Desktop\\科研项目\\1.负责科研项目\\Climate Policy\\CAMPF_Supplementary_V2\\data_origin\\OpenAlex_paper\\Total_Data_Analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools # 用于生成两两组合\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_DIR = os.path.join(\"..\", \"..\", \"data_origin\", \"OpenAlex_paper\")\n",
    "TOTAL_DIR = os.path.join(BASE_DIR, \"Total_Data_Analysis\")\n",
    "\n",
    "TOPICS = {\n",
    "    \"T10471\": \"Climate_Change_Policy_and_Economics\",\n",
    "    \"T12013\": \"Sustainable_Development_and_Env_Policy\",\n",
    "    \"T12656\": \"Climate_Adaptation_and_Migration\",\n",
    "    \"T11488\": \"Climate_Communication_and_Perception\"\n",
    "}\n",
    "\n",
    "# --- Helpers ---\n",
    "def get_region_label(is_south):\n",
    "    if is_south is True: return \"Global South\"\n",
    "    if is_south is False: return \"Global North\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def get_interaction_type(src_south, tgt_south):\n",
    "    if src_south is None or tgt_south is None: return \"Unknown\"\n",
    "    val = int(src_south) + int(tgt_south)\n",
    "    if val == 0: return \"North-North\"\n",
    "    if val == 2: return \"South-South\"\n",
    "    return \"North-South\"\n",
    "\n",
    "def process_all_data():\n",
    "    # 1. Setup Global Writers\n",
    "    os.makedirs(TOTAL_DIR, exist_ok=True)\n",
    "    \n",
    "    f_total_net = open(os.path.join(TOTAL_DIR, \"OD_Network.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "    f_total_rich = open(os.path.join(TOTAL_DIR, \"Works_Enriched.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "    path_total_base = os.path.join(TOTAL_DIR, \"Country_Baseline.jsonl\")\n",
    "    \n",
    "    global_stats = defaultdict(lambda: {\"score\": 0.0, \"is_south\": None, \"count\": 0})\n",
    "    seen_work_ids = set()\n",
    "    \n",
    "    print(f\"Root Path: {os.path.abspath(BASE_DIR)}\")\n",
    "    print(f\"Total Analysis Path: {os.path.abspath(TOTAL_DIR)}\\n\")\n",
    "\n",
    "    # 2. Iterate Topics\n",
    "    for _, tname in TOPICS.items():\n",
    "        topic_dir = os.path.join(BASE_DIR, tname)\n",
    "        raw_path = os.path.join(topic_dir, \"Raw_Works.jsonl\")\n",
    "        \n",
    "        if not os.path.exists(raw_path):\n",
    "            print(f\"Skipping: {tname}\")\n",
    "            continue\n",
    "\n",
    "        # Setup Topic Writers\n",
    "        output_dir = os.path.join(topic_dir, \"OD_Build\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        f_topic_net = open(os.path.join(output_dir, \"OD_Network.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "        f_topic_rich = open(os.path.join(output_dir, \"Works_Enriched.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "        path_topic_base = os.path.join(output_dir, \"Country_Baseline.jsonl\")\n",
    "        \n",
    "        topic_stats = defaultdict(lambda: {\"score\": 0.0, \"is_south\": None, \"count\": 0})\n",
    "        \n",
    "        print(f\"[Processing] {tname}\")\n",
    "\n",
    "        with open(raw_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "            for line in f_in:\n",
    "                try:\n",
    "                    work = json.loads(line)\n",
    "                except: continue\n",
    "                \n",
    "                wid = work.get(\"id\")\n",
    "                year = work.get(\"publication_year\")\n",
    "\n",
    "                # --- Extract Metadata (Enriched) ---\n",
    "                sdgs = [s['display_name'] for s in (work.get('sustainable_development_goals') or [])]\n",
    "                concepts = [{\"name\": c['display_name'], \"score\": c['score']} for c in (work.get('concepts') or [])[:5]]\n",
    "                grants = [g.get('funder_display_name') or g.get('funder') for g in (work.get('grants') or []) if g.get('funder')]\n",
    "                \n",
    "                primary_loc = work.get(\"primary_location\") or {}\n",
    "                source = primary_loc.get(\"source\") or {}\n",
    "                journal = source.get(\"display_name\")\n",
    "                \n",
    "                rich_data = json.dumps({\n",
    "                    \"work_id\": wid,\n",
    "                    \"title\": work.get(\"title\"),\n",
    "                    \"year\": year,\n",
    "                    \"doi\": work.get(\"doi\"),\n",
    "                    \"cited_by\": work.get(\"cited_by_count\"),\n",
    "                    \"fwci\": work.get(\"fwci\"),\n",
    "                    \"is_oa\": work.get(\"open_access\", {}).get(\"is_oa\"),\n",
    "                    \"journal\": journal,\n",
    "                    \"sdgs\": sdgs,\n",
    "                    \"concepts\": concepts,\n",
    "                    \"funders\": grants\n",
    "                }, ensure_ascii=False)\n",
    "\n",
    "                # --- Build Network (Full Mesh / All Pairs) ---\n",
    "                edges = []\n",
    "                country_map = {} # {code: is_south}\n",
    "                \n",
    "                # Extract all unique countries from all authors\n",
    "                authorships = work.get(\"authorships\") or []\n",
    "                for auth in authorships:\n",
    "                    insts = auth.get(\"institutions\") or []\n",
    "                    if not insts: continue\n",
    "                    inst = insts[0]\n",
    "                    cc = inst.get(\"country_code\")\n",
    "                    if cc:\n",
    "                        country_map[cc] = inst.get(\"is_global_south\")\n",
    "                \n",
    "                # Sort to ensure consistent A-B order (e.g. always CN-US, never US-CN)\n",
    "                unique_countries = sorted(list(country_map.keys()))\n",
    "                n = len(unique_countries)\n",
    "                \n",
    "                if n > 0:\n",
    "                    # 1. Update Node Stats (Score = 1/N)\n",
    "                    for cc in unique_countries:\n",
    "                        is_south = country_map[cc]\n",
    "                        score_contrib = 1.0 / n\n",
    "                        \n",
    "                        # Topic Level\n",
    "                        topic_stats[cc][\"score\"] += score_contrib\n",
    "                        topic_stats[cc][\"count\"] += 1\n",
    "                        if topic_stats[cc][\"is_south\"] is None: topic_stats[cc][\"is_south\"] = is_south\n",
    "                        \n",
    "                        # Global Level (Calculated later to handle duplicates properly)\n",
    "                    \n",
    "                    # 2. Generate Edges (Combinations of 2)\n",
    "                    if n > 1:\n",
    "                        # Total edges in complete graph = N * (N-1) / 2\n",
    "                        num_edges = n * (n - 1) / 2\n",
    "                        weight = 1.0 / num_edges  # Sum of weights = 1.0\n",
    "                        \n",
    "                        # Generate all unique pairs\n",
    "                        for c1, c2 in itertools.combinations(unique_countries, 2):\n",
    "                            s1 = country_map[c1]\n",
    "                            s2 = country_map[c2]\n",
    "                            \n",
    "                            edges.append(json.dumps({\n",
    "                                \"work_id\": wid,\n",
    "                                \"year\": year,\n",
    "                                \"source\": c1, # Alphabetically first\n",
    "                                \"target\": c2, # Alphabetically second\n",
    "                                \"weight\": weight,\n",
    "                                \"interaction\": get_interaction_type(s1, s2)\n",
    "                            }, ensure_ascii=False))\n",
    "\n",
    "                # --- Write to Files ---\n",
    "                \n",
    "                # Topic Files\n",
    "                f_topic_rich.write(rich_data + \"\\n\")\n",
    "                for e in edges:\n",
    "                    f_topic_net.write(e + \"\\n\")\n",
    "\n",
    "                # Global Files (Deduplicated)\n",
    "                if wid not in seen_work_ids:\n",
    "                    f_total_rich.write(rich_data + \"\\n\")\n",
    "                    for e in edges:\n",
    "                        f_total_net.write(e + \"\\n\")\n",
    "                    \n",
    "                    # Update Global Stats (Only for new unique papers)\n",
    "                    if n > 0:\n",
    "                        for cc in unique_countries:\n",
    "                            global_stats[cc][\"score\"] += 1.0 / n\n",
    "                            global_stats[cc][\"count\"] += 1\n",
    "                            if global_stats[cc][\"is_south\"] is None: \n",
    "                                global_stats[cc][\"is_south\"] = country_map[cc]\n",
    "                    \n",
    "                    seen_work_ids.add(wid)\n",
    "\n",
    "        # Cleanup Topic\n",
    "        f_topic_net.close()\n",
    "        f_topic_rich.close()\n",
    "        \n",
    "        if topic_stats:\n",
    "            with open(path_topic_base, \"w\", encoding=\"utf-8\") as f_base:\n",
    "                for cc, val in sorted(topic_stats.items(), key=lambda x: -x[1][\"score\"]):\n",
    "                    f_base.write(json.dumps({\n",
    "                        \"country\": cc,\n",
    "                        \"score\": val[\"score\"],\n",
    "                        \"count\": val[\"count\"],\n",
    "                        \"region\": get_region_label(val[\"is_south\"])\n",
    "                    }, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"   Saved: .../{tname}/OD_Build\")\n",
    "\n",
    "    # Cleanup Global\n",
    "    if global_stats:\n",
    "        with open(path_total_base, \"w\", encoding=\"utf-8\") as f_base:\n",
    "            for cc, val in sorted(global_stats.items(), key=lambda x: -x[1][\"score\"]):\n",
    "                f_base.write(json.dumps({\n",
    "                    \"country\": cc,\n",
    "                    \"score\": val[\"score\"],\n",
    "                    \"count\": val[\"count\"],\n",
    "                    \"region\": get_region_label(val[\"is_south\"])\n",
    "                }, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    f_total_net.close()\n",
    "    f_total_rich.close()\n",
    "    \n",
    "    print(f\"\\n[Done] Processed {len(seen_work_ids)} unique works.\")\n",
    "    print(f\"Merged output: {os.path.abspath(TOTAL_DIR)}\")\n",
    "\n",
    "# --- Run ---\n",
    "process_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68e213",
   "metadata": {},
   "source": [
    "#### 归一化及矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69fb295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 1-7-T10471-Paper_collab-Climate_Change_Policy_and_Economics.csv (Shape: (49, 49))\n",
      "Saved: 1-7-T12013-Paper_collab-Sustainable_Development_and_Env_Policy.csv (Shape: (49, 49))\n",
      "Saved: 1-7-T12656-Paper_collab-Climate_Adaptation_and_Migration.csv (Shape: (49, 49))\n",
      "Saved: 1-7-T11488-Paper_collab-Climate_Communication_and_Perception.csv (Shape: (49, 49))\n",
      "Saved: 1-7-All-Paper_collab-Total.csv (Shape: (49, 49))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ================= Configuration =================\n",
    "JSON_PATH = '../country_list.json'\n",
    "BASE_DIR = '../../data_origin/OpenAlex_paper'\n",
    "OUTPUT_DIR = '../../data'\n",
    "\n",
    "TOPICS = {\n",
    "    \"T10471\": \"Climate_Change_Policy_and_Economics\",\n",
    "    \"T12013\": \"Sustainable_Development_and_Env_Policy\",\n",
    "    \"T12656\": \"Climate_Adaptation_and_Migration\",\n",
    "    \"T11488\": \"Climate_Communication_and_Perception\"\n",
    "}\n",
    "\n",
    "YEAR_START, YEAR_END = 2005, 2023\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ================= Logic =================\n",
    "\n",
    "def get_iso_mapping(json_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping ISO-2 (OpenAlex format) to ISO-3 (Target format).\n",
    "    Example: {'CN': 'CHN', 'US': 'USA'}\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        return {item['iso_2']: item['iso'] for item in json.load(f)['countries']}\n",
    "\n",
    "def generate_matrix(input_path: str, output_path: str, iso_map: dict) -> None:\n",
    "    \"\"\"\n",
    "    Reads OD Network (ISO-2), calculates normalized matrix, \n",
    "    converts headers to ISO-3, and saves as CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Skipping (File not found): {input_path}\")\n",
    "        return\n",
    "\n",
    "    # 1. Load Data\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "    \n",
    "    if not data:\n",
    "        print(f\"Skipping (Empty file): {input_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Target keys for filtering are the ISO-2 codes\n",
    "    target_iso2 = list(iso_map.keys())\n",
    "\n",
    "    # 2. Filter Scope (using ISO-2)\n",
    "    df = df[\n",
    "        (df['year'] >= YEAR_START) & \n",
    "        (df['year'] <= YEAR_END) & \n",
    "        (df['source'].isin(target_iso2)) & \n",
    "        (df['target'].isin(target_iso2))\n",
    "    ]\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"Skipping (No valid data): {input_path}\")\n",
    "        return\n",
    "\n",
    "    # 3. Aggregate (Sum weights per pair per year)\n",
    "    df_agg = df.groupby(['source', 'target', 'year'])['weight'].sum().reset_index()\n",
    "\n",
    "    # 4. Symmetrize\n",
    "    df_sym = pd.concat([\n",
    "        df_agg.rename(columns={'source': 'ccode1', 'target': 'ccode2'}),\n",
    "        df_agg.rename(columns={'source': 'ccode2', 'target': 'ccode1'})\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # Consolidate sums\n",
    "    df_sym = df_sym.groupby(['ccode1', 'ccode2', 'year'])['weight'].sum().reset_index()\n",
    "\n",
    "    # 5. Log Transformation & Yearly Normalization\n",
    "    # 在归一化之前进行 log1p 操作\n",
    "    df_sym['weight'] = np.log1p(df_sym['weight'])\n",
    "\n",
    "    grouped = df_sym.groupby('year')['weight']\n",
    "    min_vals = grouped.transform('min')\n",
    "    max_vals = grouped.transform('max')\n",
    "    denominator = max_vals - min_vals\n",
    "\n",
    "    df_sym['norm_score'] = np.where(\n",
    "        denominator == 0, \n",
    "        0.0, \n",
    "        (df_sym['weight'] - min_vals) / denominator\n",
    "    )\n",
    "\n",
    "    # 6. Average over years\n",
    "    df_final = df_sym.groupby(['ccode1', 'ccode2'])['norm_score'].mean().reset_index()\n",
    "\n",
    "    # 7. Pivot & Polish\n",
    "    matrix = df_final.pivot(index='ccode1', columns='ccode2', values='norm_score')\n",
    "    \n",
    "    # [CRITICAL] Reindex using ISO-2 first to align with data\n",
    "    matrix = matrix.reindex(index=target_iso2, columns=target_iso2).fillna(0)\n",
    "    \n",
    "    # [CRITICAL] Map Index and Columns from ISO-2 to ISO-3\n",
    "    matrix.index = matrix.index.map(iso_map)\n",
    "    matrix.columns = matrix.columns.map(iso_map)\n",
    "    \n",
    "    # Final cleanup (Diagonal 0, Sort by ISO-3)\n",
    "    np.fill_diagonal(matrix.values, 0)\n",
    "    matrix.sort_index(axis=0, inplace=True)\n",
    "    matrix.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    # 8. Save\n",
    "    matrix.to_csv(output_path)\n",
    "    print(f\"Saved: {os.path.basename(output_path)} (Shape: {matrix.shape})\")\n",
    "\n",
    "# ================= Execution =================\n",
    "\n",
    "# Load Map: {'CN': 'CHN', ...}\n",
    "iso_map = get_iso_mapping(JSON_PATH)\n",
    "\n",
    "# 1. Process Individual Topics\n",
    "for tid, tname in TOPICS.items():\n",
    "    in_path = os.path.join(BASE_DIR, tname, \"OD_Build\", \"OD_Network.jsonl\")\n",
    "    filename = f\"1-7-{tid}-Paper_collab-{tname}.csv\"\n",
    "    out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    generate_matrix(in_path, out_path, iso_map)\n",
    "\n",
    "# 2. Process Total Analysis\n",
    "total_in_path = os.path.join(BASE_DIR, \"Total_Data_Analysis\", \"OD_Network.jsonl\")\n",
    "total_out_path = os.path.join(OUTPUT_DIR, \"1-7-All-Paper_collab-Total.csv\")\n",
    "\n",
    "generate_matrix(total_in_path, total_out_path, iso_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_policy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
