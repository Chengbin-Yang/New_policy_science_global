{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787845be",
   "metadata": {},
   "source": [
    "### 最终采用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be4f79",
   "metadata": {},
   "source": [
    "##### 单独算一个语言，避免共线性（最终采用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7eb07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在生成合并后的语言指数矩阵...\n",
      "文件已保存至: ../../data/1-4-Language_Index.csv\n",
      "矩阵维度: (49, 49)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# ==========================================\n",
    "# 配置与路径\n",
    "# ==========================================\n",
    "GRAVITY_CSV = \"../../data_origin/Gravity/Gravity_csv_V202211/Gravity_V202211.csv\"\n",
    "JSON_PATH = \"../country_list.json\"\n",
    "\n",
    "# 修改输出路径：合并为一个文件\n",
    "OUT_PATH = \"../../data/1-4-Language_Index.csv\"\n",
    "\n",
    "def get_target_isos(meta_path: str) -> List[str]:\n",
    "    \"\"\"从JSON配置文件加载目标国家ISO代码列表\"\"\"\n",
    "    with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "        return [c['iso'] for c in json.load(f)['countries']]\n",
    "\n",
    "def build_merged_language_matrix(csv_path: str, target_isos: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    处理Gravity数据集，生成合并后的语言指数矩阵\n",
    "    逻辑：Language_Index = (comlang_off + comlang_ethno) / 2\n",
    "    \"\"\"\n",
    "    # 1. 读取数据\n",
    "    df = pd.read_csv(csv_path, usecols=['year', 'iso3_o', 'iso3_d', 'comlang_off', 'comlang_ethno'])\n",
    "    \n",
    "    # 2. 筛选：特定年份 + 目标国家\n",
    "    mask = (df['year'] == 2019) & (df['iso3_o'].isin(target_isos)) & (df['iso3_d'].isin(target_isos))\n",
    "    df_filtered = df[mask].copy() # 使用 copy 避免 SettingWithCopyWarning\n",
    "\n",
    "    # 3. 【核心修改】计算合并指数 (取平均值)\n",
    "    # comlang_off 和 comlang_ethno 通常是 0 或 1\n",
    "    # 结果可能是 0, 0.5, 或 1\n",
    "    df_filtered['lang_index'] = (df_filtered['comlang_off'] + df_filtered['comlang_ethno']) / 2\n",
    "\n",
    "    # 4. 矩阵化\n",
    "    matrix = df_filtered.pivot_table(index='iso3_o', columns='iso3_d', values='lang_index', aggfunc='max')\n",
    "    \n",
    "    # 5. 对齐国家列表并填充缺失值\n",
    "    matrix = matrix.reindex(index=target_isos, columns=target_isos).fillna(0)\n",
    "    \n",
    "    # 6. 【对角线处理】\n",
    "    # 逻辑：自己和自己的语言相似度应该是 0 (最高)\n",
    "    # 如果你之前的 GDP_Sim 对角线是 0，这里也建议设为 0，保持逻辑统一\n",
    "    np.fill_diagonal(matrix.values, 0)\n",
    "\n",
    "    # 排序\n",
    "    return matrix.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "    \n",
    "    # 执行处理逻辑\n",
    "    target_countries = get_target_isos(JSON_PATH)\n",
    "    \n",
    "    print(\"正在生成合并后的语言指数矩阵...\")\n",
    "    m_lang_index = build_merged_language_matrix(GRAVITY_CSV, target_countries)\n",
    "    \n",
    "    # 保存结果\n",
    "    m_lang_index.to_csv(OUT_PATH)\n",
    "    \n",
    "    print(f\"文件已保存至: {OUT_PATH}\")\n",
    "    print(f\"矩阵维度: {m_lang_index.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ccc02",
   "metadata": {},
   "source": [
    "#### 分开算官方语言和民族语言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a6d5513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 4a & 4b processing complete. Shapes: (49, 49)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "# ==========================================\n",
    "# 配置与路径\n",
    "# ==========================================\n",
    "GRAVITY_CSV = \"../../data_origin/Gravity/Gravity_csv_V202211/Gravity_V202211.csv\"\n",
    "JSON_PATH = \"../country_list.json\"\n",
    "OUT_OFF = \"../../data/1-4a-Common_Official_Language.csv\"\n",
    "OUT_ETHNO = \"../../data/1-4b-Common_Ethno_Language.csv\"\n",
    "\n",
    "def get_target_isos(meta_path: str) -> List[str]:\n",
    "    \"\"\"从JSON配置文件加载目标国家ISO代码列表\"\"\"\n",
    "    with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "        return [c['iso'] for c in json.load(f)['countries']]\n",
    "\n",
    "def build_dual_language_matrices(csv_path: str, target_isos: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    处理Gravity数据集，生成官方语言(comlang_off)和民族语言(comlang_ethno)矩阵\n",
    "    参数:\n",
    "        csv_path: CEPII Gravity CSV文件路径\n",
    "        target_isos: 目标国家代码列表\n",
    "    返回:\n",
    "        (官方语言矩阵, 民族语言矩阵)\n",
    "    \"\"\"\n",
    "    # 仅读取必要列并过滤2019年数据\n",
    "    df = pd.read_csv(csv_path, usecols=['year', 'iso3_o', 'iso3_d', 'comlang_off', 'comlang_ethno'])\n",
    "    mask = (df['year'] == 2019) & (df['iso3_o'].isin(target_isos)) & (df['iso3_d'].isin(target_isos))\n",
    "    df_filtered = df[mask]\n",
    "\n",
    "    results = []\n",
    "    # 分别处理官方语言 和民族语言(>9%人口)\n",
    "    for var in ['comlang_off', 'comlang_ethno']:\n",
    "        # 矩阵化并对齐49国顺序\n",
    "        matrix = df_filtered.pivot_table(index='iso3_o', columns='iso3_d', values=var, aggfunc='max')\n",
    "        matrix = matrix.reindex(index=target_isos, columns=target_isos).fillna(0)\n",
    "        \n",
    "        # 对角线置零（国家自身联系不计入网络分析）\n",
    "        np.fill_diagonal(matrix.values, 0)\n",
    "        results.append(matrix.sort_index(axis=0).sort_index(axis=1))\n",
    "\n",
    "    return tuple(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(os.path.dirname(OUT_OFF), exist_ok=True)\n",
    "    \n",
    "    # 执行处理逻辑\n",
    "    target_countries = get_target_isos(JSON_PATH)\n",
    "    m_official, m_ethno = build_dual_language_matrices(GRAVITY_CSV, target_countries)\n",
    "    \n",
    "    # 保存结果\n",
    "    m_official.to_csv(OUT_OFF)\n",
    "    m_ethno.to_csv(OUT_ETHNO)\n",
    "    \n",
    "    print(f\"Layer 4a & 4b processing complete. Shapes: {m_official.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8345ef6",
   "metadata": {},
   "source": [
    "### 共同口语 存在采样不均和阈值过于严格的问题，故最后没有采用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36dd927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取数据文件: ../../data_origin/Language/ling_web.dta ...\n",
      "--- 任务成功 ---\n",
      "Layer 4 (高级口语交流网络) 已保存至: ../../data/1-4-Common_Spoken_Language.csv\n",
      "矩阵维度: (49, 49)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 配置 (根据你的实际路径调整的相对路径)\n",
    "# ==========================================\n",
    "# 从 code/data_prepare/ 出发：\n",
    "# ../../ 回到项目根目录 CAMPF_Supplementary_V2\n",
    "LING_DTA = \"../../data_origin/Language/ling_web.dta\" \n",
    "JSON_PATH = \"../country_list.json\" # 假设在 code/ 目录下\n",
    "OUT_FILE = \"../../data/1-4-Common_Spoken_Language.csv\"\n",
    "\n",
    "def process_language_layer(dta_path, meta_path):\n",
    "    \"\"\"\n",
    "    处理 Melitz-Toubal 语言数据集，提取共同口语强度 (csl)\n",
    "    \"\"\"\n",
    "    # 1. 加载 49 国列表\n",
    "    with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "        meta = json.load(f)['countries']\n",
    "        target_isos = [c['iso'] for c in meta]\n",
    "\n",
    "    # 2. 读取 STATA 文件\n",
    "    # pandas 会自动处理 .dta 格式\n",
    "    print(f\"正在读取数据文件: {dta_path} ...\")\n",
    "    df = pd.read_stata(dta_path)\n",
    "    \n",
    "    # 3. 筛选 49 国数据\n",
    "    # 只保留出发国和目的国都在列表中的记录\n",
    "    df_filtered = df[df['iso_o'].isin(target_isos) & df['iso_d'].isin(target_isos)]\n",
    "    \n",
    "    # 4. 矩阵化\n",
    "    # csl (Common Spoken Language) 是 0-1 的概率值，直接使用\n",
    "    m = df_filtered.pivot_table(index='iso_o', columns='iso_d', values='csl', aggfunc='mean')\n",
    "    \n",
    "    # 5. 补齐国家并对齐顺序 (确保 49x49 且顺序一致)\n",
    "    m = m.reindex(index=target_isos, columns=target_isos).fillna(0)\n",
    "    \n",
    "    # 6. 对角线处理 (国家自身的交流强度在网络分析中通常设为 0)\n",
    "    for i in range(len(m)):\n",
    "        m.iloc[i, i] = 0\n",
    "        \n",
    "    return m.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# 执行\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    if os.path.exists(LING_DTA):\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)\n",
    "        \n",
    "        # 处理数据\n",
    "        lang_matrix = process_language_layer(LING_DTA, JSON_PATH)\n",
    "        \n",
    "        # 保存结果\n",
    "        lang_matrix.to_csv(OUT_FILE)\n",
    "        print(f\"--- 任务成功 ---\")\n",
    "        print(f\"Layer 4 (高级口语交流网络) 已保存至: {OUT_FILE}\")\n",
    "        print(f\"矩阵维度: {lang_matrix.shape}\")\n",
    "    else:\n",
    "        print(f\"错误：在路径 {os.path.abspath(LING_DTA)} 下找不到原始数据文件。\")\n",
    "        print(\"请检查相对路径层级是否正确。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_policy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
